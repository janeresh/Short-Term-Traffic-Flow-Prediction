{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import numpy.linalg as la\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "#import matplotlib.pyplot as plt\n",
    "import time\n",
    "time_start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of Dataset-SZ Traffic\n",
    "def load_sz_data():\n",
    "    sz_adj = pd.read_csv('sz_adj.csv',header=None)\n",
    "    adj = np.mat(sz_adj)\n",
    "    sz_tf = pd.read_csv('sz_speed.csv')\n",
    "    return sz_tf, adj\n",
    "\n",
    "data, adj = load_sz_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          90217      90218      90219      90220      90221      90222  \\\n",
      "0      8.471971  18.455410  20.590635  15.345258   9.585218  21.501821   \n",
      "1      7.807137  15.713816  27.523695  11.087895   9.455280  17.332246   \n",
      "2      8.809457   8.979647  20.280394  16.523419   8.003314  15.789483   \n",
      "3     51.590372  23.631243  20.224094  15.116459   6.642644  17.575806   \n",
      "4     58.770433  20.437740  20.465606  14.820217  11.344404   0.000000   \n",
      "5     58.289126  10.332738  25.331018  18.485616   2.028513  10.718488   \n",
      "6     61.334449  20.818480  15.839392   8.768217  13.933326  13.866124   \n",
      "7     58.903144   8.291826  44.043729  15.250859   0.000000   0.000000   \n",
      "8     57.210441  25.765667  18.677565   5.066051   0.000000  12.645903   \n",
      "9     59.559265  16.803983  20.314610  23.393083   2.612821   0.000000   \n",
      "10    60.217867  26.773576  17.684078  17.735375  12.957171   7.404007   \n",
      "11    56.336405  18.866552  27.502295  18.201105  23.973238  22.215358   \n",
      "12    58.511395  15.802270  15.900551  15.825524  28.359067  13.519283   \n",
      "13    59.327670  28.104659  23.004938  17.177101   5.770516   0.000000   \n",
      "14    59.415925  20.049136  24.239933  16.541053  18.319476  30.199940   \n",
      "15    57.000535  25.916699  23.620492  15.149346  18.415374  10.608687   \n",
      "16    59.468388  23.153764  22.637581  21.761991   7.845323  14.191932   \n",
      "17    57.489538  29.269876  21.906870  15.336707  23.427912  22.823073   \n",
      "18    60.289596  24.220646  18.846753  20.107602  17.853574   0.000000   \n",
      "19    58.943452  28.973177  23.110562  17.220674  21.988291  42.517631   \n",
      "20    59.584971  28.455832  19.782971  18.319035  20.363029   9.263639   \n",
      "21    61.006783  20.343802  23.375934  14.071018  11.776268  14.986706   \n",
      "22    61.391417  16.989282  17.488080  16.687639  18.837553  12.817948   \n",
      "23    62.847104  17.922808  20.711342  12.684088  14.431455  37.580218   \n",
      "24    59.712049  22.491029  20.029159  19.459853  10.929650  12.636402   \n",
      "25    60.128278  11.853744  21.438214  16.230672  18.646959  33.716293   \n",
      "26    57.230341  21.015119  20.436854  18.399933  10.754621  28.184720   \n",
      "27    61.317819  18.813310  22.462014  18.800135  19.252080  31.657746   \n",
      "28    57.949689  23.351677  24.078893  15.910953  19.144494  30.628915   \n",
      "29    60.440370  19.183227  22.276537  18.166327  20.971482   9.271435   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "2946  37.047736  11.747849   9.158301  12.016694  10.121105  16.834794   \n",
      "2947  32.587985  11.186664  10.718418  12.244456  11.516860  22.302047   \n",
      "2948  28.728285  14.481847   9.443261   9.845491   6.633443   5.380683   \n",
      "2949  27.710759  15.547506  10.484638  12.722840  10.626220  10.623221   \n",
      "2950  26.003952  15.903029   8.968620  10.328466   9.006967  27.212662   \n",
      "2951  30.641217  13.588164   9.242868  11.115546   8.813212  22.479598   \n",
      "2952  29.589988  13.610331   9.388144   8.920857   9.951734  25.764707   \n",
      "2953  32.527293  11.195964   9.614881  11.488864   9.484497  26.042143   \n",
      "2954  32.717067  11.197344   9.038040  10.987866   8.974821  12.656677   \n",
      "2955  32.833424  11.759863   7.935098  11.153611   8.666250  26.168973   \n",
      "2956  30.866704  13.977036   9.085705  10.476089  10.399103  13.961974   \n",
      "2957  36.407212  13.445718   8.871406  12.298801   9.390320  13.086711   \n",
      "2958  29.498956  15.263794   9.542898  13.793024  11.678501  10.690330   \n",
      "2959  34.159054   8.463801   9.084855  10.914051   8.824929  10.646655   \n",
      "2960  29.827767  10.139989   8.011831   9.475263   8.314195  34.699582   \n",
      "2961  34.712911  13.224499   7.302345   9.891491   9.414917  22.504949   \n",
      "2962  29.886521  13.882277   8.170605   9.800999   8.409821  22.904986   \n",
      "2963  31.016664  10.664699   6.565841  10.013826  10.228492  28.740343   \n",
      "2964  31.287008  13.376588   6.999327   9.330356  10.903492  22.816266   \n",
      "2965  32.527834  11.507783   7.120969   9.997413  10.916735  14.875885   \n",
      "2966  31.001262  10.235263   7.584673  11.063059   9.750946  18.449771   \n",
      "2967  28.383740  11.333047   8.815655   9.994421   6.719870  17.476265   \n",
      "2968  31.422072  11.200472   8.240685  12.568828   8.239171  18.232270   \n",
      "2969  30.696860   9.195974   7.187724  10.021121   8.282266  14.751376   \n",
      "2970  29.795250   9.772127   6.848839   9.454226   7.350518  15.062446   \n",
      "2971  34.595076  11.353815   8.251759  10.507924   8.129874  17.123473   \n",
      "2972  28.697368  13.005691   8.131853  10.418131   8.087082  10.528642   \n",
      "2973  32.797527  12.241658   9.035159   8.513272   9.346719   6.296222   \n",
      "2974  34.331867  16.075872  10.543411  11.254747   9.930924  26.989660   \n",
      "2975  32.930700  12.101170   9.006258  11.971602   9.063041   3.209035   \n",
      "\n",
      "          90223      90224      90225      90226  ...     112746     116097  \\\n",
      "0     31.611759   0.000000   0.000000  22.008941  ...   0.000000  27.623514   \n",
      "1     33.976531  12.968823   0.000000  17.921757  ...   4.193466  14.889713   \n",
      "2     13.747267  12.221143   0.000000  21.588447  ...  11.076185   0.000000   \n",
      "3     17.657556  15.998745   0.000000  24.228905  ...   0.000000  34.132564   \n",
      "4     33.878502   5.676248   0.000000  26.250064  ...   0.000000  15.904281   \n",
      "5     10.439684   0.000000   2.122070  20.836691  ...  10.167713  15.566301   \n",
      "6     31.703332   4.823769   5.320506  29.906599  ...   0.000000  46.163459   \n",
      "7     28.502493   8.702932  15.535269  21.537570  ...   0.000000  29.304136   \n",
      "8     41.644261   0.000000   0.000000  36.559919  ...   5.753177  14.196596   \n",
      "9     43.230732  17.152937   5.947999  36.524772  ...   0.000000  21.855200   \n",
      "10    34.944085  20.617665   0.000000  31.518823  ...   0.000000  15.034488   \n",
      "11    36.325173   0.000000   0.000000  36.831651  ...   8.816271   4.736363   \n",
      "12    26.471023   0.000000   0.000000  39.778615  ...   0.000000  14.252345   \n",
      "13    42.172147  10.559143   7.318216  19.045834  ...   1.536469  19.284731   \n",
      "14    32.131190   8.375252   0.000000  25.769071  ...   7.341624  14.752431   \n",
      "15    51.908341   8.897444   3.245168  36.120403  ...   0.000000  38.125425   \n",
      "16    29.510518  16.612282   0.000000  24.700598  ...   2.491167  14.258565   \n",
      "17    36.651121   7.383970   0.000000  44.773479  ...   2.860599  26.084234   \n",
      "18    27.925195  24.273771   0.000000  23.039533  ...   0.725449   0.000000   \n",
      "19    27.891778   0.000000   0.000000  34.002083  ...   6.751413  11.150303   \n",
      "20    17.395813   0.000000   0.000000  11.116550  ...   2.888578  23.275499   \n",
      "21    22.301599  19.320293   0.000000  27.543710  ...   0.000000  24.007239   \n",
      "22    21.587101  17.851547   0.000000  21.746151  ...  12.737200  27.440866   \n",
      "23    23.709984  14.883667   0.000000  33.107521  ...   3.938674  36.426266   \n",
      "24    29.708086  22.742133   0.000000  37.725845  ...   0.000000  22.046767   \n",
      "25    30.235825   4.255047   0.000000  26.690446  ...   0.000000  35.862150   \n",
      "26    25.918242  25.447402   0.000000  14.266611  ...   0.000000  47.437139   \n",
      "27    28.459396  29.610378   0.000000  50.132373  ...   0.000000  36.277945   \n",
      "28    38.874517  22.601449   0.000000  33.561759  ...   0.000000  29.070403   \n",
      "29    31.017825  21.766361   0.000000  27.425346  ...   3.894236  11.285954   \n",
      "...         ...        ...        ...        ...  ...        ...        ...   \n",
      "2946  16.621054  16.076322   0.000000  11.406278  ...   0.000000   9.345173   \n",
      "2947  20.332373  20.779564   4.914102   6.002957  ...   0.000000  23.526664   \n",
      "2948  10.166654   0.548397   0.000000   0.508297  ...   1.438116  10.878190   \n",
      "2949  18.220014  10.257785   0.000000   7.386936  ...   1.742917  10.012569   \n",
      "2950  20.167706  10.898557   4.130814   7.967307  ...   1.510360  15.547205   \n",
      "2951  21.556030  14.203493   4.529964  11.716027  ...   1.532794  17.600544   \n",
      "2952  18.924204  23.254352   0.578904  10.899913  ...   0.000000  16.460960   \n",
      "2953  15.332775   0.750467   0.000000   2.450325  ...   0.000000   7.781984   \n",
      "2954  15.669264  17.051805   1.283620  21.034440  ...   2.356488  10.683835   \n",
      "2955  18.567192  20.202813   0.000000  11.974987  ...   0.183038  12.470673   \n",
      "2956  10.783107  14.458228   5.365179  19.067691  ...   6.014137   5.274705   \n",
      "2957  18.530033  19.199612  20.599443  12.775609  ...   2.521016  10.053292   \n",
      "2958  16.016819  22.607717   0.000000  22.509989  ...   0.846702   7.677273   \n",
      "2959  19.582120  12.469384   0.000000   5.861398  ...   2.295160  18.952776   \n",
      "2960  22.171771  16.004300   0.000000   8.106583  ...   0.000000  18.136384   \n",
      "2961  21.443091  17.334934   4.503638   5.556081  ...   9.450010  12.695900   \n",
      "2962  23.441071  21.412342   6.674789  16.785712  ...   0.000000  18.576055   \n",
      "2963  21.795302   6.274750   0.000000   5.124548  ...   1.799690  19.570846   \n",
      "2964  23.759857  10.202150   7.289540  20.279794  ...   3.150157  15.732537   \n",
      "2965  18.442424  18.924473   0.000000  17.888589  ...   0.000000  12.109941   \n",
      "2966  19.474964  15.931830   0.000000   6.037346  ...   0.000000  15.671586   \n",
      "2967  29.395769  15.423292   0.000000   1.334491  ...   7.010127  12.289426   \n",
      "2968  11.817172  25.411707   0.000000   7.466439  ...   0.737591  11.044343   \n",
      "2969  22.074365  15.854219   0.000000   7.530802  ...   3.303066  11.018375   \n",
      "2970  17.397767   6.669475   0.000000   3.884560  ...   2.290444  13.209082   \n",
      "2971  19.888889   6.678460   0.000000  12.145205  ...   1.879424  12.670002   \n",
      "2972  19.072223   3.387923   0.000000   0.000000  ...   0.000000  19.371605   \n",
      "2973  15.410514  13.703730   0.000000   4.225793  ...   0.058773   9.656911   \n",
      "2974  17.512098  18.424676   0.000000   2.560759  ...   0.000000   9.801162   \n",
      "2975   7.570859   0.000000   0.603126   3.032373  ...   0.000000  12.359037   \n",
      "\n",
      "         116098     116099     116100    116157     116219     116220  \\\n",
      "0     31.126936  45.680952  18.801449  0.000000  22.791096  16.589562   \n",
      "1     29.400996  47.128457  18.081014  0.000000  22.492316  39.614822   \n",
      "2     30.198482  44.345346  17.947455  0.000000  22.289410  33.331948   \n",
      "3     60.259055  57.122521  35.479929  0.000000  40.244356  12.942474   \n",
      "4     66.134278  66.451325  37.001600  0.000000  37.958725  38.225794   \n",
      "5     65.929097  66.349569  48.915435  0.000000  50.373773  24.276129   \n",
      "6     68.030431  71.886087  46.005796  0.000000  49.722241   0.000000   \n",
      "7     68.634745  73.103156  56.698859  0.000000  51.135885  27.220163   \n",
      "8     70.592116  60.747958  58.174526  0.000000  57.160444  23.119363   \n",
      "9     64.980035  69.594903  52.444499  0.000000  51.360643  36.325469   \n",
      "10    68.788526  69.007137  58.214619  0.000000  58.809694  30.052202   \n",
      "11    65.816174  72.206940  52.140354  0.000000  59.963354  26.341642   \n",
      "12    67.929099  66.163011  59.177468  0.000000  63.581915  24.927127   \n",
      "13    66.100885  72.714835  59.859032  0.000000  59.160402  40.284147   \n",
      "14    67.667761  68.202443  59.104579  0.000000  58.480160  34.665834   \n",
      "15    68.621476  69.453213  60.992506  0.000000  62.027281  43.359989   \n",
      "16    68.291143  68.354823  59.714517  0.000000  62.954463  43.632490   \n",
      "17    65.407774  71.312358  51.137745  0.000000  58.018560  32.251570   \n",
      "18    67.674847  73.182116  58.340013  0.000000  64.439813  34.048774   \n",
      "19    70.993967  70.873641  62.416882  0.000000  61.103834  18.805094   \n",
      "20    69.600229  72.078667  57.909412  6.701554  61.983545  22.334514   \n",
      "21    69.941719  69.263506  60.122118  0.000000  63.982623  33.036327   \n",
      "22    68.173481  67.246559  44.310628  0.000000  62.433895  31.826195   \n",
      "23    74.798464  73.245167  65.568166  0.000000  53.040289  31.590351   \n",
      "24    70.751899  64.776135  61.661042  1.802561  62.753117   9.259704   \n",
      "25    69.534026  71.664232  62.448415  0.000000  57.813226  25.087152   \n",
      "26    71.973866  70.020455  68.500494  0.000000  66.173209  54.375801   \n",
      "27    71.913695  66.090129  66.616435  0.000000  64.483602  51.170332   \n",
      "28    70.402014  66.678358  50.324825  0.000000  62.840546  47.793719   \n",
      "29    71.731718  66.003012  15.793436  0.000000  59.481418  35.213490   \n",
      "...         ...        ...        ...       ...        ...        ...   \n",
      "2946  37.698755  46.348432  22.095332  0.038467  27.841142   4.972559   \n",
      "2947  42.109588  48.671108  21.874720  0.000000  31.934600   5.136800   \n",
      "2948  35.645960  44.343302  23.498465  0.000000  30.949700   1.280021   \n",
      "2949  34.260596  37.677862  20.907736  0.000000  27.324777   8.351562   \n",
      "2950  34.192267  38.770371  25.385398  0.082594  28.387821  15.862299   \n",
      "2951  34.483287  34.561348  22.088021  0.188568  21.321251  22.688723   \n",
      "2952  33.738771  41.084354  23.877117  1.514338  25.379574  16.285831   \n",
      "2953  37.859767  43.016123  23.172915  0.000000  24.860740  11.838873   \n",
      "2954  35.396790  41.690636  23.696140  0.000000  25.700716  21.654983   \n",
      "2955  35.334183  47.698038  19.111376  0.000000  20.146626  12.261926   \n",
      "2956  36.788607  38.790310  24.760289  0.000000  29.946767  28.026876   \n",
      "2957  42.467836  50.668277  28.074623  0.008285  28.768498   6.275687   \n",
      "2958  35.008602  41.272904  26.981336  0.000000  35.262194  29.381163   \n",
      "2959  45.717488  52.763768  24.494698  0.489733  28.631877  24.373179   \n",
      "2960  38.542626  40.700446  25.678615  0.000000  28.849982  19.085754   \n",
      "2961  39.000975  49.249257  29.728343  0.011392  28.671993  24.607453   \n",
      "2962  39.461325  47.809948  31.135147  3.316910  36.147069  30.515198   \n",
      "2963  37.448850  47.315214  24.331721  2.348221  26.926014  18.197321   \n",
      "2964  39.832593  44.922287  27.598128  0.216922  29.565925  19.243576   \n",
      "2965  35.477865  46.270300  24.320381  0.012342  27.235592  24.085565   \n",
      "2966  38.827909  47.911611  22.792098  0.000000  28.698006  23.857062   \n",
      "2967  39.723126  48.089489  25.060553  5.272592  33.668220   7.470575   \n",
      "2968  37.766642  45.750956  24.560689  0.431034  29.747896  16.170493   \n",
      "2969  31.164363  43.177665  21.279160  0.107526  31.910840  14.589180   \n",
      "2970  42.762818  48.637535  23.263640  2.119708  30.360498  17.211133   \n",
      "2971  34.848367  42.262780  25.921609  0.004418  29.229571  16.286234   \n",
      "2972  38.794024  45.165799  26.114714  0.000000  25.117042   5.176496   \n",
      "2973  37.280788  46.780534  27.677188  0.000000  28.269513  11.758752   \n",
      "2974  39.112511  38.064220  26.452214  0.000000  26.609735  14.093485   \n",
      "2975  45.995439  45.785918  27.200510  0.854993  17.063615  16.468485   \n",
      "\n",
      "         116221     117189  \n",
      "0     32.590224   0.000000  \n",
      "1     32.588299   0.000000  \n",
      "2     37.471168   0.000000  \n",
      "3     47.140562  39.697219  \n",
      "4     39.550687   0.000000  \n",
      "5     44.648038   0.000000  \n",
      "6     38.623554  11.761946  \n",
      "7     35.115407   0.000000  \n",
      "8     32.632697   0.000000  \n",
      "9     43.623113   0.000000  \n",
      "10    35.687806   0.000000  \n",
      "11    42.476165   0.000000  \n",
      "12    34.540010   0.000000  \n",
      "13    36.533095   0.000000  \n",
      "14    37.561942   7.188408  \n",
      "15    45.600814   0.000000  \n",
      "16    36.047202   0.000000  \n",
      "17    48.167841   0.000000  \n",
      "18    43.047044   0.000000  \n",
      "19    41.153677   0.000000  \n",
      "20    34.789538   0.000000  \n",
      "21    43.543865   0.000000  \n",
      "22    42.535417   0.000000  \n",
      "23    51.173023   9.211282  \n",
      "24    39.690650   0.000000  \n",
      "25    41.027179   0.000000  \n",
      "26    40.371523   0.000000  \n",
      "27    36.336421   0.000000  \n",
      "28    39.679429   0.000000  \n",
      "29    37.998813   0.000000  \n",
      "...         ...        ...  \n",
      "2946  13.806343   3.208666  \n",
      "2947  14.615504   0.000000  \n",
      "2948   7.805262   2.934000  \n",
      "2949  15.666566   0.000000  \n",
      "2950  11.369534   3.220515  \n",
      "2951  13.741226   0.000000  \n",
      "2952  13.299441   0.000000  \n",
      "2953  15.991623   0.000000  \n",
      "2954  11.234242   0.000000  \n",
      "2955  14.298898   0.000000  \n",
      "2956  17.569062   0.000000  \n",
      "2957  15.170356   0.000000  \n",
      "2958  12.867905   0.000000  \n",
      "2959  14.427421   1.776432  \n",
      "2960  14.088725   0.000000  \n",
      "2961  12.329245   0.000000  \n",
      "2962  14.083769   0.000000  \n",
      "2963  16.804706   8.421107  \n",
      "2964  16.205415   0.000000  \n",
      "2965  15.549589   0.000000  \n",
      "2966  10.890544   0.000000  \n",
      "2967  16.232275   0.000000  \n",
      "2968  13.814595   0.000000  \n",
      "2969  15.200403   0.000000  \n",
      "2970  12.847533   0.000000  \n",
      "2971  17.721697   0.000000  \n",
      "2972  15.243260   0.000000  \n",
      "2973  17.273817   0.000000  \n",
      "2974  14.656461   0.000000  \n",
      "2975  15.950042   0.000000  \n",
      "\n",
      "[2976 rows x 156 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data) # Time sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(adj) #Adjacency Matrix of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2976, 156)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "time_len=data.shape[0] # Time sequence length\n",
    "num_nodes=data.shape[1] #Number of Roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the variables\n",
    "output_dim=pre_len=1\n",
    "seq_len=4\n",
    "num_units=100\n",
    "train_rate=0.8\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0980221  0.21353212 0.23823702 ... 0.19194394 0.37707424 0.        ]\n",
      " [0.09032986 0.18181142 0.31845367 ... 0.45834997 0.37705195 0.        ]\n",
      " [0.10192686 0.10389599 0.23464748 ... 0.3856561  0.43354756 0.        ]\n",
      " ...\n",
      " [0.37947276 0.141638   0.10453826 ... 0.13605069 0.1998609  0.        ]\n",
      " [0.39722532 0.18600048 0.121989   ... 0.16306393 0.16957766 0.        ]\n",
      " [0.38101357 0.14001252 0.10420388 ... 0.19054307 0.1845446  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Normalization : Traffic Speed Data\n",
    "\n",
    "data1 =np.mat(data,dtype=np.float32)\n",
    "\n",
    "max_value = np.max(data1)\n",
    "data1  = data1/max_value\n",
    "print(data1)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2380, 156) -----> (596, 156)\n",
      "Train Test Split Details :\n",
      "Train x ---->  2375\n",
      "Train y ---->  2375\n",
      "(2375, 4, 156)\n",
      "Test x ---->  591\n",
      "Test y ---->  591\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data, time_len, rate, seq_len, pre_len):\n",
    "    train_size = int(time_len * rate) #2976 *0.8 =2380\n",
    "    train_data = data[0:train_size] #  [0:2380]\n",
    "    test_data = data[train_size:time_len] #[2380:2976]\n",
    "    print(train_data.shape,'----->',test_data.shape)\n",
    "\n",
    "    trainX, trainY, testX, testY = [], [], [], []\n",
    "    for i in range(len(train_data) - seq_len - pre_len): #(2380-4-1)=2375\n",
    "        a = train_data[i: i + seq_len + pre_len] #[0:0+4+1] =[0:5]\n",
    "        trainX.append(a[0 : seq_len]) #a[0:4] 4 time * 156 roads\n",
    "        trainY.append(a[seq_len : seq_len + pre_len]) #a[4:4+1] 1 time*156 \n",
    "    for i in range(len(test_data) - seq_len -pre_len):\n",
    "        b = test_data[i: i + seq_len + pre_len]\n",
    "        testX.append(b[0 : seq_len])\n",
    "        testY.append(b[seq_len : seq_len + pre_len])\n",
    "      \n",
    "    trainX1 = np.array(trainX) \n",
    "    trainY1 = np.array(trainY)\n",
    "    testX1 = np.array(testX)\n",
    "    testY1 = np.array(testY)\n",
    "    return trainX1, trainY1, testX1, testY1\n",
    "\n",
    "trainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len)\n",
    "\n",
    "totalbatch = int(trainX.shape[0]/batch_size)\n",
    "training_data_count = len(trainX)  \n",
    "print('Train Test Split Details :')\n",
    "print('Train x ----> ',len(trainX))\n",
    "print('Train y ----> ',len(trainY))\n",
    "print(trainX.shape)\n",
    "print('Test x ----> ',len(testX))\n",
    "print('Test y ----> ',len(testY))\n",
    "#print('\\nTrain Sample Details :')\n",
    "#print(trainX[0],'--->',trainY[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "        init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "        initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "        return tf.Variable(initial,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stgcnCell(RNNCell):\n",
    "    \"\"\"Temporal Graph Convolutional Network \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, num_units, adj, num_nodes, input_size=None,\n",
    "                 act=tf.nn.tanh, reuse=None):\n",
    "\n",
    "        super(stgcnCell, self).__init__(_reuse=reuse)\n",
    "        self._act = act\n",
    "        self._nodes = num_nodes\n",
    "        self._units = num_units\n",
    "        self._adj = []\n",
    "        self._adj.append(self.calculate_laplacian(adj))\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_sparse_matrix(L):\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
    "        return tf.sparse_reorder(L)\n",
    "\n",
    "    def calculate_laplacian(self,adj, lambda_max=1):  \n",
    "        adj = self.normalized_adj(adj + sp.eye(adj.shape[0])) # normalisation(self identity matrix + adj)\n",
    "        adj = sp.csr_matrix(adj) #compressed sparse matrix\n",
    "        adj = adj.astype(np.float32)\n",
    "        return self.sparse_to_tuple(adj)\n",
    "    \n",
    "    def normalized_adj(self,adj):\n",
    "        adj = sp.coo_matrix(adj)\n",
    "        degree = np.array(adj.sum(1)) # Degree Matrix row wise sum\n",
    "        d_inv_sqrt = np.power(degree, -0.5).flatten() # D inv = Degree ^-0.5 \n",
    "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt) #substitution of the 1D array degree in a 2D matrix diagonals\n",
    "        normalized_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo() # norm= D^-0.5 * adj * D^-0.5\n",
    "        normalized_adj = normalized_adj.astype(np.float32) \n",
    "        return normalized_adj\n",
    "    \n",
    "    def sparse_to_tuple(self,mx):\n",
    "        mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose() #coordinate stacking row and column wise and transpose\n",
    "        L = tf.SparseTensor(coords, mx.data, mx.shape) # mx.shape= (156,156)\n",
    "        #print('shape ---->',mx.shape)\n",
    "        return tf.sparse_reorder(L) #row major ordering\n",
    "        \n",
    "    def init_state(self,batch_size):       \n",
    "        state = tf.zeros(shape=[batch_size, self._num_nodes*self._num_units], dtype=tf.float32)\n",
    "        return state  \n",
    "               \n",
    "    @staticmethod\n",
    "    def _concat(x, x_):\n",
    "        x_ = tf.expand_dims(x_, 0)\n",
    "        return tf.concat([x, x_], axis=0)   \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._nodes * self._units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "\n",
    "        with tf.variable_scope(scope or \"tgcn\"):\n",
    "            with tf.variable_scope(\"gates\"):  \n",
    "                value = tf.nn.sigmoid(\n",
    "                    self._gc(inputs, state, 2 * self._units, bias=1.0, scope=scope)) #ut (or) rt = sigma(Wu [f(A;Xt); h{t-1}] + bu)\n",
    "                r, u = tf.split(value=value, num_or_size_splits=2, axis=1)\n",
    "            with tf.variable_scope(\"candidate\"):\n",
    "                r_state = r * state #r* h{t-1}\n",
    "                c = self._act(self._gc(inputs, r_state, self._units, scope=scope))#ct = tanh(Wc [f(A;Xt); r_state] + bc) \n",
    "            new_h = u * state + (1 - u) * c #ht = ut * h{t-1} + (1 - u{t}) * ct\n",
    "        return new_h, new_h\n",
    "\n",
    "\n",
    "    def _gc(self, inputs, state, output_size, bias=0.0, scope=None):\n",
    "        ## inputs:(-1,num_nodes)\n",
    "        inputs = tf.expand_dims(inputs, 2)#None,156,None\n",
    "        ## state:(batch,num_node,gru_units)\n",
    "        state = tf.reshape(state, (-1, self._nodes, self._units)) #32,156,64\n",
    "        ## concat\n",
    "        x_s = tf.concat([inputs, state], axis=2) #32,156,65\n",
    "        input_size = x_s.get_shape()[2].value #65\n",
    "        ## (num_node,input_size,-1)\n",
    "        x0 = tf.transpose(x_s, perm=[1, 2, 0]) #156,65,32\n",
    "        x0 = tf.reshape(x0, shape=[self._nodes, -1]) #156,65*32\n",
    "        scope = tf.get_variable_scope()\n",
    "        with tf.variable_scope(scope):\n",
    "            for m in self._adj:#1,156\n",
    "                x1 = tf.sparse_tensor_dense_matmul(m, x0) #1,65*32\n",
    "#                print(x1)\n",
    "            x = tf.reshape(x1, shape=[self._nodes, input_size,-1]) #156,65,32\n",
    "            x = tf.transpose(x,perm=[2,0,1]) #32,156,65\n",
    "            x = tf.reshape(x, shape=[-1, input_size]) #156*32,65\n",
    "            weights = tf.get_variable( # 65,64\n",
    "                'weights', [input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            x = tf.matmul(x, weights)  # (batch_size * self._nodes, output_size) \n",
    "            biases = tf.get_variable( #64\n",
    "                \"biases\", [output_size], initializer=tf.constant_initializer(bias, dtype=tf.float32))\n",
    "            x = tf.nn.bias_add(x, biases) #biases added\n",
    "            x = tf.reshape(x, shape=[-1, self._nodes, output_size]) #32,156,64\n",
    "            x = tf.reshape(x, shape=[-1, self._nodes * output_size])#32,156*64\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tf.convert_to_tensor(trainX, np.float32)\n",
    "#labels = tf.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\n",
    "#g=stgcnCell(num_units, adj, inputs, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STGCN(_X, _weights, _biases):\n",
    "    ###\n",
    "    cell_1 = stgcnCell(num_units, adj, num_nodes=num_nodes)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True) #stack rnn cells\n",
    "    _X = tf.unstack(_X, axis=1) # 4 tensorflow arrays of shape None,156 (seq_len=4)\n",
    "    outputs, states = tf.nn.static_rnn(cell, _X, dtype=tf.float32) #Creates a recurrent neural network specified by RNNCell cell\n",
    "    #4 outputs and 1 state None,9984(156*64)\n",
    "    m = []\n",
    "    for i in outputs:\n",
    "        o = tf.reshape(i,shape=[-1,num_nodes,num_units])#None,156,64\n",
    "        o = tf.reshape(o,shape=[-1,num_units])#None*156,64\n",
    "        m.append(o) #4 objects\n",
    "    last_output = m[-1] #last one\n",
    "    output = tf.matmul(last_output, _weights['out']) + _biases['out'] #multiply with weights and add bias None*156,1+len(1)=156,1\n",
    "    output = tf.reshape(output,shape=[-1,num_nodes,pre_len]) # None,156,1\n",
    "    output = tf.transpose(output, perm=[0,2,1])#None,1,156\n",
    "    output = tf.reshape(output, shape=[-1,num_nodes]) #None*1,156\n",
    "    return output, m, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Lema Labs ML Workshop x64\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "WARNING:tensorflow:From <ipython-input-12-dfd1196d70e3>:4: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-dfd1196d70e3>:6: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.placeholder(tf.float32, shape=[None, seq_len, num_nodes])\n",
    "labels = tf.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_units, pre_len], mean=1.0), name='weight_o')} #64,1\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([pre_len]),name='bias_o')} #1\n",
    "print(type(inputs))\n",
    "pred,ttts,ttto = STGCN(inputs, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_loss = 0.0015\n",
    "Lreg = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "label = tf.reshape(labels, [-1,num_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.l2_loss(y_pred-label) + Lreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = tf.sqrt(tf.reduce_mean(tf.square(y_pred-label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = tf.global_variables()\n",
    "training_epoch=1000\n",
    "saver = tf.train.Saver(tf.global_variables()) #\n",
    "#sess = tf.Session()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "out = 'out/%s'%(\"STGCN\")\n",
    "#out = 'out/%s_%s'%(model_name,'perturbation')\n",
    "path1 = '%s_%s_lr%r_batch%r_unit%r_seq%r_pre%r_epoch%r'%(\"STGCN\",\"sz\",lr,batch_size,num_units,seq_len,pre_len,training_epoch)\n",
    "path = os.path.join(out,path1)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(a,b):\n",
    "    rmse = math.sqrt(mean_squared_error(a,b))\n",
    "    mae = mean_absolute_error(a, b)\n",
    "    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n",
    "    r2 = 1-((a-b)**2).sum()/((a-a.mean())**2).sum()\n",
    "    var = 1-(np.var(a-b))/np.var(a)\n",
    "    return rmse, mae, 1-F_norm, r2, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []\n",
    "test_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(a,b):  \n",
    "    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n",
    "    train_acc=1-F_norm\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Accuracy ---->  0.6102120280265808\n",
      "Iter:0 train_rmse:5.625 test_loss:222.4 test_rmse:0.06938 test_acc:0.5823\n",
      "Epoch  1\n",
      "Accuracy ---->  0.6152190566062927\n",
      "Iter:1 train_rmse:5.553 test_loss:215.9 test_rmse:0.06836 test_acc:0.5884\n",
      "Epoch  2\n",
      "Accuracy ---->  0.6161063313484192\n",
      "Iter:2 train_rmse:5.54 test_loss:214.6 test_rmse:0.06815 test_acc:0.5897\n",
      "Epoch  3\n",
      "Accuracy ---->  0.6164804697036743\n",
      "Iter:3 train_rmse:5.534 test_loss:213.9 test_rmse:0.06805 test_acc:0.5903\n",
      "Epoch  4\n",
      "Accuracy ---->  0.6168162524700165\n",
      "Iter:4 train_rmse:5.53 test_loss:213.4 test_rmse:0.06797 test_acc:0.5908\n",
      "Epoch  5\n",
      "Accuracy ---->  0.6172611713409424\n",
      "Iter:5 train_rmse:5.523 test_loss:212.9 test_rmse:0.06789 test_acc:0.5913\n",
      "Epoch  6\n",
      "Accuracy ---->  0.6177465319633484\n",
      "Iter:6 train_rmse:5.516 test_loss:212.4 test_rmse:0.06781 test_acc:0.5918\n",
      "Epoch  7\n",
      "Accuracy ---->  0.6182535886764526\n",
      "Iter:7 train_rmse:5.509 test_loss:211.8 test_rmse:0.06772 test_acc:0.5923\n",
      "Epoch  8\n",
      "Accuracy ---->  0.6187959909439087\n",
      "Iter:8 train_rmse:5.501 test_loss:211.3 test_rmse:0.06763 test_acc:0.5928\n",
      "Epoch  9\n",
      "Accuracy ---->  0.6194313764572144\n",
      "Iter:9 train_rmse:5.492 test_loss:210.7 test_rmse:0.06753 test_acc:0.5934\n",
      "Epoch  10\n",
      "Accuracy ---->  0.620172917842865\n",
      "Iter:10 train_rmse:5.481 test_loss:210.0 test_rmse:0.06743 test_acc:0.594\n",
      "Epoch  11\n",
      "Accuracy ---->  0.6210081875324249\n",
      "Iter:11 train_rmse:5.469 test_loss:209.3 test_rmse:0.06732 test_acc:0.5947\n",
      "Epoch  12\n",
      "Accuracy ---->  0.6218964457511902\n",
      "Iter:12 train_rmse:5.456 test_loss:208.5 test_rmse:0.06719 test_acc:0.5955\n",
      "Epoch  13\n",
      "Accuracy ---->  0.6228380799293518\n",
      "Iter:13 train_rmse:5.443 test_loss:207.7 test_rmse:0.06706 test_acc:0.5963\n",
      "Epoch  14\n",
      "Accuracy ---->  0.6239273548126221\n",
      "Iter:14 train_rmse:5.427 test_loss:206.8 test_rmse:0.06691 test_acc:0.5972\n",
      "Epoch  15\n",
      "Accuracy ---->  0.6251750886440277\n",
      "Iter:15 train_rmse:5.409 test_loss:205.8 test_rmse:0.06675 test_acc:0.5981\n",
      "Epoch  16\n",
      "Accuracy ---->  0.6264145970344543\n",
      "Iter:16 train_rmse:5.391 test_loss:204.8 test_rmse:0.06659 test_acc:0.5991\n",
      "Epoch  17\n",
      "Accuracy ---->  0.6275452375411987\n",
      "Iter:17 train_rmse:5.375 test_loss:203.9 test_rmse:0.06644 test_acc:0.6\n",
      "Epoch  18\n",
      "Accuracy ---->  0.6287660598754883\n",
      "Iter:18 train_rmse:5.357 test_loss:203.1 test_rmse:0.0663 test_acc:0.6008\n",
      "Epoch  19\n",
      "Accuracy ---->  0.6300086081027985\n",
      "Iter:19 train_rmse:5.339 test_loss:202.3 test_rmse:0.06618 test_acc:0.6016\n",
      "Epoch  20\n",
      "Accuracy ---->  0.6310910582542419\n",
      "Iter:20 train_rmse:5.324 test_loss:201.5 test_rmse:0.06605 test_acc:0.6023\n",
      "Epoch  21\n",
      "Accuracy ---->  0.6321455240249634\n",
      "Iter:21 train_rmse:5.308 test_loss:200.9 test_rmse:0.06594 test_acc:0.603\n",
      "Epoch  22\n",
      "Accuracy ---->  0.6331858038902283\n",
      "Iter:22 train_rmse:5.293 test_loss:200.3 test_rmse:0.06585 test_acc:0.6035\n",
      "Epoch  23\n",
      "Accuracy ---->  0.6341236233711243\n",
      "Iter:23 train_rmse:5.28 test_loss:199.8 test_rmse:0.06577 test_acc:0.6041\n",
      "Epoch  24\n",
      "Accuracy ---->  0.6349226534366608\n",
      "Iter:24 train_rmse:5.268 test_loss:199.2 test_rmse:0.06567 test_acc:0.6046\n",
      "Epoch  25\n",
      "Accuracy ---->  0.6356009542942047\n",
      "Iter:25 train_rmse:5.259 test_loss:198.8 test_rmse:0.0656 test_acc:0.6051\n",
      "Epoch  26\n",
      "Accuracy ---->  0.6361969113349915\n",
      "Iter:26 train_rmse:5.25 test_loss:198.4 test_rmse:0.06554 test_acc:0.6054\n",
      "Epoch  27\n",
      "Accuracy ---->  0.6367923319339752\n",
      "Iter:27 train_rmse:5.241 test_loss:198.1 test_rmse:0.06549 test_acc:0.6057\n",
      "Epoch  28\n",
      "Accuracy ---->  0.6372774243354797\n",
      "Iter:28 train_rmse:5.234 test_loss:197.8 test_rmse:0.06543 test_acc:0.6061\n",
      "Epoch  29\n",
      "Accuracy ---->  0.6377166211605072\n",
      "Iter:29 train_rmse:5.228 test_loss:197.4 test_rmse:0.06537 test_acc:0.6065\n",
      "Epoch  30\n",
      "Accuracy ---->  0.638148158788681\n",
      "Iter:30 train_rmse:5.222 test_loss:197.0 test_rmse:0.06531 test_acc:0.6068\n",
      "Epoch  31\n",
      "Accuracy ---->  0.6385451853275299\n",
      "Iter:31 train_rmse:5.216 test_loss:196.7 test_rmse:0.06525 test_acc:0.6071\n",
      "Epoch  32\n",
      "Accuracy ---->  0.6389427185058594\n",
      "Iter:32 train_rmse:5.21 test_loss:196.4 test_rmse:0.0652 test_acc:0.6075\n",
      "Epoch  33\n",
      "Accuracy ---->  0.6393320262432098\n",
      "Iter:33 train_rmse:5.205 test_loss:196.0 test_rmse:0.06514 test_acc:0.6078\n",
      "Epoch  34\n",
      "Accuracy ---->  0.639725387096405\n",
      "Iter:34 train_rmse:5.199 test_loss:195.7 test_rmse:0.06508 test_acc:0.6082\n",
      "Epoch  35\n",
      "Accuracy ---->  0.6401159763336182\n",
      "Iter:35 train_rmse:5.193 test_loss:195.3 test_rmse:0.06502 test_acc:0.6085\n",
      "Epoch  36\n",
      "Accuracy ---->  0.6405028402805328\n",
      "Iter:36 train_rmse:5.188 test_loss:195.0 test_rmse:0.06497 test_acc:0.6089\n",
      "Epoch  37\n",
      "Accuracy ---->  0.6409010887145996\n",
      "Iter:37 train_rmse:5.182 test_loss:194.7 test_rmse:0.06491 test_acc:0.6092\n",
      "Epoch  38\n",
      "Accuracy ---->  0.641313761472702\n",
      "Iter:38 train_rmse:5.176 test_loss:194.3 test_rmse:0.06485 test_acc:0.6096\n",
      "Epoch  39\n",
      "Accuracy ---->  0.6417039930820465\n",
      "Iter:39 train_rmse:5.17 test_loss:194.0 test_rmse:0.06479 test_acc:0.6099\n",
      "Epoch  40\n",
      "Accuracy ---->  0.6420914828777313\n",
      "Iter:40 train_rmse:5.165 test_loss:193.6 test_rmse:0.06473 test_acc:0.6103\n",
      "Epoch  41\n",
      "Accuracy ---->  0.6424823701381683\n",
      "Iter:41 train_rmse:5.159 test_loss:193.3 test_rmse:0.06467 test_acc:0.6106\n",
      "Epoch  42\n",
      "Accuracy ---->  0.6428747773170471\n",
      "Iter:42 train_rmse:5.154 test_loss:193.0 test_rmse:0.06462 test_acc:0.6109\n",
      "Epoch  43\n",
      "Accuracy ---->  0.6432511806488037\n",
      "Iter:43 train_rmse:5.148 test_loss:192.7 test_rmse:0.06458 test_acc:0.6112\n",
      "Epoch  44\n",
      "Accuracy ---->  0.643584668636322\n",
      "Iter:44 train_rmse:5.143 test_loss:192.4 test_rmse:0.06452 test_acc:0.6115\n",
      "Epoch  45\n",
      "Accuracy ---->  0.6438791155815125\n",
      "Iter:45 train_rmse:5.139 test_loss:192.0 test_rmse:0.06446 test_acc:0.6119\n",
      "Epoch  46\n",
      "Accuracy ---->  0.6441605687141418\n",
      "Iter:46 train_rmse:5.135 test_loss:191.6 test_rmse:0.0644 test_acc:0.6123\n",
      "Epoch  47\n",
      "Accuracy ---->  0.6444509923458099\n",
      "Iter:47 train_rmse:5.131 test_loss:191.2 test_rmse:0.06433 test_acc:0.6127\n",
      "Epoch  48\n",
      "Accuracy ---->  0.6447644233703613\n",
      "Iter:48 train_rmse:5.126 test_loss:190.8 test_rmse:0.06426 test_acc:0.6131\n",
      "Epoch  49\n",
      "Accuracy ---->  0.6451109051704407\n",
      "Iter:49 train_rmse:5.121 test_loss:190.4 test_rmse:0.06419 test_acc:0.6135\n",
      "Epoch  50\n",
      "Accuracy ---->  0.6454989612102509\n",
      "Iter:50 train_rmse:5.116 test_loss:190.0 test_rmse:0.06412 test_acc:0.614\n",
      "Epoch  51\n",
      "Accuracy ---->  0.6459360718727112\n",
      "Iter:51 train_rmse:5.109 test_loss:189.6 test_rmse:0.06405 test_acc:0.6144\n",
      "Epoch  52\n",
      "Accuracy ---->  0.6464264094829559\n",
      "Iter:52 train_rmse:5.102 test_loss:189.2 test_rmse:0.06398 test_acc:0.6148\n",
      "Epoch  53\n",
      "Accuracy ---->  0.6469665467739105\n",
      "Iter:53 train_rmse:5.095 test_loss:188.8 test_rmse:0.06391 test_acc:0.6152\n",
      "Epoch  54\n",
      "Accuracy ---->  0.6475429534912109\n",
      "Iter:54 train_rmse:5.086 test_loss:188.4 test_rmse:0.06383 test_acc:0.6157\n",
      "Epoch  55\n",
      "Accuracy ---->  0.6481342911720276\n",
      "Iter:55 train_rmse:5.078 test_loss:187.9 test_rmse:0.06375 test_acc:0.6162\n",
      "Epoch  56\n",
      "Accuracy ---->  0.6487218141555786\n",
      "Iter:56 train_rmse:5.069 test_loss:187.4 test_rmse:0.06367 test_acc:0.6167\n",
      "Epoch  57\n",
      "Accuracy ---->  0.6492996513843536\n",
      "Iter:57 train_rmse:5.061 test_loss:186.9 test_rmse:0.06358 test_acc:0.6172\n",
      "Epoch  58\n",
      "Accuracy ---->  0.6498802602291107\n",
      "Iter:58 train_rmse:5.052 test_loss:186.3 test_rmse:0.06348 test_acc:0.6178\n",
      "Epoch  59\n",
      "Accuracy ---->  0.6504919826984406\n",
      "Iter:59 train_rmse:5.044 test_loss:185.7 test_rmse:0.06338 test_acc:0.6184\n",
      "Epoch  60\n",
      "Accuracy ---->  0.6511678397655487\n",
      "Iter:60 train_rmse:5.034 test_loss:185.1 test_rmse:0.06328 test_acc:0.619\n",
      "Epoch  61\n",
      "Accuracy ---->  0.6519272327423096\n",
      "Iter:61 train_rmse:5.023 test_loss:184.5 test_rmse:0.06317 test_acc:0.6197\n",
      "Epoch  62\n",
      "Accuracy ---->  0.6527584791183472\n",
      "Iter:62 train_rmse:5.011 test_loss:183.8 test_rmse:0.06305 test_acc:0.6204\n",
      "Epoch  63\n",
      "Accuracy ---->  0.6536190509796143\n",
      "Iter:63 train_rmse:4.999 test_loss:183.0 test_rmse:0.06292 test_acc:0.6212\n",
      "Epoch  64\n",
      "Accuracy ---->  0.6544608175754547\n",
      "Iter:64 train_rmse:4.986 test_loss:182.3 test_rmse:0.06279 test_acc:0.622\n",
      "Epoch  65\n",
      "Accuracy ---->  0.6552587449550629\n",
      "Iter:65 train_rmse:4.975 test_loss:181.5 test_rmse:0.06265 test_acc:0.6228\n",
      "Epoch  66\n",
      "Accuracy ---->  0.6560168862342834\n",
      "Iter:66 train_rmse:4.964 test_loss:180.7 test_rmse:0.06251 test_acc:0.6236\n",
      "Epoch  67\n",
      "Accuracy ---->  0.6567531526088715\n",
      "Iter:67 train_rmse:4.953 test_loss:179.9 test_rmse:0.06238 test_acc:0.6245\n",
      "Epoch  68\n",
      "Accuracy ---->  0.6574850082397461\n",
      "Iter:68 train_rmse:4.943 test_loss:179.1 test_rmse:0.06224 test_acc:0.6253\n",
      "Epoch  69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.6582269370555878\n",
      "Iter:69 train_rmse:4.932 test_loss:178.3 test_rmse:0.0621 test_acc:0.6261\n",
      "Epoch  70\n",
      "Accuracy ---->  0.6589884459972382\n",
      "Iter:70 train_rmse:4.921 test_loss:177.6 test_rmse:0.06196 test_acc:0.6269\n",
      "Epoch  71\n",
      "Accuracy ---->  0.6597726345062256\n",
      "Iter:71 train_rmse:4.91 test_loss:176.8 test_rmse:0.06183 test_acc:0.6277\n",
      "Epoch  72\n",
      "Accuracy ---->  0.6605752408504486\n",
      "Iter:72 train_rmse:4.898 test_loss:176.1 test_rmse:0.0617 test_acc:0.6285\n",
      "Epoch  73\n",
      "Accuracy ---->  0.661386102437973\n",
      "Iter:73 train_rmse:4.886 test_loss:175.3 test_rmse:0.06157 test_acc:0.6293\n",
      "Epoch  74\n",
      "Accuracy ---->  0.6621928811073303\n",
      "Iter:74 train_rmse:4.875 test_loss:174.6 test_rmse:0.06144 test_acc:0.6301\n",
      "Epoch  75\n",
      "Accuracy ---->  0.662984699010849\n",
      "Iter:75 train_rmse:4.863 test_loss:173.9 test_rmse:0.06132 test_acc:0.6308\n",
      "Epoch  76\n",
      "Accuracy ---->  0.6637539565563202\n",
      "Iter:76 train_rmse:4.852 test_loss:173.2 test_rmse:0.06119 test_acc:0.6316\n",
      "Epoch  77\n",
      "Accuracy ---->  0.6644959449768066\n",
      "Iter:77 train_rmse:4.842 test_loss:172.5 test_rmse:0.06107 test_acc:0.6323\n",
      "Epoch  78\n",
      "Accuracy ---->  0.6652094721794128\n",
      "Iter:78 train_rmse:4.831 test_loss:171.9 test_rmse:0.06095 test_acc:0.633\n",
      "Epoch  79\n",
      "Accuracy ---->  0.6658946871757507\n",
      "Iter:79 train_rmse:4.821 test_loss:171.2 test_rmse:0.06084 test_acc:0.6337\n",
      "Epoch  80\n",
      "Accuracy ---->  0.666552871465683\n",
      "Iter:80 train_rmse:4.812 test_loss:170.6 test_rmse:0.06073 test_acc:0.6344\n",
      "Epoch  81\n",
      "Accuracy ---->  0.6671858131885529\n",
      "Iter:81 train_rmse:4.803 test_loss:170.0 test_rmse:0.06062 test_acc:0.635\n",
      "Epoch  82\n",
      "Accuracy ---->  0.6677946746349335\n",
      "Iter:82 train_rmse:4.794 test_loss:169.4 test_rmse:0.06052 test_acc:0.6357\n",
      "Epoch  83\n",
      "Accuracy ---->  0.668380618095398\n",
      "Iter:83 train_rmse:4.785 test_loss:168.9 test_rmse:0.06042 test_acc:0.6363\n",
      "Epoch  84\n",
      "Accuracy ---->  0.6689447462558746\n",
      "Iter:84 train_rmse:4.777 test_loss:168.3 test_rmse:0.06032 test_acc:0.6368\n",
      "Epoch  85\n",
      "Accuracy ---->  0.6694877445697784\n",
      "Iter:85 train_rmse:4.77 test_loss:167.9 test_rmse:0.06023 test_acc:0.6374\n",
      "Epoch  86\n",
      "Accuracy ---->  0.6700102686882019\n",
      "Iter:86 train_rmse:4.762 test_loss:167.4 test_rmse:0.06015 test_acc:0.6379\n",
      "Epoch  87\n",
      "Accuracy ---->  0.6705129444599152\n",
      "Iter:87 train_rmse:4.755 test_loss:166.9 test_rmse:0.06007 test_acc:0.6383\n",
      "Epoch  88\n",
      "Accuracy ---->  0.6709967255592346\n",
      "Iter:88 train_rmse:4.748 test_loss:166.5 test_rmse:0.05999 test_acc:0.6388\n",
      "Epoch  89\n",
      "Accuracy ---->  0.6714624464511871\n",
      "Iter:89 train_rmse:4.741 test_loss:166.1 test_rmse:0.05992 test_acc:0.6392\n",
      "Epoch  90\n",
      "Accuracy ---->  0.6719110012054443\n",
      "Iter:90 train_rmse:4.735 test_loss:165.8 test_rmse:0.05985 test_acc:0.6396\n",
      "Epoch  91\n",
      "Accuracy ---->  0.6723432242870331\n",
      "Iter:91 train_rmse:4.728 test_loss:165.4 test_rmse:0.05979 test_acc:0.64\n",
      "Epoch  92\n",
      "Accuracy ---->  0.6727598607540131\n",
      "Iter:92 train_rmse:4.722 test_loss:165.1 test_rmse:0.05973 test_acc:0.6404\n",
      "Epoch  93\n",
      "Accuracy ---->  0.6731618344783783\n",
      "Iter:93 train_rmse:4.716 test_loss:164.7 test_rmse:0.05967 test_acc:0.6408\n",
      "Epoch  94\n",
      "Accuracy ---->  0.6735499501228333\n",
      "Iter:94 train_rmse:4.711 test_loss:164.4 test_rmse:0.05962 test_acc:0.6411\n",
      "Epoch  95\n",
      "Accuracy ---->  0.6739248931407928\n",
      "Iter:95 train_rmse:4.705 test_loss:164.2 test_rmse:0.05956 test_acc:0.6414\n",
      "Epoch  96\n",
      "Accuracy ---->  0.6742871403694153\n",
      "Iter:96 train_rmse:4.7 test_loss:163.9 test_rmse:0.05952 test_acc:0.6417\n",
      "Epoch  97\n",
      "Accuracy ---->  0.6746376752853394\n",
      "Iter:97 train_rmse:4.695 test_loss:163.6 test_rmse:0.05947 test_acc:0.642\n",
      "Epoch  98\n",
      "Accuracy ---->  0.6749777793884277\n",
      "Iter:98 train_rmse:4.69 test_loss:163.4 test_rmse:0.05943 test_acc:0.6422\n",
      "Epoch  99\n",
      "Accuracy ---->  0.6753092706203461\n",
      "Iter:99 train_rmse:4.686 test_loss:163.2 test_rmse:0.05939 test_acc:0.6425\n",
      "Epoch  100\n",
      "Accuracy ---->  0.6756348013877869\n",
      "Iter:100 train_rmse:4.681 test_loss:163.0 test_rmse:0.05935 test_acc:0.6427\n",
      "Epoch  101\n",
      "Accuracy ---->  0.6759573221206665\n",
      "Iter:101 train_rmse:4.676 test_loss:162.8 test_rmse:0.05931 test_acc:0.6429\n",
      "Epoch  102\n",
      "Accuracy ---->  0.676281064748764\n",
      "Iter:102 train_rmse:4.671 test_loss:162.6 test_rmse:0.05927 test_acc:0.6431\n",
      "Epoch  103\n",
      "Accuracy ---->  0.6766092777252197\n",
      "Iter:103 train_rmse:4.667 test_loss:162.4 test_rmse:0.05924 test_acc:0.6434\n",
      "Epoch  104\n",
      "Accuracy ---->  0.6769453585147858\n",
      "Iter:104 train_rmse:4.662 test_loss:162.2 test_rmse:0.0592 test_acc:0.6436\n",
      "Epoch  105\n",
      "Accuracy ---->  0.6772911250591278\n",
      "Iter:105 train_rmse:4.657 test_loss:162.0 test_rmse:0.05917 test_acc:0.6438\n",
      "Epoch  106\n",
      "Accuracy ---->  0.6776472628116608\n",
      "Iter:106 train_rmse:4.652 test_loss:161.8 test_rmse:0.05914 test_acc:0.644\n",
      "Epoch  107\n",
      "Accuracy ---->  0.6780128479003906\n",
      "Iter:107 train_rmse:4.646 test_loss:161.6 test_rmse:0.0591 test_acc:0.6442\n",
      "Epoch  108\n",
      "Accuracy ---->  0.6783857345581055\n",
      "Iter:108 train_rmse:4.641 test_loss:161.5 test_rmse:0.05907 test_acc:0.6444\n",
      "Epoch  109\n",
      "Accuracy ---->  0.6787626445293427\n",
      "Iter:109 train_rmse:4.636 test_loss:161.3 test_rmse:0.05904 test_acc:0.6446\n",
      "Epoch  110\n",
      "Accuracy ---->  0.6791408360004425\n",
      "Iter:110 train_rmse:4.63 test_loss:161.1 test_rmse:0.05901 test_acc:0.6448\n",
      "Epoch  111\n",
      "Accuracy ---->  0.679517537355423\n",
      "Iter:111 train_rmse:4.625 test_loss:160.9 test_rmse:0.05897 test_acc:0.645\n",
      "Epoch  112\n",
      "Accuracy ---->  0.6798923313617706\n",
      "Iter:112 train_rmse:4.619 test_loss:160.7 test_rmse:0.05894 test_acc:0.6452\n",
      "Epoch  113\n",
      "Accuracy ---->  0.6802667379379272\n",
      "Iter:113 train_rmse:4.614 test_loss:160.6 test_rmse:0.0589 test_acc:0.6454\n",
      "Epoch  114\n",
      "Accuracy ---->  0.6806440651416779\n",
      "Iter:114 train_rmse:4.609 test_loss:160.4 test_rmse:0.05887 test_acc:0.6456\n",
      "Epoch  115\n",
      "Accuracy ---->  0.6810289323329926\n",
      "Iter:115 train_rmse:4.603 test_loss:160.2 test_rmse:0.05883 test_acc:0.6458\n",
      "Epoch  116\n",
      "Accuracy ---->  0.6814246773719788\n",
      "Iter:116 train_rmse:4.597 test_loss:159.9 test_rmse:0.05879 test_acc:0.6461\n",
      "Epoch  117\n",
      "Accuracy ---->  0.681831955909729\n",
      "Iter:117 train_rmse:4.591 test_loss:159.7 test_rmse:0.05875 test_acc:0.6463\n",
      "Epoch  118\n",
      "Accuracy ---->  0.682248443365097\n",
      "Iter:118 train_rmse:4.585 test_loss:159.5 test_rmse:0.0587 test_acc:0.6466\n",
      "Epoch  119\n",
      "Accuracy ---->  0.6826691627502441\n",
      "Iter:119 train_rmse:4.579 test_loss:159.3 test_rmse:0.05866 test_acc:0.6468\n",
      "Epoch  120\n",
      "Accuracy ---->  0.6830875277519226\n",
      "Iter:120 train_rmse:4.573 test_loss:159.0 test_rmse:0.05862 test_acc:0.6471\n",
      "Epoch  121\n",
      "Accuracy ---->  0.6834968626499176\n",
      "Iter:121 train_rmse:4.567 test_loss:158.8 test_rmse:0.05858 test_acc:0.6473\n",
      "Epoch  122\n",
      "Accuracy ---->  0.6838912665843964\n",
      "Iter:122 train_rmse:4.562 test_loss:158.6 test_rmse:0.05853 test_acc:0.6476\n",
      "Epoch  123\n",
      "Accuracy ---->  0.6842665374279022\n",
      "Iter:123 train_rmse:4.556 test_loss:158.3 test_rmse:0.05849 test_acc:0.6478\n",
      "Epoch  124\n",
      "Accuracy ---->  0.6846196353435516\n",
      "Iter:124 train_rmse:4.551 test_loss:158.1 test_rmse:0.05845 test_acc:0.6481\n",
      "Epoch  125\n",
      "Accuracy ---->  0.6849486231803894\n",
      "Iter:125 train_rmse:4.546 test_loss:157.9 test_rmse:0.05841 test_acc:0.6484\n",
      "Epoch  126\n",
      "Accuracy ---->  0.6852524876594543\n",
      "Iter:126 train_rmse:4.542 test_loss:157.7 test_rmse:0.05837 test_acc:0.6486\n",
      "Epoch  127\n",
      "Accuracy ---->  0.6855306029319763\n",
      "Iter:127 train_rmse:4.538 test_loss:157.4 test_rmse:0.05833 test_acc:0.6489\n",
      "Epoch  128\n",
      "Accuracy ---->  0.6857823729515076\n",
      "Iter:128 train_rmse:4.534 test_loss:157.2 test_rmse:0.05828 test_acc:0.6491\n",
      "Epoch  129\n",
      "Accuracy ---->  0.6860071420669556\n",
      "Iter:129 train_rmse:4.531 test_loss:157.0 test_rmse:0.05824 test_acc:0.6494\n",
      "Epoch  130\n",
      "Accuracy ---->  0.686203807592392\n",
      "Iter:130 train_rmse:4.528 test_loss:156.8 test_rmse:0.0582 test_acc:0.6496\n",
      "Epoch  131\n",
      "Accuracy ---->  0.6863703429698944\n",
      "Iter:131 train_rmse:4.526 test_loss:156.6 test_rmse:0.05816 test_acc:0.6499\n",
      "Epoch  132\n",
      "Accuracy ---->  0.6865055561065674\n",
      "Iter:132 train_rmse:4.524 test_loss:156.3 test_rmse:0.05812 test_acc:0.6501\n",
      "Epoch  133\n",
      "Accuracy ---->  0.6866082549095154\n",
      "Iter:133 train_rmse:4.522 test_loss:156.1 test_rmse:0.05807 test_acc:0.6504\n",
      "Epoch  134\n",
      "Accuracy ---->  0.6866802275180817\n",
      "Iter:134 train_rmse:4.521 test_loss:155.8 test_rmse:0.05802 test_acc:0.6507\n",
      "Epoch  135\n",
      "Accuracy ---->  0.6867265105247498\n",
      "Iter:135 train_rmse:4.521 test_loss:155.6 test_rmse:0.05797 test_acc:0.651\n",
      "Epoch  136\n",
      "Accuracy ---->  0.6867562830448151\n",
      "Iter:136 train_rmse:4.52 test_loss:155.3 test_rmse:0.05792 test_acc:0.6513\n",
      "Epoch  137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.6867814064025879\n",
      "Iter:137 train_rmse:4.52 test_loss:155.0 test_rmse:0.05786 test_acc:0.6516\n",
      "Epoch  138\n",
      "Accuracy ---->  0.6868138909339905\n",
      "Iter:138 train_rmse:4.519 test_loss:154.7 test_rmse:0.0578 test_acc:0.652\n",
      "Epoch  139\n",
      "Accuracy ---->  0.6868619620800018\n",
      "Iter:139 train_rmse:4.519 test_loss:154.4 test_rmse:0.05774 test_acc:0.6523\n",
      "Epoch  140\n",
      "Accuracy ---->  0.6869294345378876\n",
      "Iter:140 train_rmse:4.518 test_loss:154.0 test_rmse:0.05769 test_acc:0.6527\n",
      "Epoch  141\n",
      "Accuracy ---->  0.6870155036449432\n",
      "Iter:141 train_rmse:4.517 test_loss:153.7 test_rmse:0.05763 test_acc:0.653\n",
      "Epoch  142\n",
      "Accuracy ---->  0.6871172487735748\n",
      "Iter:142 train_rmse:4.515 test_loss:153.4 test_rmse:0.05757 test_acc:0.6534\n",
      "Epoch  143\n",
      "Accuracy ---->  0.6872313022613525\n",
      "Iter:143 train_rmse:4.513 test_loss:153.2 test_rmse:0.05752 test_acc:0.6537\n",
      "Epoch  144\n",
      "Accuracy ---->  0.6873547434806824\n",
      "Iter:144 train_rmse:4.512 test_loss:152.9 test_rmse:0.05747 test_acc:0.654\n",
      "Epoch  145\n",
      "Accuracy ---->  0.6874853372573853\n",
      "Iter:145 train_rmse:4.51 test_loss:152.6 test_rmse:0.05742 test_acc:0.6543\n",
      "Epoch  146\n",
      "Accuracy ---->  0.6876220703125\n",
      "Iter:146 train_rmse:4.508 test_loss:152.4 test_rmse:0.05737 test_acc:0.6546\n",
      "Epoch  147\n",
      "Accuracy ---->  0.6877641081809998\n",
      "Iter:147 train_rmse:4.506 test_loss:152.2 test_rmse:0.05733 test_acc:0.6548\n",
      "Epoch  148\n",
      "Accuracy ---->  0.6879111528396606\n",
      "Iter:148 train_rmse:4.504 test_loss:151.9 test_rmse:0.05729 test_acc:0.6551\n",
      "Epoch  149\n",
      "Accuracy ---->  0.6880625784397125\n",
      "Iter:149 train_rmse:4.501 test_loss:151.7 test_rmse:0.05724 test_acc:0.6554\n",
      "Epoch  150\n",
      "Accuracy ---->  0.6882186233997345\n",
      "Iter:150 train_rmse:4.499 test_loss:151.5 test_rmse:0.0572 test_acc:0.6556\n",
      "Epoch  151\n",
      "Accuracy ---->  0.6883791089057922\n",
      "Iter:151 train_rmse:4.497 test_loss:151.3 test_rmse:0.05717 test_acc:0.6558\n",
      "Epoch  152\n",
      "Accuracy ---->  0.6885438859462738\n",
      "Iter:152 train_rmse:4.495 test_loss:151.1 test_rmse:0.05713 test_acc:0.6561\n",
      "Epoch  153\n",
      "Accuracy ---->  0.6887126266956329\n",
      "Iter:153 train_rmse:4.492 test_loss:150.9 test_rmse:0.05709 test_acc:0.6563\n",
      "Epoch  154\n",
      "Accuracy ---->  0.6888848543167114\n",
      "Iter:154 train_rmse:4.49 test_loss:150.7 test_rmse:0.05706 test_acc:0.6565\n",
      "Epoch  155\n",
      "Accuracy ---->  0.6890601217746735\n",
      "Iter:155 train_rmse:4.487 test_loss:150.5 test_rmse:0.05702 test_acc:0.6567\n",
      "Epoch  156\n",
      "Accuracy ---->  0.6892376244068146\n",
      "Iter:156 train_rmse:4.485 test_loss:150.4 test_rmse:0.05699 test_acc:0.6569\n",
      "Epoch  157\n",
      "Accuracy ---->  0.6894167065620422\n",
      "Iter:157 train_rmse:4.482 test_loss:150.2 test_rmse:0.05696 test_acc:0.6571\n",
      "Epoch  158\n",
      "Accuracy ---->  0.68959641456604\n",
      "Iter:158 train_rmse:4.479 test_loss:150.0 test_rmse:0.05693 test_acc:0.6573\n",
      "Epoch  159\n",
      "Accuracy ---->  0.6897760331630707\n",
      "Iter:159 train_rmse:4.477 test_loss:149.9 test_rmse:0.0569 test_acc:0.6575\n",
      "Epoch  160\n",
      "Accuracy ---->  0.6899548172950745\n",
      "Iter:160 train_rmse:4.474 test_loss:149.7 test_rmse:0.05687 test_acc:0.6576\n",
      "Epoch  161\n",
      "Accuracy ---->  0.690132200717926\n",
      "Iter:161 train_rmse:4.472 test_loss:149.6 test_rmse:0.05684 test_acc:0.6578\n",
      "Epoch  162\n",
      "Accuracy ---->  0.6903077065944672\n",
      "Iter:162 train_rmse:4.469 test_loss:149.4 test_rmse:0.05681 test_acc:0.658\n",
      "Epoch  163\n",
      "Accuracy ---->  0.6904805302619934\n",
      "Iter:163 train_rmse:4.467 test_loss:149.3 test_rmse:0.05678 test_acc:0.6581\n",
      "Epoch  164\n",
      "Accuracy ---->  0.6906507611274719\n",
      "Iter:164 train_rmse:4.464 test_loss:149.1 test_rmse:0.05676 test_acc:0.6583\n",
      "Epoch  165\n",
      "Accuracy ---->  0.6908183693885803\n",
      "Iter:165 train_rmse:4.462 test_loss:149.0 test_rmse:0.05673 test_acc:0.6585\n",
      "Epoch  166\n",
      "Accuracy ---->  0.6909832060337067\n",
      "Iter:166 train_rmse:4.459 test_loss:148.9 test_rmse:0.0567 test_acc:0.6586\n",
      "Epoch  167\n",
      "Accuracy ---->  0.69114550948143\n",
      "Iter:167 train_rmse:4.457 test_loss:148.7 test_rmse:0.05667 test_acc:0.6588\n",
      "Epoch  168\n",
      "Accuracy ---->  0.6913049817085266\n",
      "Iter:168 train_rmse:4.455 test_loss:148.6 test_rmse:0.05664 test_acc:0.659\n",
      "Epoch  169\n",
      "Accuracy ---->  0.6914624869823456\n",
      "Iter:169 train_rmse:4.452 test_loss:148.4 test_rmse:0.05662 test_acc:0.6591\n",
      "Epoch  170\n",
      "Accuracy ---->  0.6916180849075317\n",
      "Iter:170 train_rmse:4.45 test_loss:148.3 test_rmse:0.05659 test_acc:0.6593\n",
      "Epoch  171\n",
      "Accuracy ---->  0.6917716860771179\n",
      "Iter:171 train_rmse:4.448 test_loss:148.1 test_rmse:0.05656 test_acc:0.6595\n",
      "Epoch  172\n",
      "Accuracy ---->  0.6919238865375519\n",
      "Iter:172 train_rmse:4.446 test_loss:148.0 test_rmse:0.05653 test_acc:0.6596\n",
      "Epoch  173\n",
      "Accuracy ---->  0.6920747756958008\n",
      "Iter:173 train_rmse:4.444 test_loss:147.8 test_rmse:0.0565 test_acc:0.6598\n",
      "Epoch  174\n",
      "Accuracy ---->  0.6922248899936676\n",
      "Iter:174 train_rmse:4.441 test_loss:147.7 test_rmse:0.05647 test_acc:0.66\n",
      "Epoch  175\n",
      "Accuracy ---->  0.6923742294311523\n",
      "Iter:175 train_rmse:4.439 test_loss:147.5 test_rmse:0.05645 test_acc:0.6602\n",
      "Epoch  176\n",
      "Accuracy ---->  0.6925226747989655\n",
      "Iter:176 train_rmse:4.437 test_loss:147.4 test_rmse:0.05642 test_acc:0.6603\n",
      "Epoch  177\n",
      "Accuracy ---->  0.6926709711551666\n",
      "Iter:177 train_rmse:4.435 test_loss:147.2 test_rmse:0.05639 test_acc:0.6605\n",
      "Epoch  178\n",
      "Accuracy ---->  0.6928188800811768\n",
      "Iter:178 train_rmse:4.433 test_loss:147.1 test_rmse:0.05636 test_acc:0.6607\n",
      "Epoch  179\n",
      "Accuracy ---->  0.6929662525653839\n",
      "Iter:179 train_rmse:4.431 test_loss:146.9 test_rmse:0.05633 test_acc:0.6609\n",
      "Epoch  180\n",
      "Accuracy ---->  0.6931136548519135\n",
      "Iter:180 train_rmse:4.429 test_loss:146.8 test_rmse:0.0563 test_acc:0.661\n",
      "Epoch  181\n",
      "Accuracy ---->  0.6932610273361206\n",
      "Iter:181 train_rmse:4.426 test_loss:146.6 test_rmse:0.05627 test_acc:0.6612\n",
      "Epoch  182\n",
      "Accuracy ---->  0.6934081614017487\n",
      "Iter:182 train_rmse:4.424 test_loss:146.5 test_rmse:0.05625 test_acc:0.6614\n",
      "Epoch  183\n",
      "Accuracy ---->  0.6935555338859558\n",
      "Iter:183 train_rmse:4.422 test_loss:146.3 test_rmse:0.05622 test_acc:0.6615\n",
      "Epoch  184\n",
      "Accuracy ---->  0.6937023401260376\n",
      "Iter:184 train_rmse:4.42 test_loss:146.2 test_rmse:0.05619 test_acc:0.6617\n",
      "Epoch  185\n",
      "Accuracy ---->  0.6938495934009552\n",
      "Iter:185 train_rmse:4.418 test_loss:146.1 test_rmse:0.05616 test_acc:0.6619\n",
      "Epoch  186\n",
      "Accuracy ---->  0.6939963400363922\n",
      "Iter:186 train_rmse:4.416 test_loss:145.9 test_rmse:0.05613 test_acc:0.662\n",
      "Epoch  187\n",
      "Accuracy ---->  0.6941431760787964\n",
      "Iter:187 train_rmse:4.414 test_loss:145.8 test_rmse:0.05611 test_acc:0.6622\n",
      "Epoch  188\n",
      "Accuracy ---->  0.6942898035049438\n",
      "Iter:188 train_rmse:4.412 test_loss:145.6 test_rmse:0.05608 test_acc:0.6624\n",
      "Epoch  189\n",
      "Accuracy ---->  0.6944361627101898\n",
      "Iter:189 train_rmse:4.409 test_loss:145.5 test_rmse:0.05606 test_acc:0.6625\n",
      "Epoch  190\n",
      "Accuracy ---->  0.694582611322403\n",
      "Iter:190 train_rmse:4.407 test_loss:145.4 test_rmse:0.05603 test_acc:0.6627\n",
      "Epoch  191\n",
      "Accuracy ---->  0.694728434085846\n",
      "Iter:191 train_rmse:4.405 test_loss:145.2 test_rmse:0.05601 test_acc:0.6628\n",
      "Epoch  192\n",
      "Accuracy ---->  0.6948744058609009\n",
      "Iter:192 train_rmse:4.403 test_loss:145.1 test_rmse:0.05598 test_acc:0.663\n",
      "Epoch  193\n",
      "Accuracy ---->  0.6950198411941528\n",
      "Iter:193 train_rmse:4.401 test_loss:145.0 test_rmse:0.05596 test_acc:0.6631\n",
      "Epoch  194\n",
      "Accuracy ---->  0.6951653361320496\n",
      "Iter:194 train_rmse:4.399 test_loss:144.9 test_rmse:0.05593 test_acc:0.6633\n",
      "Epoch  195\n",
      "Accuracy ---->  0.6953108906745911\n",
      "Iter:195 train_rmse:4.397 test_loss:144.7 test_rmse:0.05591 test_acc:0.6634\n",
      "Epoch  196\n",
      "Accuracy ---->  0.6954559981822968\n",
      "Iter:196 train_rmse:4.395 test_loss:144.6 test_rmse:0.05588 test_acc:0.6635\n",
      "Epoch  197\n",
      "Accuracy ---->  0.6956018507480621\n",
      "Iter:197 train_rmse:4.393 test_loss:144.5 test_rmse:0.05586 test_acc:0.6637\n",
      "Epoch  198\n",
      "Accuracy ---->  0.6957474946975708\n",
      "Iter:198 train_rmse:4.391 test_loss:144.4 test_rmse:0.05584 test_acc:0.6638\n",
      "Epoch  199\n",
      "Accuracy ---->  0.6958937346935272\n",
      "Iter:199 train_rmse:4.388 test_loss:144.3 test_rmse:0.05582 test_acc:0.664\n",
      "Epoch  200\n",
      "Accuracy ---->  0.6960403919219971\n",
      "Iter:200 train_rmse:4.386 test_loss:144.2 test_rmse:0.05579 test_acc:0.6641\n",
      "Epoch  201\n",
      "Accuracy ---->  0.6961880922317505\n",
      "Iter:201 train_rmse:4.384 test_loss:144.0 test_rmse:0.05577 test_acc:0.6642\n",
      "Epoch  202\n",
      "Accuracy ---->  0.6963363885879517\n",
      "Iter:202 train_rmse:4.382 test_loss:143.9 test_rmse:0.05575 test_acc:0.6644\n",
      "Epoch  203\n",
      "Accuracy ---->  0.6964859068393707\n",
      "Iter:203 train_rmse:4.38 test_loss:143.8 test_rmse:0.05573 test_acc:0.6645\n",
      "Epoch  204\n",
      "Accuracy ---->  0.6966364681720734\n",
      "Iter:204 train_rmse:4.378 test_loss:143.7 test_rmse:0.05571 test_acc:0.6646\n",
      "Epoch  205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.6967883408069611\n",
      "Iter:205 train_rmse:4.376 test_loss:143.6 test_rmse:0.05568 test_acc:0.6648\n",
      "Epoch  206\n",
      "Accuracy ---->  0.6969407200813293\n",
      "Iter:206 train_rmse:4.373 test_loss:143.5 test_rmse:0.05566 test_acc:0.6649\n",
      "Epoch  207\n",
      "Accuracy ---->  0.6970947086811066\n",
      "Iter:207 train_rmse:4.371 test_loss:143.4 test_rmse:0.05564 test_acc:0.665\n",
      "Epoch  208\n",
      "Accuracy ---->  0.697249561548233\n",
      "Iter:208 train_rmse:4.369 test_loss:143.3 test_rmse:0.05562 test_acc:0.6651\n",
      "Epoch  209\n",
      "Accuracy ---->  0.6974051594734192\n",
      "Iter:209 train_rmse:4.367 test_loss:143.2 test_rmse:0.0556 test_acc:0.6653\n",
      "Epoch  210\n",
      "Accuracy ---->  0.6975614130496979\n",
      "Iter:210 train_rmse:4.364 test_loss:143.1 test_rmse:0.05558 test_acc:0.6654\n",
      "Epoch  211\n",
      "Accuracy ---->  0.6977185308933258\n",
      "Iter:211 train_rmse:4.362 test_loss:143.0 test_rmse:0.05556 test_acc:0.6655\n",
      "Epoch  212\n",
      "Accuracy ---->  0.6978755593299866\n",
      "Iter:212 train_rmse:4.36 test_loss:142.9 test_rmse:0.05554 test_acc:0.6656\n",
      "Epoch  213\n",
      "Accuracy ---->  0.6980329155921936\n",
      "Iter:213 train_rmse:4.358 test_loss:142.8 test_rmse:0.05552 test_acc:0.6657\n",
      "Epoch  214\n",
      "Accuracy ---->  0.6981899738311768\n",
      "Iter:214 train_rmse:4.355 test_loss:142.6 test_rmse:0.0555 test_acc:0.6659\n",
      "Epoch  215\n",
      "Accuracy ---->  0.6983467638492584\n",
      "Iter:215 train_rmse:4.353 test_loss:142.5 test_rmse:0.05548 test_acc:0.666\n",
      "Epoch  216\n",
      "Accuracy ---->  0.6985026299953461\n",
      "Iter:216 train_rmse:4.351 test_loss:142.4 test_rmse:0.05546 test_acc:0.6661\n",
      "Epoch  217\n",
      "Accuracy ---->  0.6986578106880188\n",
      "Iter:217 train_rmse:4.349 test_loss:142.3 test_rmse:0.05544 test_acc:0.6662\n",
      "Epoch  218\n",
      "Accuracy ---->  0.6988116204738617\n",
      "Iter:218 train_rmse:4.346 test_loss:142.2 test_rmse:0.05542 test_acc:0.6664\n",
      "Epoch  219\n",
      "Accuracy ---->  0.6989642381668091\n",
      "Iter:219 train_rmse:4.344 test_loss:142.1 test_rmse:0.0554 test_acc:0.6665\n",
      "Epoch  220\n",
      "Accuracy ---->  0.6991149485111237\n",
      "Iter:220 train_rmse:4.342 test_loss:142.0 test_rmse:0.05538 test_acc:0.6666\n",
      "Epoch  221\n",
      "Accuracy ---->  0.6992649137973785\n",
      "Iter:221 train_rmse:4.34 test_loss:141.9 test_rmse:0.05536 test_acc:0.6667\n",
      "Epoch  222\n",
      "Accuracy ---->  0.6994121968746185\n",
      "Iter:222 train_rmse:4.338 test_loss:141.8 test_rmse:0.05534 test_acc:0.6668\n",
      "Epoch  223\n",
      "Accuracy ---->  0.6995578110218048\n",
      "Iter:223 train_rmse:4.336 test_loss:141.7 test_rmse:0.05532 test_acc:0.667\n",
      "Epoch  224\n",
      "Accuracy ---->  0.6997013688087463\n",
      "Iter:224 train_rmse:4.334 test_loss:141.6 test_rmse:0.0553 test_acc:0.6671\n",
      "Epoch  225\n",
      "Accuracy ---->  0.6998427212238312\n",
      "Iter:225 train_rmse:4.331 test_loss:141.5 test_rmse:0.05528 test_acc:0.6672\n",
      "Epoch  226\n",
      "Accuracy ---->  0.6999820172786713\n",
      "Iter:226 train_rmse:4.329 test_loss:141.4 test_rmse:0.05526 test_acc:0.6673\n",
      "Epoch  227\n",
      "Accuracy ---->  0.7001186609268188\n",
      "Iter:227 train_rmse:4.327 test_loss:141.3 test_rmse:0.05524 test_acc:0.6675\n",
      "Epoch  228\n",
      "Accuracy ---->  0.7002533972263336\n",
      "Iter:228 train_rmse:4.326 test_loss:141.2 test_rmse:0.05521 test_acc:0.6676\n",
      "Epoch  229\n",
      "Accuracy ---->  0.7003859281539917\n",
      "Iter:229 train_rmse:4.324 test_loss:141.1 test_rmse:0.05519 test_acc:0.6677\n",
      "Epoch  230\n",
      "Accuracy ---->  0.7005156576633453\n",
      "Iter:230 train_rmse:4.322 test_loss:141.0 test_rmse:0.05517 test_acc:0.6678\n",
      "Epoch  231\n",
      "Accuracy ---->  0.7006433606147766\n",
      "Iter:231 train_rmse:4.32 test_loss:140.9 test_rmse:0.05515 test_acc:0.668\n",
      "Epoch  232\n",
      "Accuracy ---->  0.700768917798996\n",
      "Iter:232 train_rmse:4.318 test_loss:140.8 test_rmse:0.05513 test_acc:0.6681\n",
      "Epoch  233\n",
      "Accuracy ---->  0.7008921205997467\n",
      "Iter:233 train_rmse:4.316 test_loss:140.7 test_rmse:0.05511 test_acc:0.6682\n",
      "Epoch  234\n",
      "Accuracy ---->  0.7010129690170288\n",
      "Iter:234 train_rmse:4.315 test_loss:140.6 test_rmse:0.05509 test_acc:0.6684\n",
      "Epoch  235\n",
      "Accuracy ---->  0.7011320292949677\n",
      "Iter:235 train_rmse:4.313 test_loss:140.4 test_rmse:0.05506 test_acc:0.6685\n",
      "Epoch  236\n",
      "Accuracy ---->  0.7012487947940826\n",
      "Iter:236 train_rmse:4.311 test_loss:140.3 test_rmse:0.05504 test_acc:0.6686\n",
      "Epoch  237\n",
      "Accuracy ---->  0.7013637721538544\n",
      "Iter:237 train_rmse:4.31 test_loss:140.2 test_rmse:0.05502 test_acc:0.6687\n",
      "Epoch  238\n",
      "Accuracy ---->  0.7014769613742828\n",
      "Iter:238 train_rmse:4.308 test_loss:140.1 test_rmse:0.055 test_acc:0.6689\n",
      "Epoch  239\n",
      "Accuracy ---->  0.7015882730484009\n",
      "Iter:239 train_rmse:4.306 test_loss:140.0 test_rmse:0.05498 test_acc:0.669\n",
      "Epoch  240\n",
      "Accuracy ---->  0.7016982734203339\n",
      "Iter:240 train_rmse:4.305 test_loss:139.9 test_rmse:0.05495 test_acc:0.6692\n",
      "Epoch  241\n",
      "Accuracy ---->  0.7018066048622131\n",
      "Iter:241 train_rmse:4.303 test_loss:139.8 test_rmse:0.05493 test_acc:0.6693\n",
      "Epoch  242\n",
      "Accuracy ---->  0.7019139528274536\n",
      "Iter:242 train_rmse:4.302 test_loss:139.6 test_rmse:0.05491 test_acc:0.6694\n",
      "Epoch  243\n",
      "Accuracy ---->  0.7020200192928314\n",
      "Iter:243 train_rmse:4.3 test_loss:139.5 test_rmse:0.05488 test_acc:0.6696\n",
      "Epoch  244\n",
      "Accuracy ---->  0.7021252810955048\n",
      "Iter:244 train_rmse:4.299 test_loss:139.4 test_rmse:0.05486 test_acc:0.6697\n",
      "Epoch  245\n",
      "Accuracy ---->  0.7022302448749542\n",
      "Iter:245 train_rmse:4.297 test_loss:139.3 test_rmse:0.05484 test_acc:0.6699\n",
      "Epoch  246\n",
      "Accuracy ---->  0.7023343145847321\n",
      "Iter:246 train_rmse:4.296 test_loss:139.2 test_rmse:0.05481 test_acc:0.67\n",
      "Epoch  247\n",
      "Accuracy ---->  0.7024374306201935\n",
      "Iter:247 train_rmse:4.294 test_loss:139.1 test_rmse:0.05479 test_acc:0.6701\n",
      "Epoch  248\n",
      "Accuracy ---->  0.7025411128997803\n",
      "Iter:248 train_rmse:4.293 test_loss:138.9 test_rmse:0.05477 test_acc:0.6703\n",
      "Epoch  249\n",
      "Accuracy ---->  0.7026444971561432\n",
      "Iter:249 train_rmse:4.291 test_loss:138.8 test_rmse:0.05474 test_acc:0.6704\n",
      "Epoch  250\n",
      "Accuracy ---->  0.7027479708194733\n",
      "Iter:250 train_rmse:4.29 test_loss:138.7 test_rmse:0.05472 test_acc:0.6706\n",
      "Epoch  251\n",
      "Accuracy ---->  0.7028516530990601\n",
      "Iter:251 train_rmse:4.288 test_loss:138.6 test_rmse:0.05469 test_acc:0.6707\n",
      "Epoch  252\n",
      "Accuracy ---->  0.7029554843902588\n",
      "Iter:252 train_rmse:4.287 test_loss:138.4 test_rmse:0.05467 test_acc:0.6709\n",
      "Epoch  253\n",
      "Accuracy ---->  0.70305997133255\n",
      "Iter:253 train_rmse:4.285 test_loss:138.3 test_rmse:0.05464 test_acc:0.671\n",
      "Epoch  254\n",
      "Accuracy ---->  0.7031649351119995\n",
      "Iter:254 train_rmse:4.284 test_loss:138.2 test_rmse:0.05462 test_acc:0.6712\n",
      "Epoch  255\n",
      "Accuracy ---->  0.7032702267169952\n",
      "Iter:255 train_rmse:4.282 test_loss:138.1 test_rmse:0.05459 test_acc:0.6713\n",
      "Epoch  256\n",
      "Accuracy ---->  0.7033759951591492\n",
      "Iter:256 train_rmse:4.28 test_loss:137.9 test_rmse:0.05457 test_acc:0.6715\n",
      "Epoch  257\n",
      "Accuracy ---->  0.7034819722175598\n",
      "Iter:257 train_rmse:4.279 test_loss:137.8 test_rmse:0.05455 test_acc:0.6716\n",
      "Epoch  258\n",
      "Accuracy ---->  0.7035882771015167\n",
      "Iter:258 train_rmse:4.277 test_loss:137.7 test_rmse:0.05452 test_acc:0.6718\n",
      "Epoch  259\n",
      "Accuracy ---->  0.7036947011947632\n",
      "Iter:259 train_rmse:4.276 test_loss:137.6 test_rmse:0.0545 test_acc:0.6719\n",
      "Epoch  260\n",
      "Accuracy ---->  0.7038004696369171\n",
      "Iter:260 train_rmse:4.274 test_loss:137.5 test_rmse:0.05447 test_acc:0.672\n",
      "Epoch  261\n",
      "Accuracy ---->  0.7039064466953278\n",
      "Iter:261 train_rmse:4.273 test_loss:137.3 test_rmse:0.05445 test_acc:0.6722\n",
      "Epoch  262\n",
      "Accuracy ---->  0.7040116786956787\n",
      "Iter:262 train_rmse:4.271 test_loss:137.2 test_rmse:0.05443 test_acc:0.6723\n",
      "Epoch  263\n",
      "Accuracy ---->  0.7041161954402924\n",
      "Iter:263 train_rmse:4.27 test_loss:137.1 test_rmse:0.0544 test_acc:0.6725\n",
      "Epoch  264\n",
      "Accuracy ---->  0.7042199373245239\n",
      "Iter:264 train_rmse:4.268 test_loss:137.0 test_rmse:0.05438 test_acc:0.6726\n",
      "Epoch  265\n",
      "Accuracy ---->  0.7043223083019257\n",
      "Iter:265 train_rmse:4.267 test_loss:136.9 test_rmse:0.05436 test_acc:0.6727\n",
      "Epoch  266\n",
      "Accuracy ---->  0.7044243216514587\n",
      "Iter:266 train_rmse:4.265 test_loss:136.8 test_rmse:0.05434 test_acc:0.6729\n",
      "Epoch  267\n",
      "Accuracy ---->  0.7045242190361023\n",
      "Iter:267 train_rmse:4.264 test_loss:136.7 test_rmse:0.05431 test_acc:0.673\n",
      "Epoch  268\n",
      "Accuracy ---->  0.7046234011650085\n",
      "Iter:268 train_rmse:4.262 test_loss:136.6 test_rmse:0.05429 test_acc:0.6731\n",
      "Epoch  269\n",
      "Accuracy ---->  0.7047214806079865\n",
      "Iter:269 train_rmse:4.261 test_loss:136.5 test_rmse:0.05427 test_acc:0.6733\n",
      "Epoch  270\n",
      "Accuracy ---->  0.7048178613185883\n",
      "Iter:270 train_rmse:4.26 test_loss:136.3 test_rmse:0.05425 test_acc:0.6734\n",
      "Epoch  271\n",
      "Accuracy ---->  0.7049136757850647\n",
      "Iter:271 train_rmse:4.258 test_loss:136.2 test_rmse:0.05423 test_acc:0.6735\n",
      "Epoch  272\n",
      "Accuracy ---->  0.7050085961818695\n",
      "Iter:272 train_rmse:4.257 test_loss:136.1 test_rmse:0.05421 test_acc:0.6736\n",
      "Epoch  273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.7051019072532654\n",
      "Iter:273 train_rmse:4.256 test_loss:136.0 test_rmse:0.05419 test_acc:0.6738\n",
      "Epoch  274\n",
      "Accuracy ---->  0.7051952481269836\n",
      "Iter:274 train_rmse:4.254 test_loss:135.9 test_rmse:0.05416 test_acc:0.6739\n",
      "Epoch  275\n",
      "Accuracy ---->  0.7052875757217407\n",
      "Iter:275 train_rmse:4.253 test_loss:135.8 test_rmse:0.05414 test_acc:0.674\n",
      "Epoch  276\n",
      "Accuracy ---->  0.70537930727005\n",
      "Iter:276 train_rmse:4.252 test_loss:135.7 test_rmse:0.05412 test_acc:0.6742\n",
      "Epoch  277\n",
      "Accuracy ---->  0.7054711580276489\n",
      "Iter:277 train_rmse:4.25 test_loss:135.6 test_rmse:0.0541 test_acc:0.6743\n",
      "Epoch  278\n",
      "Accuracy ---->  0.7055622935295105\n",
      "Iter:278 train_rmse:4.249 test_loss:135.5 test_rmse:0.05408 test_acc:0.6744\n",
      "Epoch  279\n",
      "Accuracy ---->  0.7056531012058258\n",
      "Iter:279 train_rmse:4.248 test_loss:135.4 test_rmse:0.05406 test_acc:0.6745\n",
      "Epoch  280\n",
      "Accuracy ---->  0.7057445049285889\n",
      "Iter:280 train_rmse:4.246 test_loss:135.3 test_rmse:0.05404 test_acc:0.6747\n",
      "Epoch  281\n",
      "Accuracy ---->  0.7058355808258057\n",
      "Iter:281 train_rmse:4.245 test_loss:135.2 test_rmse:0.05402 test_acc:0.6748\n",
      "Epoch  282\n",
      "Accuracy ---->  0.7059265971183777\n",
      "Iter:282 train_rmse:4.244 test_loss:135.1 test_rmse:0.05399 test_acc:0.6749\n",
      "Epoch  283\n",
      "Accuracy ---->  0.7060181498527527\n",
      "Iter:283 train_rmse:4.242 test_loss:135.0 test_rmse:0.05397 test_acc:0.6751\n",
      "Epoch  284\n",
      "Accuracy ---->  0.7061099112033844\n",
      "Iter:284 train_rmse:4.241 test_loss:134.9 test_rmse:0.05395 test_acc:0.6752\n",
      "Epoch  285\n",
      "Accuracy ---->  0.7062018811702728\n",
      "Iter:285 train_rmse:4.24 test_loss:134.8 test_rmse:0.05393 test_acc:0.6753\n",
      "Epoch  286\n",
      "Accuracy ---->  0.7062934637069702\n",
      "Iter:286 train_rmse:4.238 test_loss:134.7 test_rmse:0.05391 test_acc:0.6754\n",
      "Epoch  287\n",
      "Accuracy ---->  0.7063855230808258\n",
      "Iter:287 train_rmse:4.237 test_loss:134.6 test_rmse:0.05389 test_acc:0.6756\n",
      "Epoch  288\n",
      "Accuracy ---->  0.70647794008255\n",
      "Iter:288 train_rmse:4.236 test_loss:134.4 test_rmse:0.05387 test_acc:0.6757\n",
      "Epoch  289\n",
      "Accuracy ---->  0.7065701186656952\n",
      "Iter:289 train_rmse:4.234 test_loss:134.3 test_rmse:0.05385 test_acc:0.6758\n",
      "Epoch  290\n",
      "Accuracy ---->  0.7066624462604523\n",
      "Iter:290 train_rmse:4.233 test_loss:134.2 test_rmse:0.05383 test_acc:0.6759\n",
      "Epoch  291\n",
      "Accuracy ---->  0.7067547142505646\n",
      "Iter:291 train_rmse:4.232 test_loss:134.1 test_rmse:0.0538 test_acc:0.6761\n",
      "Epoch  292\n",
      "Accuracy ---->  0.7068469822406769\n",
      "Iter:292 train_rmse:4.23 test_loss:134.0 test_rmse:0.05378 test_acc:0.6762\n",
      "Epoch  293\n",
      "Accuracy ---->  0.7069389522075653\n",
      "Iter:293 train_rmse:4.229 test_loss:133.9 test_rmse:0.05376 test_acc:0.6763\n",
      "Epoch  294\n",
      "Accuracy ---->  0.7070312201976776\n",
      "Iter:294 train_rmse:4.228 test_loss:133.8 test_rmse:0.05374 test_acc:0.6765\n",
      "Epoch  295\n",
      "Accuracy ---->  0.7071229815483093\n",
      "Iter:295 train_rmse:4.226 test_loss:133.7 test_rmse:0.05372 test_acc:0.6766\n",
      "Epoch  296\n",
      "Accuracy ---->  0.7072145342826843\n",
      "Iter:296 train_rmse:4.225 test_loss:133.6 test_rmse:0.0537 test_acc:0.6767\n",
      "Epoch  297\n",
      "Accuracy ---->  0.7073062062263489\n",
      "Iter:297 train_rmse:4.224 test_loss:133.5 test_rmse:0.05367 test_acc:0.6769\n",
      "Epoch  298\n",
      "Accuracy ---->  0.7073974013328552\n",
      "Iter:298 train_rmse:4.222 test_loss:133.4 test_rmse:0.05365 test_acc:0.677\n",
      "Epoch  299\n",
      "Accuracy ---->  0.7074880301952362\n",
      "Iter:299 train_rmse:4.221 test_loss:133.3 test_rmse:0.05363 test_acc:0.6771\n",
      "Epoch  300\n",
      "Accuracy ---->  0.707579106092453\n",
      "Iter:300 train_rmse:4.22 test_loss:133.2 test_rmse:0.05361 test_acc:0.6773\n",
      "Epoch  301\n",
      "Accuracy ---->  0.7076694667339325\n",
      "Iter:301 train_rmse:4.219 test_loss:133.1 test_rmse:0.05359 test_acc:0.6774\n",
      "Epoch  302\n",
      "Accuracy ---->  0.7077597379684448\n",
      "Iter:302 train_rmse:4.217 test_loss:132.9 test_rmse:0.05356 test_acc:0.6775\n",
      "Epoch  303\n",
      "Accuracy ---->  0.7078497409820557\n",
      "Iter:303 train_rmse:4.216 test_loss:132.8 test_rmse:0.05354 test_acc:0.6777\n",
      "Epoch  304\n",
      "Accuracy ---->  0.7079395055770874\n",
      "Iter:304 train_rmse:4.215 test_loss:132.7 test_rmse:0.05352 test_acc:0.6778\n",
      "Epoch  305\n",
      "Accuracy ---->  0.7080289423465729\n",
      "Iter:305 train_rmse:4.213 test_loss:132.6 test_rmse:0.0535 test_acc:0.6779\n",
      "Epoch  306\n",
      "Accuracy ---->  0.7081179916858673\n",
      "Iter:306 train_rmse:4.212 test_loss:132.5 test_rmse:0.05347 test_acc:0.6781\n",
      "Epoch  307\n",
      "Accuracy ---->  0.7082072198390961\n",
      "Iter:307 train_rmse:4.211 test_loss:132.4 test_rmse:0.05345 test_acc:0.6782\n",
      "Epoch  308\n",
      "Accuracy ---->  0.7082962393760681\n",
      "Iter:308 train_rmse:4.209 test_loss:132.3 test_rmse:0.05343 test_acc:0.6783\n",
      "Epoch  309\n",
      "Accuracy ---->  0.7083843946456909\n",
      "Iter:309 train_rmse:4.208 test_loss:132.2 test_rmse:0.05341 test_acc:0.6785\n",
      "Epoch  310\n",
      "Accuracy ---->  0.7084727883338928\n",
      "Iter:310 train_rmse:4.207 test_loss:132.1 test_rmse:0.05338 test_acc:0.6786\n",
      "Epoch  311\n",
      "Accuracy ---->  0.7085607647895813\n",
      "Iter:311 train_rmse:4.206 test_loss:131.9 test_rmse:0.05336 test_acc:0.6787\n",
      "Epoch  312\n",
      "Accuracy ---->  0.7086490988731384\n",
      "Iter:312 train_rmse:4.204 test_loss:131.8 test_rmse:0.05334 test_acc:0.6789\n",
      "Epoch  313\n",
      "Accuracy ---->  0.7087365984916687\n",
      "Iter:313 train_rmse:4.203 test_loss:131.7 test_rmse:0.05331 test_acc:0.679\n",
      "Epoch  314\n",
      "Accuracy ---->  0.7088242173194885\n",
      "Iter:314 train_rmse:4.202 test_loss:131.6 test_rmse:0.05329 test_acc:0.6792\n",
      "Epoch  315\n",
      "Accuracy ---->  0.7089114189147949\n",
      "Iter:315 train_rmse:4.201 test_loss:131.5 test_rmse:0.05327 test_acc:0.6793\n",
      "Epoch  316\n",
      "Accuracy ---->  0.7089986801147461\n",
      "Iter:316 train_rmse:4.199 test_loss:131.4 test_rmse:0.05324 test_acc:0.6794\n",
      "Epoch  317\n",
      "Accuracy ---->  0.7090859711170197\n",
      "Iter:317 train_rmse:4.198 test_loss:131.3 test_rmse:0.05322 test_acc:0.6796\n",
      "Epoch  318\n",
      "Accuracy ---->  0.7091729938983917\n",
      "Iter:318 train_rmse:4.197 test_loss:131.1 test_rmse:0.0532 test_acc:0.6797\n",
      "Epoch  319\n",
      "Accuracy ---->  0.7092596888542175\n",
      "Iter:319 train_rmse:4.196 test_loss:131.0 test_rmse:0.05317 test_acc:0.6799\n",
      "Epoch  320\n",
      "Accuracy ---->  0.7093468606472015\n",
      "Iter:320 train_rmse:4.194 test_loss:130.9 test_rmse:0.05315 test_acc:0.68\n",
      "Epoch  321\n",
      "Accuracy ---->  0.7094333171844482\n",
      "Iter:321 train_rmse:4.193 test_loss:130.8 test_rmse:0.05312 test_acc:0.6802\n",
      "Epoch  322\n",
      "Accuracy ---->  0.7095202505588531\n",
      "Iter:322 train_rmse:4.192 test_loss:130.7 test_rmse:0.0531 test_acc:0.6803\n",
      "Epoch  323\n",
      "Accuracy ---->  0.7096068561077118\n",
      "Iter:323 train_rmse:4.191 test_loss:130.6 test_rmse:0.05308 test_acc:0.6804\n",
      "Epoch  324\n",
      "Accuracy ---->  0.7096932530403137\n",
      "Iter:324 train_rmse:4.189 test_loss:130.4 test_rmse:0.05305 test_acc:0.6806\n",
      "Epoch  325\n",
      "Accuracy ---->  0.7097802758216858\n",
      "Iter:325 train_rmse:4.188 test_loss:130.3 test_rmse:0.05303 test_acc:0.6807\n",
      "Epoch  326\n",
      "Accuracy ---->  0.709867000579834\n",
      "Iter:326 train_rmse:4.187 test_loss:130.2 test_rmse:0.05301 test_acc:0.6809\n",
      "Epoch  327\n",
      "Accuracy ---->  0.7099540531635284\n",
      "Iter:327 train_rmse:4.186 test_loss:130.1 test_rmse:0.05298 test_acc:0.681\n",
      "Epoch  328\n",
      "Accuracy ---->  0.7100408971309662\n",
      "Iter:328 train_rmse:4.184 test_loss:130.0 test_rmse:0.05296 test_acc:0.6812\n",
      "Epoch  329\n",
      "Accuracy ---->  0.7101280987262726\n",
      "Iter:329 train_rmse:4.183 test_loss:129.9 test_rmse:0.05293 test_acc:0.6813\n",
      "Epoch  330\n",
      "Accuracy ---->  0.7102155089378357\n",
      "Iter:330 train_rmse:4.182 test_loss:129.7 test_rmse:0.05291 test_acc:0.6815\n",
      "Epoch  331\n",
      "Accuracy ---->  0.7103034257888794\n",
      "Iter:331 train_rmse:4.181 test_loss:129.6 test_rmse:0.05288 test_acc:0.6816\n",
      "Epoch  332\n",
      "Accuracy ---->  0.7103918790817261\n",
      "Iter:332 train_rmse:4.179 test_loss:129.5 test_rmse:0.05286 test_acc:0.6818\n",
      "Epoch  333\n",
      "Accuracy ---->  0.7104800641536713\n",
      "Iter:333 train_rmse:4.178 test_loss:129.4 test_rmse:0.05284 test_acc:0.6819\n",
      "Epoch  334\n",
      "Accuracy ---->  0.7105691730976105\n",
      "Iter:334 train_rmse:4.177 test_loss:129.3 test_rmse:0.05281 test_acc:0.6821\n",
      "Epoch  335\n",
      "Accuracy ---->  0.7106584012508392\n",
      "Iter:335 train_rmse:4.175 test_loss:129.1 test_rmse:0.05279 test_acc:0.6822\n",
      "Epoch  336\n",
      "Accuracy ---->  0.7107484638690948\n",
      "Iter:336 train_rmse:4.174 test_loss:129.0 test_rmse:0.05276 test_acc:0.6823\n",
      "Epoch  337\n",
      "Accuracy ---->  0.7108392715454102\n",
      "Iter:337 train_rmse:4.173 test_loss:128.9 test_rmse:0.05274 test_acc:0.6825\n",
      "Epoch  338\n",
      "Accuracy ---->  0.7109310626983643\n",
      "Iter:338 train_rmse:4.171 test_loss:128.8 test_rmse:0.05271 test_acc:0.6826\n",
      "Epoch  339\n",
      "Accuracy ---->  0.7110236287117004\n",
      "Iter:339 train_rmse:4.17 test_loss:128.7 test_rmse:0.05269 test_acc:0.6828\n",
      "Epoch  340\n",
      "Accuracy ---->  0.7111169695854187\n",
      "Iter:340 train_rmse:4.169 test_loss:128.6 test_rmse:0.05266 test_acc:0.6829\n",
      "Epoch  341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.7112118303775787\n",
      "Iter:341 train_rmse:4.167 test_loss:128.4 test_rmse:0.05264 test_acc:0.6831\n",
      "Epoch  342\n",
      "Accuracy ---->  0.7113080620765686\n",
      "Iter:342 train_rmse:4.166 test_loss:128.3 test_rmse:0.05262 test_acc:0.6832\n",
      "Epoch  343\n",
      "Accuracy ---->  0.7114049196243286\n",
      "Iter:343 train_rmse:4.165 test_loss:128.2 test_rmse:0.05259 test_acc:0.6834\n",
      "Epoch  344\n",
      "Accuracy ---->  0.7115031480789185\n",
      "Iter:344 train_rmse:4.163 test_loss:128.1 test_rmse:0.05257 test_acc:0.6835\n",
      "Epoch  345\n",
      "Accuracy ---->  0.711603581905365\n",
      "Iter:345 train_rmse:4.162 test_loss:128.0 test_rmse:0.05254 test_acc:0.6837\n",
      "Epoch  346\n",
      "Accuracy ---->  0.7117055058479309\n",
      "Iter:346 train_rmse:4.16 test_loss:127.8 test_rmse:0.05252 test_acc:0.6838\n",
      "Epoch  347\n",
      "Accuracy ---->  0.7118090093135834\n",
      "Iter:347 train_rmse:4.159 test_loss:127.7 test_rmse:0.05249 test_acc:0.684\n",
      "Epoch  348\n",
      "Accuracy ---->  0.7119146585464478\n",
      "Iter:348 train_rmse:4.157 test_loss:127.6 test_rmse:0.05247 test_acc:0.6841\n",
      "Epoch  349\n",
      "Accuracy ---->  0.7120215594768524\n",
      "Iter:349 train_rmse:4.156 test_loss:127.5 test_rmse:0.05245 test_acc:0.6843\n",
      "Epoch  350\n",
      "Accuracy ---->  0.7121312618255615\n",
      "Iter:350 train_rmse:4.154 test_loss:127.4 test_rmse:0.05242 test_acc:0.6844\n",
      "Epoch  351\n",
      "Accuracy ---->  0.7122425138950348\n",
      "Iter:351 train_rmse:4.153 test_loss:127.3 test_rmse:0.0524 test_acc:0.6845\n",
      "Epoch  352\n",
      "Accuracy ---->  0.7123562097549438\n",
      "Iter:352 train_rmse:4.151 test_loss:127.1 test_rmse:0.05237 test_acc:0.6847\n",
      "Epoch  353\n",
      "Accuracy ---->  0.7124721705913544\n",
      "Iter:353 train_rmse:4.149 test_loss:127.0 test_rmse:0.05235 test_acc:0.6848\n",
      "Epoch  354\n",
      "Accuracy ---->  0.7125903367996216\n",
      "Iter:354 train_rmse:4.148 test_loss:126.9 test_rmse:0.05233 test_acc:0.685\n",
      "Epoch  355\n",
      "Accuracy ---->  0.7127102017402649\n",
      "Iter:355 train_rmse:4.146 test_loss:126.8 test_rmse:0.0523 test_acc:0.6851\n",
      "Epoch  356\n",
      "Accuracy ---->  0.7128332257270813\n",
      "Iter:356 train_rmse:4.144 test_loss:126.7 test_rmse:0.05228 test_acc:0.6853\n",
      "Epoch  357\n",
      "Accuracy ---->  0.712958574295044\n",
      "Iter:357 train_rmse:4.142 test_loss:126.6 test_rmse:0.05226 test_acc:0.6854\n",
      "Epoch  358\n",
      "Accuracy ---->  0.7130855023860931\n",
      "Iter:358 train_rmse:4.14 test_loss:126.5 test_rmse:0.05223 test_acc:0.6855\n",
      "Epoch  359\n",
      "Accuracy ---->  0.7132152915000916\n",
      "Iter:359 train_rmse:4.138 test_loss:126.4 test_rmse:0.05221 test_acc:0.6857\n",
      "Epoch  360\n",
      "Accuracy ---->  0.7133477330207825\n",
      "Iter:360 train_rmse:4.137 test_loss:126.3 test_rmse:0.05219 test_acc:0.6858\n",
      "Epoch  361\n",
      "Accuracy ---->  0.7134813368320465\n",
      "Iter:361 train_rmse:4.135 test_loss:126.1 test_rmse:0.05217 test_acc:0.6859\n",
      "Epoch  362\n",
      "Accuracy ---->  0.7136181890964508\n",
      "Iter:362 train_rmse:4.133 test_loss:126.0 test_rmse:0.05214 test_acc:0.6861\n",
      "Epoch  363\n",
      "Accuracy ---->  0.7137565314769745\n",
      "Iter:363 train_rmse:4.131 test_loss:125.9 test_rmse:0.05212 test_acc:0.6862\n",
      "Epoch  364\n",
      "Accuracy ---->  0.713897705078125\n",
      "Iter:364 train_rmse:4.129 test_loss:125.8 test_rmse:0.0521 test_acc:0.6863\n",
      "Epoch  365\n",
      "Accuracy ---->  0.7140398919582367\n",
      "Iter:365 train_rmse:4.127 test_loss:125.7 test_rmse:0.05208 test_acc:0.6865\n",
      "Epoch  366\n",
      "Accuracy ---->  0.7141856551170349\n",
      "Iter:366 train_rmse:4.124 test_loss:125.6 test_rmse:0.05206 test_acc:0.6866\n",
      "Epoch  367\n",
      "Accuracy ---->  0.7143326699733734\n",
      "Iter:367 train_rmse:4.122 test_loss:125.5 test_rmse:0.05204 test_acc:0.6867\n",
      "Epoch  368\n",
      "Accuracy ---->  0.7144810259342194\n",
      "Iter:368 train_rmse:4.12 test_loss:125.4 test_rmse:0.05201 test_acc:0.6868\n",
      "Epoch  369\n",
      "Accuracy ---->  0.7146314382553101\n",
      "Iter:369 train_rmse:4.118 test_loss:125.3 test_rmse:0.05199 test_acc:0.687\n",
      "Epoch  370\n",
      "Accuracy ---->  0.7147842943668365\n",
      "Iter:370 train_rmse:4.116 test_loss:125.2 test_rmse:0.05197 test_acc:0.6871\n",
      "Epoch  371\n",
      "Accuracy ---->  0.7149380147457123\n",
      "Iter:371 train_rmse:4.114 test_loss:125.1 test_rmse:0.05195 test_acc:0.6872\n",
      "Epoch  372\n",
      "Accuracy ---->  0.7150931358337402\n",
      "Iter:372 train_rmse:4.111 test_loss:125.0 test_rmse:0.05193 test_acc:0.6873\n",
      "Epoch  373\n",
      "Accuracy ---->  0.7152508795261383\n",
      "Iter:373 train_rmse:4.109 test_loss:124.9 test_rmse:0.05191 test_acc:0.6875\n",
      "Epoch  374\n",
      "Accuracy ---->  0.7154087722301483\n",
      "Iter:374 train_rmse:4.107 test_loss:124.8 test_rmse:0.05189 test_acc:0.6876\n",
      "Epoch  375\n",
      "Accuracy ---->  0.7155685424804688\n",
      "Iter:375 train_rmse:4.105 test_loss:124.8 test_rmse:0.05187 test_acc:0.6877\n",
      "Epoch  376\n",
      "Accuracy ---->  0.7157283425331116\n",
      "Iter:376 train_rmse:4.102 test_loss:124.7 test_rmse:0.05185 test_acc:0.6878\n",
      "Epoch  377\n",
      "Accuracy ---->  0.7158903181552887\n",
      "Iter:377 train_rmse:4.1 test_loss:124.6 test_rmse:0.05184 test_acc:0.6879\n",
      "Epoch  378\n",
      "Accuracy ---->  0.7160524725914001\n",
      "Iter:378 train_rmse:4.098 test_loss:124.5 test_rmse:0.05182 test_acc:0.688\n",
      "Epoch  379\n",
      "Accuracy ---->  0.7162151336669922\n",
      "Iter:379 train_rmse:4.095 test_loss:124.4 test_rmse:0.0518 test_acc:0.6881\n",
      "Epoch  380\n",
      "Accuracy ---->  0.7163785398006439\n",
      "Iter:380 train_rmse:4.093 test_loss:124.3 test_rmse:0.05178 test_acc:0.6883\n",
      "Epoch  381\n",
      "Accuracy ---->  0.7165420949459076\n",
      "Iter:381 train_rmse:4.09 test_loss:124.2 test_rmse:0.05176 test_acc:0.6884\n",
      "Epoch  382\n",
      "Accuracy ---->  0.7167062759399414\n",
      "Iter:382 train_rmse:4.088 test_loss:124.1 test_rmse:0.05174 test_acc:0.6885\n",
      "Epoch  383\n",
      "Accuracy ---->  0.7168703377246857\n",
      "Iter:383 train_rmse:4.086 test_loss:124.1 test_rmse:0.05173 test_acc:0.6886\n",
      "Epoch  384\n",
      "Accuracy ---->  0.7170345485210419\n",
      "Iter:384 train_rmse:4.083 test_loss:124.0 test_rmse:0.05171 test_acc:0.6887\n",
      "Epoch  385\n",
      "Accuracy ---->  0.7171986699104309\n",
      "Iter:385 train_rmse:4.081 test_loss:123.9 test_rmse:0.05169 test_acc:0.6888\n",
      "Epoch  386\n",
      "Accuracy ---->  0.7173623144626617\n",
      "Iter:386 train_rmse:4.079 test_loss:123.8 test_rmse:0.05167 test_acc:0.6889\n",
      "Epoch  387\n",
      "Accuracy ---->  0.7175251245498657\n",
      "Iter:387 train_rmse:4.076 test_loss:123.7 test_rmse:0.05166 test_acc:0.689\n",
      "Epoch  388\n",
      "Accuracy ---->  0.717687577009201\n",
      "Iter:388 train_rmse:4.074 test_loss:123.6 test_rmse:0.05164 test_acc:0.6891\n",
      "Epoch  389\n",
      "Accuracy ---->  0.7178493440151215\n",
      "Iter:389 train_rmse:4.072 test_loss:123.6 test_rmse:0.05162 test_acc:0.6892\n",
      "Epoch  390\n",
      "Accuracy ---->  0.7180107831954956\n",
      "Iter:390 train_rmse:4.069 test_loss:123.5 test_rmse:0.05161 test_acc:0.6893\n",
      "Epoch  391\n",
      "Accuracy ---->  0.7181703746318817\n",
      "Iter:391 train_rmse:4.067 test_loss:123.4 test_rmse:0.05159 test_acc:0.6894\n",
      "Epoch  392\n",
      "Accuracy ---->  0.718329131603241\n",
      "Iter:392 train_rmse:4.065 test_loss:123.3 test_rmse:0.05158 test_acc:0.6895\n",
      "Epoch  393\n",
      "Accuracy ---->  0.7184865176677704\n",
      "Iter:393 train_rmse:4.062 test_loss:123.3 test_rmse:0.05156 test_acc:0.6896\n",
      "Epoch  394\n",
      "Accuracy ---->  0.7186430096626282\n",
      "Iter:394 train_rmse:4.06 test_loss:123.2 test_rmse:0.05154 test_acc:0.6897\n",
      "Epoch  395\n",
      "Accuracy ---->  0.718797504901886\n",
      "Iter:395 train_rmse:4.058 test_loss:123.1 test_rmse:0.05153 test_acc:0.6898\n",
      "Epoch  396\n",
      "Accuracy ---->  0.7189508676528931\n",
      "Iter:396 train_rmse:4.056 test_loss:123.0 test_rmse:0.05151 test_acc:0.6899\n",
      "Epoch  397\n",
      "Accuracy ---->  0.7191019058227539\n",
      "Iter:397 train_rmse:4.054 test_loss:123.0 test_rmse:0.0515 test_acc:0.69\n",
      "Epoch  398\n",
      "Accuracy ---->  0.7192520201206207\n",
      "Iter:398 train_rmse:4.051 test_loss:122.9 test_rmse:0.05148 test_acc:0.6901\n",
      "Epoch  399\n",
      "Accuracy ---->  0.7193998992443085\n",
      "Iter:399 train_rmse:4.049 test_loss:122.8 test_rmse:0.05146 test_acc:0.6902\n",
      "Epoch  400\n",
      "Accuracy ---->  0.7195454835891724\n",
      "Iter:400 train_rmse:4.047 test_loss:122.7 test_rmse:0.05145 test_acc:0.6902\n",
      "Epoch  401\n",
      "Accuracy ---->  0.7196888327598572\n",
      "Iter:401 train_rmse:4.045 test_loss:122.7 test_rmse:0.05143 test_acc:0.6903\n",
      "Epoch  402\n",
      "Accuracy ---->  0.7198309898376465\n",
      "Iter:402 train_rmse:4.043 test_loss:122.6 test_rmse:0.05142 test_acc:0.6904\n",
      "Epoch  403\n",
      "Accuracy ---->  0.7199707329273224\n",
      "Iter:403 train_rmse:4.041 test_loss:122.5 test_rmse:0.0514 test_acc:0.6905\n",
      "Epoch  404\n",
      "Accuracy ---->  0.7201083302497864\n",
      "Iter:404 train_rmse:4.039 test_loss:122.5 test_rmse:0.05139 test_acc:0.6906\n",
      "Epoch  405\n",
      "Accuracy ---->  0.7202438414096832\n",
      "Iter:405 train_rmse:4.037 test_loss:122.4 test_rmse:0.05137 test_acc:0.6907\n",
      "Epoch  406\n",
      "Accuracy ---->  0.7203772664070129\n",
      "Iter:406 train_rmse:4.035 test_loss:122.3 test_rmse:0.05136 test_acc:0.6908\n",
      "Epoch  407\n",
      "Accuracy ---->  0.720508486032486\n",
      "Iter:407 train_rmse:4.033 test_loss:122.2 test_rmse:0.05134 test_acc:0.6909\n",
      "Epoch  408\n",
      "Accuracy ---->  0.7206372022628784\n",
      "Iter:408 train_rmse:4.031 test_loss:122.2 test_rmse:0.05133 test_acc:0.691\n",
      "Epoch  409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.7207638025283813\n",
      "Iter:409 train_rmse:4.03 test_loss:122.1 test_rmse:0.05131 test_acc:0.6911\n",
      "Epoch  410\n",
      "Accuracy ---->  0.7208885550498962\n",
      "Iter:410 train_rmse:4.028 test_loss:122.0 test_rmse:0.0513 test_acc:0.6911\n",
      "Epoch  411\n",
      "Accuracy ---->  0.7210103273391724\n",
      "Iter:411 train_rmse:4.026 test_loss:122.0 test_rmse:0.05129 test_acc:0.6912\n",
      "Epoch  412\n",
      "Accuracy ---->  0.7211306989192963\n",
      "Iter:412 train_rmse:4.024 test_loss:121.9 test_rmse:0.05127 test_acc:0.6913\n",
      "Epoch  413\n",
      "Accuracy ---->  0.7212490737438202\n",
      "Iter:413 train_rmse:4.023 test_loss:121.8 test_rmse:0.05126 test_acc:0.6914\n",
      "Epoch  414\n",
      "Accuracy ---->  0.7213646769523621\n",
      "Iter:414 train_rmse:4.021 test_loss:121.8 test_rmse:0.05124 test_acc:0.6915\n",
      "Epoch  415\n",
      "Accuracy ---->  0.7214789092540741\n",
      "Iter:415 train_rmse:4.019 test_loss:121.7 test_rmse:0.05123 test_acc:0.6916\n",
      "Epoch  416\n",
      "Accuracy ---->  0.7215901613235474\n",
      "Iter:416 train_rmse:4.018 test_loss:121.6 test_rmse:0.05121 test_acc:0.6917\n",
      "Epoch  417\n",
      "Accuracy ---->  0.7217007577419281\n",
      "Iter:417 train_rmse:4.016 test_loss:121.6 test_rmse:0.0512 test_acc:0.6917\n",
      "Epoch  418\n",
      "Accuracy ---->  0.721808671951294\n",
      "Iter:418 train_rmse:4.014 test_loss:121.5 test_rmse:0.05119 test_acc:0.6918\n",
      "Epoch  419\n",
      "Accuracy ---->  0.7219148874282837\n",
      "Iter:419 train_rmse:4.013 test_loss:121.4 test_rmse:0.05117 test_acc:0.6919\n",
      "Epoch  420\n",
      "Accuracy ---->  0.7220191061496735\n",
      "Iter:420 train_rmse:4.011 test_loss:121.4 test_rmse:0.05116 test_acc:0.692\n",
      "Epoch  421\n",
      "Accuracy ---->  0.7221214175224304\n",
      "Iter:421 train_rmse:4.01 test_loss:121.3 test_rmse:0.05115 test_acc:0.6921\n",
      "Epoch  422\n",
      "Accuracy ---->  0.7222224771976471\n",
      "Iter:422 train_rmse:4.009 test_loss:121.2 test_rmse:0.05113 test_acc:0.6922\n",
      "Epoch  423\n",
      "Accuracy ---->  0.7223213016986847\n",
      "Iter:423 train_rmse:4.007 test_loss:121.2 test_rmse:0.05112 test_acc:0.6922\n",
      "Epoch  424\n",
      "Accuracy ---->  0.722418874502182\n",
      "Iter:424 train_rmse:4.006 test_loss:121.1 test_rmse:0.0511 test_acc:0.6923\n",
      "Epoch  425\n",
      "Accuracy ---->  0.7225149571895599\n",
      "Iter:425 train_rmse:4.004 test_loss:121.0 test_rmse:0.05109 test_acc:0.6924\n",
      "Epoch  426\n",
      "Accuracy ---->  0.7226095497608185\n",
      "Iter:426 train_rmse:4.003 test_loss:121.0 test_rmse:0.05108 test_acc:0.6925\n",
      "Epoch  427\n",
      "Accuracy ---->  0.7227025330066681\n",
      "Iter:427 train_rmse:4.002 test_loss:120.9 test_rmse:0.05106 test_acc:0.6926\n",
      "Epoch  428\n",
      "Accuracy ---->  0.7227942645549774\n",
      "Iter:428 train_rmse:4.0 test_loss:120.9 test_rmse:0.05105 test_acc:0.6926\n",
      "Epoch  429\n",
      "Accuracy ---->  0.7228848934173584\n",
      "Iter:429 train_rmse:3.999 test_loss:120.8 test_rmse:0.05104 test_acc:0.6927\n",
      "Epoch  430\n",
      "Accuracy ---->  0.7229736149311066\n",
      "Iter:430 train_rmse:3.998 test_loss:120.7 test_rmse:0.05102 test_acc:0.6928\n",
      "Epoch  431\n",
      "Accuracy ---->  0.7230615019798279\n",
      "Iter:431 train_rmse:3.996 test_loss:120.7 test_rmse:0.05101 test_acc:0.6929\n",
      "Epoch  432\n",
      "Accuracy ---->  0.7231481373310089\n",
      "Iter:432 train_rmse:3.995 test_loss:120.6 test_rmse:0.051 test_acc:0.693\n",
      "Epoch  433\n",
      "Accuracy ---->  0.7232340276241302\n",
      "Iter:433 train_rmse:3.994 test_loss:120.6 test_rmse:0.05099 test_acc:0.693\n",
      "Epoch  434\n",
      "Accuracy ---->  0.7233181893825531\n",
      "Iter:434 train_rmse:3.993 test_loss:120.5 test_rmse:0.05097 test_acc:0.6931\n",
      "Epoch  435\n",
      "Accuracy ---->  0.7234015166759491\n",
      "Iter:435 train_rmse:3.992 test_loss:120.4 test_rmse:0.05096 test_acc:0.6932\n",
      "Epoch  436\n",
      "Accuracy ---->  0.7234839200973511\n",
      "Iter:436 train_rmse:3.99 test_loss:120.4 test_rmse:0.05095 test_acc:0.6933\n",
      "Epoch  437\n",
      "Accuracy ---->  0.7235655188560486\n",
      "Iter:437 train_rmse:3.989 test_loss:120.3 test_rmse:0.05094 test_acc:0.6933\n",
      "Epoch  438\n",
      "Accuracy ---->  0.7236453890800476\n",
      "Iter:438 train_rmse:3.988 test_loss:120.3 test_rmse:0.05092 test_acc:0.6934\n",
      "Epoch  439\n",
      "Accuracy ---->  0.7237254083156586\n",
      "Iter:439 train_rmse:3.987 test_loss:120.2 test_rmse:0.05091 test_acc:0.6935\n",
      "Epoch  440\n",
      "Accuracy ---->  0.7238040566444397\n",
      "Iter:440 train_rmse:3.986 test_loss:120.1 test_rmse:0.0509 test_acc:0.6936\n",
      "Epoch  441\n",
      "Accuracy ---->  0.7238821089267731\n",
      "Iter:441 train_rmse:3.985 test_loss:120.1 test_rmse:0.05089 test_acc:0.6936\n",
      "Epoch  442\n",
      "Accuracy ---->  0.7239592969417572\n",
      "Iter:442 train_rmse:3.983 test_loss:120.0 test_rmse:0.05088 test_acc:0.6937\n",
      "Epoch  443\n",
      "Accuracy ---->  0.7240354716777802\n",
      "Iter:443 train_rmse:3.982 test_loss:120.0 test_rmse:0.05086 test_acc:0.6938\n",
      "Epoch  444\n",
      "Accuracy ---->  0.7241106033325195\n",
      "Iter:444 train_rmse:3.981 test_loss:119.9 test_rmse:0.05085 test_acc:0.6938\n",
      "Epoch  445\n",
      "Accuracy ---->  0.7241857349872589\n",
      "Iter:445 train_rmse:3.98 test_loss:119.9 test_rmse:0.05084 test_acc:0.6939\n",
      "Epoch  446\n",
      "Accuracy ---->  0.7242594063282013\n",
      "Iter:446 train_rmse:3.979 test_loss:119.8 test_rmse:0.05083 test_acc:0.694\n",
      "Epoch  447\n",
      "Accuracy ---->  0.7243331372737885\n",
      "Iter:447 train_rmse:3.978 test_loss:119.8 test_rmse:0.05082 test_acc:0.6941\n",
      "Epoch  448\n",
      "Accuracy ---->  0.7244056463241577\n",
      "Iter:448 train_rmse:3.977 test_loss:119.7 test_rmse:0.05081 test_acc:0.6941\n",
      "Epoch  449\n",
      "Accuracy ---->  0.7244778573513031\n",
      "Iter:449 train_rmse:3.976 test_loss:119.7 test_rmse:0.05079 test_acc:0.6942\n",
      "Epoch  450\n",
      "Accuracy ---->  0.7245491147041321\n",
      "Iter:450 train_rmse:3.975 test_loss:119.6 test_rmse:0.05078 test_acc:0.6943\n",
      "Epoch  451\n",
      "Accuracy ---->  0.7246197164058685\n",
      "Iter:451 train_rmse:3.974 test_loss:119.6 test_rmse:0.05077 test_acc:0.6943\n",
      "Epoch  452\n",
      "Accuracy ---->  0.7246904969215393\n",
      "Iter:452 train_rmse:3.973 test_loss:119.5 test_rmse:0.05076 test_acc:0.6944\n",
      "Epoch  453\n",
      "Accuracy ---->  0.724760115146637\n",
      "Iter:453 train_rmse:3.972 test_loss:119.4 test_rmse:0.05075 test_acc:0.6945\n",
      "Epoch  454\n",
      "Accuracy ---->  0.7248291075229645\n",
      "Iter:454 train_rmse:3.971 test_loss:119.4 test_rmse:0.05074 test_acc:0.6945\n",
      "Epoch  455\n",
      "Accuracy ---->  0.7248978912830353\n",
      "Iter:455 train_rmse:3.97 test_loss:119.3 test_rmse:0.05073 test_acc:0.6946\n",
      "Epoch  456\n",
      "Accuracy ---->  0.7249664068222046\n",
      "Iter:456 train_rmse:3.969 test_loss:119.3 test_rmse:0.05072 test_acc:0.6947\n",
      "Epoch  457\n",
      "Accuracy ---->  0.7250335514545441\n",
      "Iter:457 train_rmse:3.968 test_loss:119.2 test_rmse:0.05071 test_acc:0.6947\n",
      "Epoch  458\n",
      "Accuracy ---->  0.7251010239124298\n",
      "Iter:458 train_rmse:3.967 test_loss:119.2 test_rmse:0.0507 test_acc:0.6948\n",
      "Epoch  459\n",
      "Accuracy ---->  0.7251676023006439\n",
      "Iter:459 train_rmse:3.966 test_loss:119.1 test_rmse:0.05069 test_acc:0.6948\n",
      "Epoch  460\n",
      "Accuracy ---->  0.72523432970047\n",
      "Iter:460 train_rmse:3.965 test_loss:119.1 test_rmse:0.05068 test_acc:0.6949\n",
      "Epoch  461\n",
      "Accuracy ---->  0.7252998352050781\n",
      "Iter:461 train_rmse:3.964 test_loss:119.0 test_rmse:0.05066 test_acc:0.695\n",
      "Epoch  462\n",
      "Accuracy ---->  0.7253651022911072\n",
      "Iter:462 train_rmse:3.963 test_loss:119.0 test_rmse:0.05065 test_acc:0.695\n",
      "Epoch  463\n",
      "Accuracy ---->  0.7254303395748138\n",
      "Iter:463 train_rmse:3.962 test_loss:119.0 test_rmse:0.05064 test_acc:0.6951\n",
      "Epoch  464\n",
      "Accuracy ---->  0.725494772195816\n",
      "Iter:464 train_rmse:3.961 test_loss:118.9 test_rmse:0.05063 test_acc:0.6952\n",
      "Epoch  465\n",
      "Accuracy ---->  0.7255593836307526\n",
      "Iter:465 train_rmse:3.96 test_loss:118.9 test_rmse:0.05062 test_acc:0.6952\n",
      "Epoch  466\n",
      "Accuracy ---->  0.7256234884262085\n",
      "Iter:466 train_rmse:3.959 test_loss:118.8 test_rmse:0.05061 test_acc:0.6953\n",
      "Epoch  467\n",
      "Accuracy ---->  0.7256864905357361\n",
      "Iter:467 train_rmse:3.959 test_loss:118.8 test_rmse:0.0506 test_acc:0.6953\n",
      "Epoch  468\n",
      "Accuracy ---->  0.7257499098777771\n",
      "Iter:468 train_rmse:3.958 test_loss:118.7 test_rmse:0.05059 test_acc:0.6954\n",
      "Epoch  469\n",
      "Accuracy ---->  0.7258126139640808\n",
      "Iter:469 train_rmse:3.957 test_loss:118.7 test_rmse:0.05058 test_acc:0.6955\n",
      "Epoch  470\n",
      "Accuracy ---->  0.7258756160736084\n",
      "Iter:470 train_rmse:3.956 test_loss:118.6 test_rmse:0.05057 test_acc:0.6955\n",
      "Epoch  471\n",
      "Accuracy ---->  0.7259376049041748\n",
      "Iter:471 train_rmse:3.955 test_loss:118.6 test_rmse:0.05056 test_acc:0.6956\n",
      "Epoch  472\n",
      "Accuracy ---->  0.7259992063045502\n",
      "Iter:472 train_rmse:3.954 test_loss:118.5 test_rmse:0.05055 test_acc:0.6957\n",
      "Epoch  473\n",
      "Accuracy ---->  0.7260611653327942\n",
      "Iter:473 train_rmse:3.953 test_loss:118.5 test_rmse:0.05054 test_acc:0.6957\n",
      "Epoch  474\n",
      "Accuracy ---->  0.7261224687099457\n",
      "Iter:474 train_rmse:3.952 test_loss:118.4 test_rmse:0.05053 test_acc:0.6958\n",
      "Epoch  475\n",
      "Accuracy ---->  0.7261834442615509\n",
      "Iter:475 train_rmse:3.951 test_loss:118.4 test_rmse:0.05052 test_acc:0.6958\n",
      "Epoch  476\n",
      "Accuracy ---->  0.7262440323829651\n",
      "Iter:476 train_rmse:3.95 test_loss:118.3 test_rmse:0.05051 test_acc:0.6959\n",
      "Epoch  477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.7263039648532867\n",
      "Iter:477 train_rmse:3.95 test_loss:118.3 test_rmse:0.0505 test_acc:0.696\n",
      "Epoch  478\n",
      "Accuracy ---->  0.7263640463352203\n",
      "Iter:478 train_rmse:3.949 test_loss:118.2 test_rmse:0.05049 test_acc:0.696\n",
      "Epoch  479\n",
      "Accuracy ---->  0.7264237403869629\n",
      "Iter:479 train_rmse:3.948 test_loss:118.2 test_rmse:0.05048 test_acc:0.6961\n",
      "Epoch  480\n",
      "Accuracy ---->  0.7264832556247711\n",
      "Iter:480 train_rmse:3.947 test_loss:118.1 test_rmse:0.05047 test_acc:0.6961\n",
      "Epoch  481\n",
      "Accuracy ---->  0.7265424728393555\n",
      "Iter:481 train_rmse:3.946 test_loss:118.1 test_rmse:0.05046 test_acc:0.6962\n",
      "Epoch  482\n",
      "Accuracy ---->  0.7266009151935577\n",
      "Iter:482 train_rmse:3.945 test_loss:118.1 test_rmse:0.05045 test_acc:0.6963\n",
      "Epoch  483\n",
      "Accuracy ---->  0.7266592383384705\n",
      "Iter:483 train_rmse:3.944 test_loss:118.0 test_rmse:0.05044 test_acc:0.6963\n",
      "Epoch  484\n",
      "Accuracy ---->  0.7267175018787384\n",
      "Iter:484 train_rmse:3.944 test_loss:118.0 test_rmse:0.05043 test_acc:0.6964\n",
      "Epoch  485\n",
      "Accuracy ---->  0.7267746925354004\n",
      "Iter:485 train_rmse:3.943 test_loss:117.9 test_rmse:0.05042 test_acc:0.6964\n",
      "Epoch  486\n",
      "Accuracy ---->  0.7268321216106415\n",
      "Iter:486 train_rmse:3.942 test_loss:117.9 test_rmse:0.05041 test_acc:0.6965\n",
      "Epoch  487\n",
      "Accuracy ---->  0.7268887460231781\n",
      "Iter:487 train_rmse:3.941 test_loss:117.8 test_rmse:0.0504 test_acc:0.6966\n",
      "Epoch  488\n",
      "Accuracy ---->  0.7269453406333923\n",
      "Iter:488 train_rmse:3.94 test_loss:117.8 test_rmse:0.05039 test_acc:0.6966\n",
      "Epoch  489\n",
      "Accuracy ---->  0.727001279592514\n",
      "Iter:489 train_rmse:3.94 test_loss:117.7 test_rmse:0.05038 test_acc:0.6967\n",
      "Epoch  490\n",
      "Accuracy ---->  0.7270571291446686\n",
      "Iter:490 train_rmse:3.939 test_loss:117.7 test_rmse:0.05037 test_acc:0.6967\n",
      "Epoch  491\n",
      "Accuracy ---->  0.7271126210689545\n",
      "Iter:491 train_rmse:3.938 test_loss:117.6 test_rmse:0.05036 test_acc:0.6968\n",
      "Epoch  492\n",
      "Accuracy ---->  0.727167546749115\n",
      "Iter:492 train_rmse:3.937 test_loss:117.6 test_rmse:0.05035 test_acc:0.6968\n",
      "Epoch  493\n",
      "Accuracy ---->  0.7272217571735382\n",
      "Iter:493 train_rmse:3.936 test_loss:117.6 test_rmse:0.05034 test_acc:0.6969\n",
      "Epoch  494\n",
      "Accuracy ---->  0.727276086807251\n",
      "Iter:494 train_rmse:3.936 test_loss:117.5 test_rmse:0.05034 test_acc:0.697\n",
      "Epoch  495\n",
      "Accuracy ---->  0.7273298799991608\n",
      "Iter:495 train_rmse:3.935 test_loss:117.5 test_rmse:0.05033 test_acc:0.697\n",
      "Epoch  496\n",
      "Accuracy ---->  0.7273833155632019\n",
      "Iter:496 train_rmse:3.934 test_loss:117.4 test_rmse:0.05032 test_acc:0.6971\n",
      "Epoch  497\n",
      "Accuracy ---->  0.7274360060691833\n",
      "Iter:497 train_rmse:3.933 test_loss:117.4 test_rmse:0.05031 test_acc:0.6971\n",
      "Epoch  498\n",
      "Accuracy ---->  0.7274889349937439\n",
      "Iter:498 train_rmse:3.933 test_loss:117.3 test_rmse:0.0503 test_acc:0.6972\n",
      "Epoch  499\n",
      "Accuracy ---->  0.7275409400463104\n",
      "Iter:499 train_rmse:3.932 test_loss:117.3 test_rmse:0.05029 test_acc:0.6972\n",
      "Epoch  500\n",
      "Accuracy ---->  0.7275928258895874\n",
      "Iter:500 train_rmse:3.931 test_loss:117.3 test_rmse:0.05028 test_acc:0.6973\n",
      "Epoch  501\n",
      "Accuracy ---->  0.727644294500351\n",
      "Iter:501 train_rmse:3.93 test_loss:117.2 test_rmse:0.05027 test_acc:0.6974\n",
      "Epoch  502\n",
      "Accuracy ---->  0.727695643901825\n",
      "Iter:502 train_rmse:3.93 test_loss:117.2 test_rmse:0.05026 test_acc:0.6974\n",
      "Epoch  503\n",
      "Accuracy ---->  0.7277460694313049\n",
      "Iter:503 train_rmse:3.929 test_loss:117.1 test_rmse:0.05025 test_acc:0.6975\n",
      "Epoch  504\n",
      "Accuracy ---->  0.7277958691120148\n",
      "Iter:504 train_rmse:3.928 test_loss:117.1 test_rmse:0.05024 test_acc:0.6975\n",
      "Epoch  505\n",
      "Accuracy ---->  0.7278465032577515\n",
      "Iter:505 train_rmse:3.927 test_loss:117.0 test_rmse:0.05023 test_acc:0.6976\n",
      "Epoch  506\n",
      "Accuracy ---->  0.7278960347175598\n",
      "Iter:506 train_rmse:3.927 test_loss:117.0 test_rmse:0.05022 test_acc:0.6976\n",
      "Epoch  507\n",
      "Accuracy ---->  0.727944940328598\n",
      "Iter:507 train_rmse:3.926 test_loss:117.0 test_rmse:0.05022 test_acc:0.6977\n",
      "Epoch  508\n",
      "Accuracy ---->  0.7279939651489258\n",
      "Iter:508 train_rmse:3.925 test_loss:116.9 test_rmse:0.05021 test_acc:0.6977\n",
      "Epoch  509\n",
      "Accuracy ---->  0.7280425727367401\n",
      "Iter:509 train_rmse:3.925 test_loss:116.9 test_rmse:0.0502 test_acc:0.6978\n",
      "Epoch  510\n",
      "Accuracy ---->  0.7280908823013306\n",
      "Iter:510 train_rmse:3.924 test_loss:116.8 test_rmse:0.05019 test_acc:0.6978\n",
      "Epoch  511\n",
      "Accuracy ---->  0.7281389832496643\n",
      "Iter:511 train_rmse:3.923 test_loss:116.8 test_rmse:0.05018 test_acc:0.6979\n",
      "Epoch  512\n",
      "Accuracy ---->  0.7281865179538727\n",
      "Iter:512 train_rmse:3.922 test_loss:116.8 test_rmse:0.05017 test_acc:0.6979\n",
      "Epoch  513\n",
      "Accuracy ---->  0.7282340824604034\n",
      "Iter:513 train_rmse:3.922 test_loss:116.7 test_rmse:0.05016 test_acc:0.698\n",
      "Epoch  514\n",
      "Accuracy ---->  0.7282814979553223\n",
      "Iter:514 train_rmse:3.921 test_loss:116.7 test_rmse:0.05015 test_acc:0.698\n",
      "Epoch  515\n",
      "Accuracy ---->  0.7283287048339844\n",
      "Iter:515 train_rmse:3.92 test_loss:116.6 test_rmse:0.05015 test_acc:0.6981\n",
      "Epoch  516\n",
      "Accuracy ---->  0.7283756136894226\n",
      "Iter:516 train_rmse:3.92 test_loss:116.6 test_rmse:0.05014 test_acc:0.6981\n",
      "Epoch  517\n",
      "Accuracy ---->  0.7284219563007355\n",
      "Iter:517 train_rmse:3.919 test_loss:116.6 test_rmse:0.05013 test_acc:0.6982\n",
      "Epoch  518\n",
      "Accuracy ---->  0.7284685969352722\n",
      "Iter:518 train_rmse:3.918 test_loss:116.5 test_rmse:0.05012 test_acc:0.6982\n",
      "Epoch  519\n",
      "Accuracy ---->  0.7285148203372955\n",
      "Iter:519 train_rmse:3.918 test_loss:116.5 test_rmse:0.05011 test_acc:0.6983\n",
      "Epoch  520\n",
      "Accuracy ---->  0.7285610437393188\n",
      "Iter:520 train_rmse:3.917 test_loss:116.4 test_rmse:0.0501 test_acc:0.6983\n",
      "Epoch  521\n",
      "Accuracy ---->  0.7286067605018616\n",
      "Iter:521 train_rmse:3.916 test_loss:116.4 test_rmse:0.0501 test_acc:0.6984\n",
      "Epoch  522\n",
      "Accuracy ---->  0.7286527156829834\n",
      "Iter:522 train_rmse:3.916 test_loss:116.4 test_rmse:0.05009 test_acc:0.6984\n",
      "Epoch  523\n",
      "Accuracy ---->  0.7286986112594604\n",
      "Iter:523 train_rmse:3.915 test_loss:116.3 test_rmse:0.05008 test_acc:0.6985\n",
      "Epoch  524\n",
      "Accuracy ---->  0.7287441194057465\n",
      "Iter:524 train_rmse:3.914 test_loss:116.3 test_rmse:0.05007 test_acc:0.6985\n",
      "Epoch  525\n",
      "Accuracy ---->  0.7287898063659668\n",
      "Iter:525 train_rmse:3.914 test_loss:116.3 test_rmse:0.05006 test_acc:0.6986\n",
      "Epoch  526\n",
      "Accuracy ---->  0.7288350164890289\n",
      "Iter:526 train_rmse:3.913 test_loss:116.2 test_rmse:0.05006 test_acc:0.6986\n",
      "Epoch  527\n",
      "Accuracy ---->  0.7288804352283478\n",
      "Iter:527 train_rmse:3.912 test_loss:116.2 test_rmse:0.05005 test_acc:0.6987\n",
      "Epoch  528\n",
      "Accuracy ---->  0.728926032781601\n",
      "Iter:528 train_rmse:3.912 test_loss:116.1 test_rmse:0.05004 test_acc:0.6987\n",
      "Epoch  529\n",
      "Accuracy ---->  0.7289712727069855\n",
      "Iter:529 train_rmse:3.911 test_loss:116.1 test_rmse:0.05003 test_acc:0.6988\n",
      "Epoch  530\n",
      "Accuracy ---->  0.72901651263237\n",
      "Iter:530 train_rmse:3.91 test_loss:116.1 test_rmse:0.05003 test_acc:0.6988\n",
      "Epoch  531\n",
      "Accuracy ---->  0.729061633348465\n",
      "Iter:531 train_rmse:3.91 test_loss:116.0 test_rmse:0.05002 test_acc:0.6989\n",
      "Epoch  532\n",
      "Accuracy ---->  0.7291069030761719\n",
      "Iter:532 train_rmse:3.909 test_loss:116.0 test_rmse:0.05001 test_acc:0.6989\n",
      "Epoch  533\n",
      "Accuracy ---->  0.7291519045829773\n",
      "Iter:533 train_rmse:3.909 test_loss:116.0 test_rmse:0.05 test_acc:0.699\n",
      "Epoch  534\n",
      "Accuracy ---->  0.7291973531246185\n",
      "Iter:534 train_rmse:3.908 test_loss:115.9 test_rmse:0.05 test_acc:0.699\n",
      "Epoch  535\n",
      "Accuracy ---->  0.729242354631424\n",
      "Iter:535 train_rmse:3.907 test_loss:115.9 test_rmse:0.04999 test_acc:0.699\n",
      "Epoch  536\n",
      "Accuracy ---->  0.7292875647544861\n",
      "Iter:536 train_rmse:3.907 test_loss:115.9 test_rmse:0.04998 test_acc:0.6991\n",
      "Epoch  537\n",
      "Accuracy ---->  0.729332447052002\n",
      "Iter:537 train_rmse:3.906 test_loss:115.8 test_rmse:0.04997 test_acc:0.6991\n",
      "Epoch  538\n",
      "Accuracy ---->  0.7293773293495178\n",
      "Iter:538 train_rmse:3.905 test_loss:115.8 test_rmse:0.04997 test_acc:0.6992\n",
      "Epoch  539\n",
      "Accuracy ---->  0.7294218242168427\n",
      "Iter:539 train_rmse:3.905 test_loss:115.8 test_rmse:0.04996 test_acc:0.6992\n",
      "Epoch  540\n",
      "Accuracy ---->  0.7294666767120361\n",
      "Iter:540 train_rmse:3.904 test_loss:115.7 test_rmse:0.04995 test_acc:0.6993\n",
      "Epoch  541\n",
      "Accuracy ---->  0.7295114099979401\n",
      "Iter:541 train_rmse:3.903 test_loss:115.7 test_rmse:0.04994 test_acc:0.6993\n",
      "Epoch  542\n",
      "Accuracy ---->  0.729555755853653\n",
      "Iter:542 train_rmse:3.903 test_loss:115.7 test_rmse:0.04994 test_acc:0.6994\n",
      "Epoch  543\n",
      "Accuracy ---->  0.7296003103256226\n",
      "Iter:543 train_rmse:3.902 test_loss:115.6 test_rmse:0.04993 test_acc:0.6994\n",
      "Epoch  544\n",
      "Accuracy ---->  0.7296443283557892\n",
      "Iter:544 train_rmse:3.901 test_loss:115.6 test_rmse:0.04992 test_acc:0.6994\n",
      "Epoch  545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.729688823223114\n",
      "Iter:545 train_rmse:3.901 test_loss:115.6 test_rmse:0.04992 test_acc:0.6995\n",
      "Epoch  546\n",
      "Accuracy ---->  0.7297326624393463\n",
      "Iter:546 train_rmse:3.9 test_loss:115.5 test_rmse:0.04991 test_acc:0.6995\n",
      "Epoch  547\n",
      "Accuracy ---->  0.7297757565975189\n",
      "Iter:547 train_rmse:3.9 test_loss:115.5 test_rmse:0.0499 test_acc:0.6996\n",
      "Epoch  548\n",
      "Accuracy ---->  0.7298192381858826\n",
      "Iter:548 train_rmse:3.899 test_loss:115.5 test_rmse:0.0499 test_acc:0.6996\n",
      "Epoch  549\n",
      "Accuracy ---->  0.7298624813556671\n",
      "Iter:549 train_rmse:3.898 test_loss:115.5 test_rmse:0.04989 test_acc:0.6996\n",
      "Epoch  550\n",
      "Accuracy ---->  0.7299054861068726\n",
      "Iter:550 train_rmse:3.898 test_loss:115.4 test_rmse:0.04988 test_acc:0.6997\n",
      "Epoch  551\n",
      "Accuracy ---->  0.7299479842185974\n",
      "Iter:551 train_rmse:3.897 test_loss:115.4 test_rmse:0.04988 test_acc:0.6997\n",
      "Epoch  552\n",
      "Accuracy ---->  0.7299901247024536\n",
      "Iter:552 train_rmse:3.896 test_loss:115.4 test_rmse:0.04987 test_acc:0.6998\n",
      "Epoch  553\n",
      "Accuracy ---->  0.7300318479537964\n",
      "Iter:553 train_rmse:3.896 test_loss:115.3 test_rmse:0.04986 test_acc:0.6998\n",
      "Epoch  554\n",
      "Accuracy ---->  0.7300739586353302\n",
      "Iter:554 train_rmse:3.895 test_loss:115.3 test_rmse:0.04986 test_acc:0.6998\n",
      "Epoch  555\n",
      "Accuracy ---->  0.7301147878170013\n",
      "Iter:555 train_rmse:3.895 test_loss:115.3 test_rmse:0.04985 test_acc:0.6999\n",
      "Epoch  556\n",
      "Accuracy ---->  0.7301556766033173\n",
      "Iter:556 train_rmse:3.894 test_loss:115.2 test_rmse:0.04984 test_acc:0.6999\n",
      "Epoch  557\n",
      "Accuracy ---->  0.7301961779594421\n",
      "Iter:557 train_rmse:3.893 test_loss:115.2 test_rmse:0.04984 test_acc:0.7\n",
      "Epoch  558\n",
      "Accuracy ---->  0.7302362322807312\n",
      "Iter:558 train_rmse:3.893 test_loss:115.2 test_rmse:0.04983 test_acc:0.7\n",
      "Epoch  559\n",
      "Accuracy ---->  0.7302752733230591\n",
      "Iter:559 train_rmse:3.892 test_loss:115.2 test_rmse:0.04982 test_acc:0.7\n",
      "Epoch  560\n",
      "Accuracy ---->  0.7303146421909332\n",
      "Iter:560 train_rmse:3.892 test_loss:115.1 test_rmse:0.04982 test_acc:0.7001\n",
      "Epoch  561\n",
      "Accuracy ---->  0.730353444814682\n",
      "Iter:561 train_rmse:3.891 test_loss:115.1 test_rmse:0.04981 test_acc:0.7001\n",
      "Epoch  562\n",
      "Accuracy ---->  0.730391651391983\n",
      "Iter:562 train_rmse:3.891 test_loss:115.1 test_rmse:0.04981 test_acc:0.7001\n",
      "Epoch  563\n",
      "Accuracy ---->  0.7304300367832184\n",
      "Iter:563 train_rmse:3.89 test_loss:115.0 test_rmse:0.0498 test_acc:0.7002\n",
      "Epoch  564\n",
      "Accuracy ---->  0.7304671704769135\n",
      "Iter:564 train_rmse:3.89 test_loss:115.0 test_rmse:0.04979 test_acc:0.7002\n",
      "Epoch  565\n",
      "Accuracy ---->  0.7305042147636414\n",
      "Iter:565 train_rmse:3.889 test_loss:115.0 test_rmse:0.04979 test_acc:0.7003\n",
      "Epoch  566\n",
      "Accuracy ---->  0.7305412888526917\n",
      "Iter:566 train_rmse:3.888 test_loss:115.0 test_rmse:0.04978 test_acc:0.7003\n",
      "Epoch  567\n",
      "Accuracy ---->  0.7305774986743927\n",
      "Iter:567 train_rmse:3.888 test_loss:114.9 test_rmse:0.04977 test_acc:0.7003\n",
      "Epoch  568\n",
      "Accuracy ---->  0.7306137084960938\n",
      "Iter:568 train_rmse:3.887 test_loss:114.9 test_rmse:0.04977 test_acc:0.7004\n",
      "Epoch  569\n",
      "Accuracy ---->  0.7306495308876038\n",
      "Iter:569 train_rmse:3.887 test_loss:114.9 test_rmse:0.04976 test_acc:0.7004\n",
      "Epoch  570\n",
      "Accuracy ---->  0.730684369802475\n",
      "Iter:570 train_rmse:3.886 test_loss:114.8 test_rmse:0.04976 test_acc:0.7004\n",
      "Epoch  571\n",
      "Accuracy ---->  0.730719119310379\n",
      "Iter:571 train_rmse:3.886 test_loss:114.8 test_rmse:0.04975 test_acc:0.7005\n",
      "Epoch  572\n",
      "Accuracy ---->  0.7307538986206055\n",
      "Iter:572 train_rmse:3.885 test_loss:114.8 test_rmse:0.04974 test_acc:0.7005\n",
      "Epoch  573\n",
      "Accuracy ---->  0.7307886183261871\n",
      "Iter:573 train_rmse:3.885 test_loss:114.8 test_rmse:0.04974 test_acc:0.7006\n",
      "Epoch  574\n",
      "Accuracy ---->  0.7308225333690643\n",
      "Iter:574 train_rmse:3.884 test_loss:114.7 test_rmse:0.04973 test_acc:0.7006\n",
      "Epoch  575\n",
      "Accuracy ---->  0.7308561205863953\n",
      "Iter:575 train_rmse:3.884 test_loss:114.7 test_rmse:0.04972 test_acc:0.7006\n",
      "Epoch  576\n",
      "Accuracy ---->  0.7308900952339172\n",
      "Iter:576 train_rmse:3.883 test_loss:114.7 test_rmse:0.04972 test_acc:0.7007\n",
      "Epoch  577\n",
      "Accuracy ---->  0.7309233844280243\n",
      "Iter:577 train_rmse:3.883 test_loss:114.6 test_rmse:0.04971 test_acc:0.7007\n",
      "Epoch  578\n",
      "Accuracy ---->  0.7309565246105194\n",
      "Iter:578 train_rmse:3.882 test_loss:114.6 test_rmse:0.04971 test_acc:0.7007\n",
      "Epoch  579\n",
      "Accuracy ---->  0.730989396572113\n",
      "Iter:579 train_rmse:3.882 test_loss:114.6 test_rmse:0.0497 test_acc:0.7008\n",
      "Epoch  580\n",
      "Accuracy ---->  0.7310225665569305\n",
      "Iter:580 train_rmse:3.882 test_loss:114.6 test_rmse:0.04969 test_acc:0.7008\n",
      "Epoch  581\n",
      "Accuracy ---->  0.7310552000999451\n",
      "Iter:581 train_rmse:3.881 test_loss:114.5 test_rmse:0.04969 test_acc:0.7009\n",
      "Epoch  582\n",
      "Accuracy ---->  0.73108771443367\n",
      "Iter:582 train_rmse:3.881 test_loss:114.5 test_rmse:0.04968 test_acc:0.7009\n",
      "Epoch  583\n",
      "Accuracy ---->  0.7311200499534607\n",
      "Iter:583 train_rmse:3.88 test_loss:114.5 test_rmse:0.04968 test_acc:0.7009\n",
      "Epoch  584\n",
      "Accuracy ---->  0.7311524152755737\n",
      "Iter:584 train_rmse:3.88 test_loss:114.4 test_rmse:0.04967 test_acc:0.701\n",
      "Epoch  585\n",
      "Accuracy ---->  0.7311843633651733\n",
      "Iter:585 train_rmse:3.879 test_loss:114.4 test_rmse:0.04966 test_acc:0.701\n",
      "Epoch  586\n",
      "Accuracy ---->  0.7312166392803192\n",
      "Iter:586 train_rmse:3.879 test_loss:114.4 test_rmse:0.04966 test_acc:0.701\n",
      "Epoch  587\n",
      "Accuracy ---->  0.7312484681606293\n",
      "Iter:587 train_rmse:3.878 test_loss:114.4 test_rmse:0.04965 test_acc:0.7011\n",
      "Epoch  588\n",
      "Accuracy ---->  0.7312804460525513\n",
      "Iter:588 train_rmse:3.878 test_loss:114.3 test_rmse:0.04964 test_acc:0.7011\n",
      "Epoch  589\n",
      "Accuracy ---->  0.7313127815723419\n",
      "Iter:589 train_rmse:3.877 test_loss:114.3 test_rmse:0.04964 test_acc:0.7012\n",
      "Epoch  590\n",
      "Accuracy ---->  0.7313439548015594\n",
      "Iter:590 train_rmse:3.877 test_loss:114.3 test_rmse:0.04963 test_acc:0.7012\n",
      "Epoch  591\n",
      "Accuracy ---->  0.7313760817050934\n",
      "Iter:591 train_rmse:3.876 test_loss:114.2 test_rmse:0.04963 test_acc:0.7012\n",
      "Epoch  592\n",
      "Accuracy ---->  0.7314076125621796\n",
      "Iter:592 train_rmse:3.876 test_loss:114.2 test_rmse:0.04962 test_acc:0.7013\n",
      "Epoch  593\n",
      "Accuracy ---->  0.7314390540122986\n",
      "Iter:593 train_rmse:3.876 test_loss:114.2 test_rmse:0.04961 test_acc:0.7013\n",
      "Epoch  594\n",
      "Accuracy ---->  0.7314704060554504\n",
      "Iter:594 train_rmse:3.875 test_loss:114.2 test_rmse:0.04961 test_acc:0.7013\n",
      "Epoch  595\n",
      "Accuracy ---->  0.7315016090869904\n",
      "Iter:595 train_rmse:3.875 test_loss:114.1 test_rmse:0.0496 test_acc:0.7014\n",
      "Epoch  596\n",
      "Accuracy ---->  0.7315328121185303\n",
      "Iter:596 train_rmse:3.874 test_loss:114.1 test_rmse:0.0496 test_acc:0.7014\n",
      "Epoch  597\n",
      "Accuracy ---->  0.7315638959407806\n",
      "Iter:597 train_rmse:3.874 test_loss:114.1 test_rmse:0.04959 test_acc:0.7014\n",
      "Epoch  598\n",
      "Accuracy ---->  0.7315952479839325\n",
      "Iter:598 train_rmse:3.873 test_loss:114.0 test_rmse:0.04958 test_acc:0.7015\n",
      "Epoch  599\n",
      "Accuracy ---->  0.7316267490386963\n",
      "Iter:599 train_rmse:3.873 test_loss:114.0 test_rmse:0.04958 test_acc:0.7015\n",
      "Epoch  600\n",
      "Accuracy ---->  0.7316577136516571\n",
      "Iter:600 train_rmse:3.872 test_loss:114.0 test_rmse:0.04957 test_acc:0.7016\n",
      "Epoch  601\n",
      "Accuracy ---->  0.7316884696483612\n",
      "Iter:601 train_rmse:3.872 test_loss:114.0 test_rmse:0.04956 test_acc:0.7016\n",
      "Epoch  602\n",
      "Accuracy ---->  0.7317191362380981\n",
      "Iter:602 train_rmse:3.871 test_loss:113.9 test_rmse:0.04956 test_acc:0.7016\n",
      "Epoch  603\n",
      "Accuracy ---->  0.7317501306533813\n",
      "Iter:603 train_rmse:3.871 test_loss:113.9 test_rmse:0.04955 test_acc:0.7017\n",
      "Epoch  604\n",
      "Accuracy ---->  0.7317808568477631\n",
      "Iter:604 train_rmse:3.871 test_loss:113.9 test_rmse:0.04955 test_acc:0.7017\n",
      "Epoch  605\n",
      "Accuracy ---->  0.7318109273910522\n",
      "Iter:605 train_rmse:3.87 test_loss:113.8 test_rmse:0.04954 test_acc:0.7017\n",
      "Epoch  606\n",
      "Accuracy ---->  0.731841653585434\n",
      "Iter:606 train_rmse:3.87 test_loss:113.8 test_rmse:0.04954 test_acc:0.7018\n",
      "Epoch  607\n",
      "Accuracy ---->  0.7318720519542694\n",
      "Iter:607 train_rmse:3.869 test_loss:113.8 test_rmse:0.04953 test_acc:0.7018\n",
      "Epoch  608\n",
      "Accuracy ---->  0.7319023013114929\n",
      "Iter:608 train_rmse:3.869 test_loss:113.8 test_rmse:0.04952 test_acc:0.7018\n",
      "Epoch  609\n",
      "Accuracy ---->  0.7319327592849731\n",
      "Iter:609 train_rmse:3.868 test_loss:113.7 test_rmse:0.04952 test_acc:0.7019\n",
      "Epoch  610\n",
      "Accuracy ---->  0.7319625318050385\n",
      "Iter:610 train_rmse:3.868 test_loss:113.7 test_rmse:0.04951 test_acc:0.7019\n",
      "Epoch  611\n",
      "Accuracy ---->  0.7319928109645844\n",
      "Iter:611 train_rmse:3.868 test_loss:113.7 test_rmse:0.04951 test_acc:0.702\n",
      "Epoch  612\n",
      "Accuracy ---->  0.7320230305194855\n",
      "Iter:612 train_rmse:3.867 test_loss:113.7 test_rmse:0.0495 test_acc:0.702\n",
      "Epoch  613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.7320525348186493\n",
      "Iter:613 train_rmse:3.867 test_loss:113.6 test_rmse:0.04949 test_acc:0.702\n",
      "Epoch  614\n",
      "Accuracy ---->  0.7320824861526489\n",
      "Iter:614 train_rmse:3.866 test_loss:113.6 test_rmse:0.04949 test_acc:0.7021\n",
      "Epoch  615\n",
      "Accuracy ---->  0.7321121394634247\n",
      "Iter:615 train_rmse:3.866 test_loss:113.6 test_rmse:0.04948 test_acc:0.7021\n",
      "Epoch  616\n",
      "Accuracy ---->  0.7321413457393646\n",
      "Iter:616 train_rmse:3.865 test_loss:113.6 test_rmse:0.04948 test_acc:0.7021\n",
      "Epoch  617\n",
      "Accuracy ---->  0.7321714162826538\n",
      "Iter:617 train_rmse:3.865 test_loss:113.5 test_rmse:0.04947 test_acc:0.7022\n",
      "Epoch  618\n",
      "Accuracy ---->  0.7322010099887848\n",
      "Iter:618 train_rmse:3.865 test_loss:113.5 test_rmse:0.04947 test_acc:0.7022\n",
      "Epoch  619\n",
      "Accuracy ---->  0.7322296798229218\n",
      "Iter:619 train_rmse:3.864 test_loss:113.5 test_rmse:0.04946 test_acc:0.7022\n",
      "Epoch  620\n",
      "Accuracy ---->  0.7322590053081512\n",
      "Iter:620 train_rmse:3.864 test_loss:113.5 test_rmse:0.04945 test_acc:0.7023\n",
      "Epoch  621\n",
      "Accuracy ---->  0.7322883009910583\n",
      "Iter:621 train_rmse:3.863 test_loss:113.4 test_rmse:0.04945 test_acc:0.7023\n",
      "Epoch  622\n",
      "Accuracy ---->  0.7323174774646759\n",
      "Iter:622 train_rmse:3.863 test_loss:113.4 test_rmse:0.04944 test_acc:0.7023\n",
      "Epoch  623\n",
      "Accuracy ---->  0.7323465049266815\n",
      "Iter:623 train_rmse:3.862 test_loss:113.4 test_rmse:0.04944 test_acc:0.7024\n",
      "Epoch  624\n",
      "Accuracy ---->  0.7323753833770752\n",
      "Iter:624 train_rmse:3.862 test_loss:113.3 test_rmse:0.04943 test_acc:0.7024\n",
      "Epoch  625\n",
      "Accuracy ---->  0.7324040830135345\n",
      "Iter:625 train_rmse:3.862 test_loss:113.3 test_rmse:0.04943 test_acc:0.7024\n",
      "Epoch  626\n",
      "Accuracy ---->  0.7324325740337372\n",
      "Iter:626 train_rmse:3.861 test_loss:113.3 test_rmse:0.04942 test_acc:0.7025\n",
      "Epoch  627\n",
      "Accuracy ---->  0.7324610948562622\n",
      "Iter:627 train_rmse:3.861 test_loss:113.3 test_rmse:0.04942 test_acc:0.7025\n",
      "Epoch  628\n",
      "Accuracy ---->  0.7324897050857544\n",
      "Iter:628 train_rmse:3.86 test_loss:113.2 test_rmse:0.04941 test_acc:0.7025\n",
      "Epoch  629\n",
      "Accuracy ---->  0.7325178682804108\n",
      "Iter:629 train_rmse:3.86 test_loss:113.2 test_rmse:0.0494 test_acc:0.7026\n",
      "Epoch  630\n",
      "Accuracy ---->  0.7325466871261597\n",
      "Iter:630 train_rmse:3.86 test_loss:113.2 test_rmse:0.0494 test_acc:0.7026\n",
      "Epoch  631\n",
      "Accuracy ---->  0.7325748205184937\n",
      "Iter:631 train_rmse:3.859 test_loss:113.2 test_rmse:0.04939 test_acc:0.7026\n",
      "Epoch  632\n",
      "Accuracy ---->  0.7326033115386963\n",
      "Iter:632 train_rmse:3.859 test_loss:113.1 test_rmse:0.04939 test_acc:0.7027\n",
      "Epoch  633\n",
      "Accuracy ---->  0.732631653547287\n",
      "Iter:633 train_rmse:3.858 test_loss:113.1 test_rmse:0.04938 test_acc:0.7027\n",
      "Epoch  634\n",
      "Accuracy ---->  0.732660323381424\n",
      "Iter:634 train_rmse:3.858 test_loss:113.1 test_rmse:0.04938 test_acc:0.7027\n",
      "Epoch  635\n",
      "Accuracy ---->  0.7326880395412445\n",
      "Iter:635 train_rmse:3.857 test_loss:113.1 test_rmse:0.04937 test_acc:0.7028\n",
      "Epoch  636\n",
      "Accuracy ---->  0.7327166199684143\n",
      "Iter:636 train_rmse:3.857 test_loss:113.1 test_rmse:0.04937 test_acc:0.7028\n",
      "Epoch  637\n",
      "Accuracy ---->  0.7327446043491364\n",
      "Iter:637 train_rmse:3.857 test_loss:113.0 test_rmse:0.04936 test_acc:0.7028\n",
      "Epoch  638\n",
      "Accuracy ---->  0.732772558927536\n",
      "Iter:638 train_rmse:3.856 test_loss:113.0 test_rmse:0.04936 test_acc:0.7029\n",
      "Epoch  639\n",
      "Accuracy ---->  0.7328005731105804\n",
      "Iter:639 train_rmse:3.856 test_loss:113.0 test_rmse:0.04935 test_acc:0.7029\n",
      "Epoch  640\n",
      "Accuracy ---->  0.7328289151191711\n",
      "Iter:640 train_rmse:3.855 test_loss:113.0 test_rmse:0.04935 test_acc:0.7029\n",
      "Epoch  641\n",
      "Accuracy ---->  0.7328570187091827\n",
      "Iter:641 train_rmse:3.855 test_loss:112.9 test_rmse:0.04934 test_acc:0.7029\n",
      "Epoch  642\n",
      "Accuracy ---->  0.7328852117061615\n",
      "Iter:642 train_rmse:3.855 test_loss:112.9 test_rmse:0.04934 test_acc:0.703\n",
      "Epoch  643\n",
      "Accuracy ---->  0.732913613319397\n",
      "Iter:643 train_rmse:3.854 test_loss:112.9 test_rmse:0.04933 test_acc:0.703\n",
      "Epoch  644\n",
      "Accuracy ---->  0.7329418063163757\n",
      "Iter:644 train_rmse:3.854 test_loss:112.9 test_rmse:0.04932 test_acc:0.703\n",
      "Epoch  645\n",
      "Accuracy ---->  0.732970267534256\n",
      "Iter:645 train_rmse:3.853 test_loss:112.8 test_rmse:0.04932 test_acc:0.7031\n",
      "Epoch  646\n",
      "Accuracy ---->  0.732998788356781\n",
      "Iter:646 train_rmse:3.853 test_loss:112.8 test_rmse:0.04931 test_acc:0.7031\n",
      "Epoch  647\n",
      "Accuracy ---->  0.7330267131328583\n",
      "Iter:647 train_rmse:3.853 test_loss:112.8 test_rmse:0.04931 test_acc:0.7031\n",
      "Epoch  648\n",
      "Accuracy ---->  0.7330555021762848\n",
      "Iter:648 train_rmse:3.852 test_loss:112.8 test_rmse:0.0493 test_acc:0.7032\n",
      "Epoch  649\n",
      "Accuracy ---->  0.7330842316150665\n",
      "Iter:649 train_rmse:3.852 test_loss:112.7 test_rmse:0.0493 test_acc:0.7032\n",
      "Epoch  650\n",
      "Accuracy ---->  0.7331129014492035\n",
      "Iter:650 train_rmse:3.851 test_loss:112.7 test_rmse:0.04929 test_acc:0.7032\n",
      "Epoch  651\n",
      "Accuracy ---->  0.7331415712833405\n",
      "Iter:651 train_rmse:3.851 test_loss:112.7 test_rmse:0.04929 test_acc:0.7033\n",
      "Epoch  652\n",
      "Accuracy ---->  0.7331705689430237\n",
      "Iter:652 train_rmse:3.851 test_loss:112.7 test_rmse:0.04928 test_acc:0.7033\n",
      "Epoch  653\n",
      "Accuracy ---->  0.7331995964050293\n",
      "Iter:653 train_rmse:3.85 test_loss:112.7 test_rmse:0.04928 test_acc:0.7033\n",
      "Epoch  654\n",
      "Accuracy ---->  0.7332291901111603\n",
      "Iter:654 train_rmse:3.85 test_loss:112.6 test_rmse:0.04927 test_acc:0.7033\n",
      "Epoch  655\n",
      "Accuracy ---->  0.7332584857940674\n",
      "Iter:655 train_rmse:3.849 test_loss:112.6 test_rmse:0.04927 test_acc:0.7034\n",
      "Epoch  656\n",
      "Accuracy ---->  0.7332881689071655\n",
      "Iter:656 train_rmse:3.849 test_loss:112.6 test_rmse:0.04926 test_acc:0.7034\n",
      "Epoch  657\n",
      "Accuracy ---->  0.7333170473575592\n",
      "Iter:657 train_rmse:3.848 test_loss:112.6 test_rmse:0.04926 test_acc:0.7034\n",
      "Epoch  658\n",
      "Accuracy ---->  0.7333472669124603\n",
      "Iter:658 train_rmse:3.848 test_loss:112.5 test_rmse:0.04925 test_acc:0.7035\n",
      "Epoch  659\n",
      "Accuracy ---->  0.7333773672580719\n",
      "Iter:659 train_rmse:3.848 test_loss:112.5 test_rmse:0.04925 test_acc:0.7035\n",
      "Epoch  660\n",
      "Accuracy ---->  0.7334074974060059\n",
      "Iter:660 train_rmse:3.847 test_loss:112.5 test_rmse:0.04925 test_acc:0.7035\n",
      "Epoch  661\n",
      "Accuracy ---->  0.7334386706352234\n",
      "Iter:661 train_rmse:3.847 test_loss:112.5 test_rmse:0.04924 test_acc:0.7035\n",
      "Epoch  662\n",
      "Accuracy ---->  0.7334689497947693\n",
      "Iter:662 train_rmse:3.846 test_loss:112.5 test_rmse:0.04924 test_acc:0.7036\n",
      "Epoch  663\n",
      "Accuracy ---->  0.7335002720355988\n",
      "Iter:663 train_rmse:3.846 test_loss:112.4 test_rmse:0.04923 test_acc:0.7036\n",
      "Epoch  664\n",
      "Accuracy ---->  0.733531266450882\n",
      "Iter:664 train_rmse:3.845 test_loss:112.4 test_rmse:0.04923 test_acc:0.7036\n",
      "Epoch  665\n",
      "Accuracy ---->  0.7335629761219025\n",
      "Iter:665 train_rmse:3.845 test_loss:112.4 test_rmse:0.04922 test_acc:0.7037\n",
      "Epoch  666\n",
      "Accuracy ---->  0.733595073223114\n",
      "Iter:666 train_rmse:3.844 test_loss:112.4 test_rmse:0.04922 test_acc:0.7037\n",
      "Epoch  667\n",
      "Accuracy ---->  0.7336270213127136\n",
      "Iter:667 train_rmse:3.844 test_loss:112.3 test_rmse:0.04921 test_acc:0.7037\n",
      "Epoch  668\n",
      "Accuracy ---->  0.7336594462394714\n",
      "Iter:668 train_rmse:3.843 test_loss:112.3 test_rmse:0.04921 test_acc:0.7037\n",
      "Epoch  669\n",
      "Accuracy ---->  0.7336920201778412\n",
      "Iter:669 train_rmse:3.843 test_loss:112.3 test_rmse:0.0492 test_acc:0.7038\n",
      "Epoch  670\n",
      "Accuracy ---->  0.7337254285812378\n",
      "Iter:670 train_rmse:3.843 test_loss:112.3 test_rmse:0.0492 test_acc:0.7038\n",
      "Epoch  671\n",
      "Accuracy ---->  0.7337586581707001\n",
      "Iter:671 train_rmse:3.842 test_loss:112.3 test_rmse:0.04919 test_acc:0.7038\n",
      "Epoch  672\n",
      "Accuracy ---->  0.73379185795784\n",
      "Iter:672 train_rmse:3.842 test_loss:112.2 test_rmse:0.04919 test_acc:0.7039\n",
      "Epoch  673\n",
      "Accuracy ---->  0.7338265776634216\n",
      "Iter:673 train_rmse:3.841 test_loss:112.2 test_rmse:0.04918 test_acc:0.7039\n",
      "Epoch  674\n",
      "Accuracy ---->  0.733861118555069\n",
      "Iter:674 train_rmse:3.841 test_loss:112.2 test_rmse:0.04918 test_acc:0.7039\n",
      "Epoch  675\n",
      "Accuracy ---->  0.7338961064815521\n",
      "Iter:675 train_rmse:3.84 test_loss:112.2 test_rmse:0.04918 test_acc:0.7039\n",
      "Epoch  676\n",
      "Accuracy ---->  0.7339317500591278\n",
      "Iter:676 train_rmse:3.84 test_loss:112.2 test_rmse:0.04917 test_acc:0.704\n",
      "Epoch  677\n",
      "Accuracy ---->  0.7339673340320587\n",
      "Iter:677 train_rmse:3.839 test_loss:112.1 test_rmse:0.04917 test_acc:0.704\n",
      "Epoch  678\n",
      "Accuracy ---->  0.7340035438537598\n",
      "Iter:678 train_rmse:3.839 test_loss:112.1 test_rmse:0.04916 test_acc:0.704\n",
      "Epoch  679\n",
      "Accuracy ---->  0.734040766954422\n",
      "Iter:679 train_rmse:3.838 test_loss:112.1 test_rmse:0.04916 test_acc:0.704\n",
      "Epoch  680\n",
      "Accuracy ---->  0.7340781092643738\n",
      "Iter:680 train_rmse:3.837 test_loss:112.1 test_rmse:0.04915 test_acc:0.7041\n",
      "Epoch  681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.7341161072254181\n",
      "Iter:681 train_rmse:3.837 test_loss:112.1 test_rmse:0.04915 test_acc:0.7041\n",
      "Epoch  682\n",
      "Accuracy ---->  0.7341555655002594\n",
      "Iter:682 train_rmse:3.836 test_loss:112.0 test_rmse:0.04915 test_acc:0.7041\n",
      "Epoch  683\n",
      "Accuracy ---->  0.7341944575309753\n",
      "Iter:683 train_rmse:3.836 test_loss:112.0 test_rmse:0.04914 test_acc:0.7041\n",
      "Epoch  684\n",
      "Accuracy ---->  0.7342348992824554\n",
      "Iter:684 train_rmse:3.835 test_loss:112.0 test_rmse:0.04914 test_acc:0.7042\n",
      "Epoch  685\n",
      "Accuracy ---->  0.7342756390571594\n",
      "Iter:685 train_rmse:3.835 test_loss:112.0 test_rmse:0.04913 test_acc:0.7042\n",
      "Epoch  686\n",
      "Accuracy ---->  0.7343172430992126\n",
      "Iter:686 train_rmse:3.834 test_loss:112.0 test_rmse:0.04913 test_acc:0.7042\n",
      "Epoch  687\n",
      "Accuracy ---->  0.7343602180480957\n",
      "Iter:687 train_rmse:3.833 test_loss:112.0 test_rmse:0.04913 test_acc:0.7042\n",
      "Epoch  688\n",
      "Accuracy ---->  0.734403133392334\n",
      "Iter:688 train_rmse:3.833 test_loss:111.9 test_rmse:0.04912 test_acc:0.7043\n",
      "Epoch  689\n",
      "Accuracy ---->  0.7344479858875275\n",
      "Iter:689 train_rmse:3.832 test_loss:111.9 test_rmse:0.04912 test_acc:0.7043\n",
      "Epoch  690\n",
      "Accuracy ---->  0.7344934344291687\n",
      "Iter:690 train_rmse:3.831 test_loss:111.9 test_rmse:0.04912 test_acc:0.7043\n",
      "Epoch  691\n",
      "Accuracy ---->  0.7345397472381592\n",
      "Iter:691 train_rmse:3.831 test_loss:111.9 test_rmse:0.04911 test_acc:0.7043\n",
      "Epoch  692\n",
      "Accuracy ---->  0.734587162733078\n",
      "Iter:692 train_rmse:3.83 test_loss:111.9 test_rmse:0.04911 test_acc:0.7043\n",
      "Epoch  693\n",
      "Accuracy ---->  0.7346368730068207\n",
      "Iter:693 train_rmse:3.829 test_loss:111.9 test_rmse:0.04911 test_acc:0.7044\n",
      "Epoch  694\n",
      "Accuracy ---->  0.7346872985363007\n",
      "Iter:694 train_rmse:3.829 test_loss:111.8 test_rmse:0.0491 test_acc:0.7044\n",
      "Epoch  695\n",
      "Accuracy ---->  0.7347387671470642\n",
      "Iter:695 train_rmse:3.828 test_loss:111.8 test_rmse:0.0491 test_acc:0.7044\n",
      "Epoch  696\n",
      "Accuracy ---->  0.7347923517227173\n",
      "Iter:696 train_rmse:3.827 test_loss:111.8 test_rmse:0.0491 test_acc:0.7044\n",
      "Epoch  697\n",
      "Accuracy ---->  0.734847217798233\n",
      "Iter:697 train_rmse:3.826 test_loss:111.8 test_rmse:0.04909 test_acc:0.7044\n",
      "Epoch  698\n",
      "Accuracy ---->  0.7349038124084473\n",
      "Iter:698 train_rmse:3.826 test_loss:111.8 test_rmse:0.04909 test_acc:0.7045\n",
      "Epoch  699\n",
      "Accuracy ---->  0.734962522983551\n",
      "Iter:699 train_rmse:3.825 test_loss:111.8 test_rmse:0.04909 test_acc:0.7045\n",
      "Epoch  700\n",
      "Accuracy ---->  0.7350232303142548\n",
      "Iter:700 train_rmse:3.824 test_loss:111.8 test_rmse:0.04909 test_acc:0.7045\n",
      "Epoch  701\n",
      "Accuracy ---->  0.735086053609848\n",
      "Iter:701 train_rmse:3.823 test_loss:111.8 test_rmse:0.04908 test_acc:0.7045\n",
      "Epoch  702\n",
      "Accuracy ---->  0.7351505756378174\n",
      "Iter:702 train_rmse:3.822 test_loss:111.7 test_rmse:0.04908 test_acc:0.7045\n",
      "Epoch  703\n",
      "Accuracy ---->  0.7352184951305389\n",
      "Iter:703 train_rmse:3.821 test_loss:111.7 test_rmse:0.04908 test_acc:0.7045\n",
      "Epoch  704\n",
      "Accuracy ---->  0.7352882921695709\n",
      "Iter:704 train_rmse:3.82 test_loss:111.7 test_rmse:0.04908 test_acc:0.7045\n",
      "Epoch  705\n",
      "Accuracy ---->  0.735361248254776\n",
      "Iter:705 train_rmse:3.819 test_loss:111.7 test_rmse:0.04908 test_acc:0.7045\n",
      "Epoch  706\n",
      "Accuracy ---->  0.7354373037815094\n",
      "Iter:706 train_rmse:3.818 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  707\n",
      "Accuracy ---->  0.7355157136917114\n",
      "Iter:707 train_rmse:3.817 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  708\n",
      "Accuracy ---->  0.7355981767177582\n",
      "Iter:708 train_rmse:3.815 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  709\n",
      "Accuracy ---->  0.7356846928596497\n",
      "Iter:709 train_rmse:3.814 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  710\n",
      "Accuracy ---->  0.7357739210128784\n",
      "Iter:710 train_rmse:3.813 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  711\n",
      "Accuracy ---->  0.7358680963516235\n",
      "Iter:711 train_rmse:3.812 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  712\n",
      "Accuracy ---->  0.7359663248062134\n",
      "Iter:712 train_rmse:3.81 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  713\n",
      "Accuracy ---->  0.7360692024230957\n",
      "Iter:713 train_rmse:3.809 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  714\n",
      "Accuracy ---->  0.736176073551178\n",
      "Iter:714 train_rmse:3.807 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  715\n",
      "Accuracy ---->  0.7362890243530273\n",
      "Iter:715 train_rmse:3.806 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  716\n",
      "Accuracy ---->  0.7364060282707214\n",
      "Iter:716 train_rmse:3.804 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  717\n",
      "Accuracy ---->  0.7365288138389587\n",
      "Iter:717 train_rmse:3.802 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  718\n",
      "Accuracy ---->  0.7366562485694885\n",
      "Iter:718 train_rmse:3.8 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  719\n",
      "Accuracy ---->  0.7367882430553436\n",
      "Iter:719 train_rmse:3.798 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  720\n",
      "Accuracy ---->  0.7369250357151031\n",
      "Iter:720 train_rmse:3.796 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  721\n",
      "Accuracy ---->  0.7370649874210358\n",
      "Iter:721 train_rmse:3.794 test_loss:111.7 test_rmse:0.04907 test_acc:0.7045\n",
      "Epoch  722\n",
      "Accuracy ---->  0.7372077703475952\n",
      "Iter:722 train_rmse:3.792 test_loss:111.7 test_rmse:0.04907 test_acc:0.7045\n",
      "Epoch  723\n",
      "Accuracy ---->  0.737351268529892\n",
      "Iter:723 train_rmse:3.79 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  724\n",
      "Accuracy ---->  0.737493097782135\n",
      "Iter:724 train_rmse:3.788 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  725\n",
      "Accuracy ---->  0.7376303672790527\n",
      "Iter:725 train_rmse:3.786 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  726\n",
      "Accuracy ---->  0.7377594113349915\n",
      "Iter:726 train_rmse:3.784 test_loss:111.7 test_rmse:0.04906 test_acc:0.7046\n",
      "Epoch  727\n",
      "Accuracy ---->  0.7378763258457184\n",
      "Iter:727 train_rmse:3.783 test_loss:111.6 test_rmse:0.04906 test_acc:0.7046\n",
      "Epoch  728\n",
      "Accuracy ---->  0.7379766404628754\n",
      "Iter:728 train_rmse:3.781 test_loss:111.6 test_rmse:0.04905 test_acc:0.7047\n",
      "Epoch  729\n",
      "Accuracy ---->  0.7380559146404266\n",
      "Iter:729 train_rmse:3.78 test_loss:111.6 test_rmse:0.04904 test_acc:0.7047\n",
      "Epoch  730\n",
      "Accuracy ---->  0.7381115257740021\n",
      "Iter:730 train_rmse:3.779 test_loss:111.5 test_rmse:0.04903 test_acc:0.7048\n",
      "Epoch  731\n",
      "Accuracy ---->  0.7381422817707062\n",
      "Iter:731 train_rmse:3.779 test_loss:111.5 test_rmse:0.04902 test_acc:0.7048\n",
      "Epoch  732\n",
      "Accuracy ---->  0.7381504476070404\n",
      "Iter:732 train_rmse:3.779 test_loss:111.5 test_rmse:0.04902 test_acc:0.7049\n",
      "Epoch  733\n",
      "Accuracy ---->  0.738141804933548\n",
      "Iter:733 train_rmse:3.779 test_loss:111.5 test_rmse:0.04902 test_acc:0.7049\n",
      "Epoch  734\n",
      "Accuracy ---->  0.738126128911972\n",
      "Iter:734 train_rmse:3.779 test_loss:111.5 test_rmse:0.04902 test_acc:0.7049\n",
      "Epoch  735\n",
      "Accuracy ---->  0.7381134331226349\n",
      "Iter:735 train_rmse:3.779 test_loss:111.5 test_rmse:0.04902 test_acc:0.7049\n",
      "Epoch  736\n",
      "Accuracy ---->  0.738113284111023\n",
      "Iter:736 train_rmse:3.779 test_loss:111.5 test_rmse:0.04903 test_acc:0.7048\n",
      "Epoch  737\n",
      "Accuracy ---->  0.7381339371204376\n",
      "Iter:737 train_rmse:3.779 test_loss:111.6 test_rmse:0.04905 test_acc:0.7047\n",
      "Epoch  738\n",
      "Accuracy ---->  0.7381795644760132\n",
      "Iter:738 train_rmse:3.778 test_loss:111.7 test_rmse:0.04907 test_acc:0.7046\n",
      "Epoch  739\n",
      "Accuracy ---->  0.7382533848285675\n",
      "Iter:739 train_rmse:3.777 test_loss:111.8 test_rmse:0.04909 test_acc:0.7044\n",
      "Epoch  740\n",
      "Accuracy ---->  0.7383566498756409\n",
      "Iter:740 train_rmse:3.776 test_loss:111.9 test_rmse:0.04912 test_acc:0.7043\n",
      "Epoch  741\n",
      "Accuracy ---->  0.7384896278381348\n",
      "Iter:741 train_rmse:3.774 test_loss:112.0 test_rmse:0.04915 test_acc:0.7041\n",
      "Epoch  742\n",
      "Accuracy ---->  0.7386508584022522\n",
      "Iter:742 train_rmse:3.771 test_loss:112.2 test_rmse:0.04918 test_acc:0.7039\n",
      "Epoch  743\n",
      "Accuracy ---->  0.7388364970684052\n",
      "Iter:743 train_rmse:3.769 test_loss:112.3 test_rmse:0.04921 test_acc:0.7037\n",
      "Epoch  744\n",
      "Accuracy ---->  0.7390369474887848\n",
      "Iter:744 train_rmse:3.766 test_loss:112.5 test_rmse:0.04924 test_acc:0.7035\n",
      "Epoch  745\n",
      "Accuracy ---->  0.739234983921051\n",
      "Iter:745 train_rmse:3.763 test_loss:112.6 test_rmse:0.04926 test_acc:0.7034\n",
      "Epoch  746\n",
      "Accuracy ---->  0.7394037842750549\n",
      "Iter:746 train_rmse:3.761 test_loss:112.6 test_rmse:0.04927 test_acc:0.7034\n",
      "Epoch  747\n",
      "Accuracy ---->  0.7395133674144745\n",
      "Iter:747 train_rmse:3.759 test_loss:112.5 test_rmse:0.04925 test_acc:0.7035\n",
      "Epoch  748\n",
      "Accuracy ---->  0.7395520210266113\n",
      "Iter:748 train_rmse:3.758 test_loss:112.3 test_rmse:0.0492 test_acc:0.7038\n",
      "Epoch  749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.7395500540733337\n",
      "Iter:749 train_rmse:3.758 test_loss:112.0 test_rmse:0.04913 test_acc:0.7042\n",
      "Epoch  750\n",
      "Accuracy ---->  0.7395627796649933\n",
      "Iter:750 train_rmse:3.758 test_loss:111.6 test_rmse:0.04905 test_acc:0.7047\n",
      "Epoch  751\n",
      "Accuracy ---->  0.7396080493927002\n",
      "Iter:751 train_rmse:3.758 test_loss:111.4 test_rmse:0.049 test_acc:0.705\n",
      "Epoch  752\n",
      "Accuracy ---->  0.7396596670150757\n",
      "Iter:752 train_rmse:3.757 test_loss:111.2 test_rmse:0.04896 test_acc:0.7052\n",
      "Epoch  753\n",
      "Accuracy ---->  0.739700585603714\n",
      "Iter:753 train_rmse:3.756 test_loss:111.1 test_rmse:0.04895 test_acc:0.7053\n",
      "Epoch  754\n",
      "Accuracy ---->  0.7397324442863464\n",
      "Iter:754 train_rmse:3.756 test_loss:111.1 test_rmse:0.04894 test_acc:0.7054\n",
      "Epoch  755\n",
      "Accuracy ---->  0.7397604584693909\n",
      "Iter:755 train_rmse:3.755 test_loss:111.1 test_rmse:0.04893 test_acc:0.7054\n",
      "Epoch  756\n",
      "Accuracy ---->  0.7397876083850861\n",
      "Iter:756 train_rmse:3.755 test_loss:111.1 test_rmse:0.04893 test_acc:0.7054\n",
      "Epoch  757\n",
      "Accuracy ---->  0.7398146390914917\n",
      "Iter:757 train_rmse:3.755 test_loss:111.0 test_rmse:0.04893 test_acc:0.7054\n",
      "Epoch  758\n",
      "Accuracy ---->  0.7398417890071869\n",
      "Iter:758 train_rmse:3.754 test_loss:111.0 test_rmse:0.04892 test_acc:0.7055\n",
      "Epoch  759\n",
      "Accuracy ---->  0.7398690283298492\n",
      "Iter:759 train_rmse:3.754 test_loss:111.0 test_rmse:0.04892 test_acc:0.7055\n",
      "Epoch  760\n",
      "Accuracy ---->  0.7398962676525116\n",
      "Iter:760 train_rmse:3.753 test_loss:111.0 test_rmse:0.04892 test_acc:0.7055\n",
      "Epoch  761\n",
      "Accuracy ---->  0.7399232685565948\n",
      "Iter:761 train_rmse:3.753 test_loss:111.0 test_rmse:0.04891 test_acc:0.7055\n",
      "Epoch  762\n",
      "Accuracy ---->  0.739950031042099\n",
      "Iter:762 train_rmse:3.753 test_loss:111.0 test_rmse:0.04891 test_acc:0.7055\n",
      "Epoch  763\n",
      "Accuracy ---->  0.7399766147136688\n",
      "Iter:763 train_rmse:3.752 test_loss:111.0 test_rmse:0.04891 test_acc:0.7055\n",
      "Epoch  764\n",
      "Accuracy ---->  0.7400029003620148\n",
      "Iter:764 train_rmse:3.752 test_loss:111.0 test_rmse:0.04891 test_acc:0.7056\n",
      "Epoch  765\n",
      "Accuracy ---->  0.7400290071964264\n",
      "Iter:765 train_rmse:3.752 test_loss:110.9 test_rmse:0.0489 test_acc:0.7056\n",
      "Epoch  766\n",
      "Accuracy ---->  0.7400549352169037\n",
      "Iter:766 train_rmse:3.751 test_loss:110.9 test_rmse:0.0489 test_acc:0.7056\n",
      "Epoch  767\n",
      "Accuracy ---->  0.7400805652141571\n",
      "Iter:767 train_rmse:3.751 test_loss:110.9 test_rmse:0.0489 test_acc:0.7056\n",
      "Epoch  768\n",
      "Accuracy ---->  0.7401060461997986\n",
      "Iter:768 train_rmse:3.75 test_loss:110.9 test_rmse:0.04889 test_acc:0.7056\n",
      "Epoch  769\n",
      "Accuracy ---->  0.7401313781738281\n",
      "Iter:769 train_rmse:3.75 test_loss:110.9 test_rmse:0.04889 test_acc:0.7056\n",
      "Epoch  770\n",
      "Accuracy ---->  0.740156501531601\n",
      "Iter:770 train_rmse:3.75 test_loss:110.9 test_rmse:0.04889 test_acc:0.7057\n",
      "Epoch  771\n",
      "Accuracy ---->  0.7401815056800842\n",
      "Iter:771 train_rmse:3.749 test_loss:110.9 test_rmse:0.04889 test_acc:0.7057\n",
      "Epoch  772\n",
      "Accuracy ---->  0.7402063608169556\n",
      "Iter:772 train_rmse:3.749 test_loss:110.9 test_rmse:0.04888 test_acc:0.7057\n",
      "Epoch  773\n",
      "Accuracy ---->  0.7402310371398926\n",
      "Iter:773 train_rmse:3.749 test_loss:110.8 test_rmse:0.04888 test_acc:0.7057\n",
      "Epoch  774\n",
      "Accuracy ---->  0.7402556836605072\n",
      "Iter:774 train_rmse:3.748 test_loss:110.8 test_rmse:0.04888 test_acc:0.7057\n",
      "Epoch  775\n",
      "Accuracy ---->  0.7402801513671875\n",
      "Iter:775 train_rmse:3.748 test_loss:110.8 test_rmse:0.04888 test_acc:0.7057\n",
      "Epoch  776\n",
      "Accuracy ---->  0.7403044700622559\n",
      "Iter:776 train_rmse:3.748 test_loss:110.8 test_rmse:0.04887 test_acc:0.7058\n",
      "Epoch  777\n",
      "Accuracy ---->  0.7403287589550018\n",
      "Iter:777 train_rmse:3.747 test_loss:110.8 test_rmse:0.04887 test_acc:0.7058\n",
      "Epoch  778\n",
      "Accuracy ---->  0.7403528094291687\n",
      "Iter:778 train_rmse:3.747 test_loss:110.8 test_rmse:0.04887 test_acc:0.7058\n",
      "Epoch  779\n",
      "Accuracy ---->  0.740376889705658\n",
      "Iter:779 train_rmse:3.747 test_loss:110.8 test_rmse:0.04887 test_acc:0.7058\n",
      "Epoch  780\n",
      "Accuracy ---->  0.7404008209705353\n",
      "Iter:780 train_rmse:3.746 test_loss:110.8 test_rmse:0.04886 test_acc:0.7058\n",
      "Epoch  781\n",
      "Accuracy ---->  0.740424633026123\n",
      "Iter:781 train_rmse:3.746 test_loss:110.7 test_rmse:0.04886 test_acc:0.7058\n",
      "Epoch  782\n",
      "Accuracy ---->  0.7404483556747437\n",
      "Iter:782 train_rmse:3.746 test_loss:110.7 test_rmse:0.04886 test_acc:0.7058\n",
      "Epoch  783\n",
      "Accuracy ---->  0.7404719293117523\n",
      "Iter:783 train_rmse:3.745 test_loss:110.7 test_rmse:0.04886 test_acc:0.7059\n",
      "Epoch  784\n",
      "Accuracy ---->  0.7404955923557281\n",
      "Iter:784 train_rmse:3.745 test_loss:110.7 test_rmse:0.04885 test_acc:0.7059\n",
      "Epoch  785\n",
      "Accuracy ---->  0.7405190765857697\n",
      "Iter:785 train_rmse:3.744 test_loss:110.7 test_rmse:0.04885 test_acc:0.7059\n",
      "Epoch  786\n",
      "Accuracy ---->  0.7405425012111664\n",
      "Iter:786 train_rmse:3.744 test_loss:110.7 test_rmse:0.04885 test_acc:0.7059\n",
      "Epoch  787\n",
      "Accuracy ---->  0.7405657768249512\n",
      "Iter:787 train_rmse:3.744 test_loss:110.7 test_rmse:0.04885 test_acc:0.7059\n",
      "Epoch  788\n",
      "Accuracy ---->  0.7405890226364136\n",
      "Iter:788 train_rmse:3.743 test_loss:110.7 test_rmse:0.04884 test_acc:0.7059\n",
      "Epoch  789\n",
      "Accuracy ---->  0.7406121492385864\n",
      "Iter:789 train_rmse:3.743 test_loss:110.7 test_rmse:0.04884 test_acc:0.7059\n",
      "Epoch  790\n",
      "Accuracy ---->  0.7406352162361145\n",
      "Iter:790 train_rmse:3.743 test_loss:110.7 test_rmse:0.04884 test_acc:0.706\n",
      "Epoch  791\n",
      "Accuracy ---->  0.7406582832336426\n",
      "Iter:791 train_rmse:3.742 test_loss:110.6 test_rmse:0.04884 test_acc:0.706\n",
      "Epoch  792\n",
      "Accuracy ---->  0.7406811714172363\n",
      "Iter:792 train_rmse:3.742 test_loss:110.6 test_rmse:0.04884 test_acc:0.706\n",
      "Epoch  793\n",
      "Accuracy ---->  0.7407040297985077\n",
      "Iter:793 train_rmse:3.742 test_loss:110.6 test_rmse:0.04883 test_acc:0.706\n",
      "Epoch  794\n",
      "Accuracy ---->  0.7407267391681671\n",
      "Iter:794 train_rmse:3.741 test_loss:110.6 test_rmse:0.04883 test_acc:0.706\n",
      "Epoch  795\n",
      "Accuracy ---->  0.7407493889331818\n",
      "Iter:795 train_rmse:3.741 test_loss:110.6 test_rmse:0.04883 test_acc:0.706\n",
      "Epoch  796\n",
      "Accuracy ---->  0.7407720685005188\n",
      "Iter:796 train_rmse:3.741 test_loss:110.6 test_rmse:0.04883 test_acc:0.706\n",
      "Epoch  797\n",
      "Accuracy ---->  0.7407946288585663\n",
      "Iter:797 train_rmse:3.741 test_loss:110.6 test_rmse:0.04883 test_acc:0.706\n",
      "Epoch  798\n",
      "Accuracy ---->  0.7408170998096466\n",
      "Iter:798 train_rmse:3.74 test_loss:110.6 test_rmse:0.04883 test_acc:0.706\n",
      "Epoch  799\n",
      "Accuracy ---->  0.7408394515514374\n",
      "Iter:799 train_rmse:3.74 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  800\n",
      "Accuracy ---->  0.7408617436885834\n",
      "Iter:800 train_rmse:3.74 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  801\n",
      "Accuracy ---->  0.7408839762210846\n",
      "Iter:801 train_rmse:3.739 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  802\n",
      "Accuracy ---->  0.7409062087535858\n",
      "Iter:802 train_rmse:3.739 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  803\n",
      "Accuracy ---->  0.7409282326698303\n",
      "Iter:803 train_rmse:3.739 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  804\n",
      "Accuracy ---->  0.7409502863883972\n",
      "Iter:804 train_rmse:3.738 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  805\n",
      "Accuracy ---->  0.7409721612930298\n",
      "Iter:805 train_rmse:3.738 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  806\n",
      "Accuracy ---->  0.74099400639534\n",
      "Iter:806 train_rmse:3.738 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  807\n",
      "Accuracy ---->  0.7410156726837158\n",
      "Iter:807 train_rmse:3.737 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  808\n",
      "Accuracy ---->  0.7410373091697693\n",
      "Iter:808 train_rmse:3.737 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  809\n",
      "Accuracy ---->  0.7410586774349213\n",
      "Iter:809 train_rmse:3.737 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  810\n",
      "Accuracy ---->  0.7410799562931061\n",
      "Iter:810 train_rmse:3.736 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  811\n",
      "Accuracy ---->  0.741100937128067\n",
      "Iter:811 train_rmse:3.736 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  812\n",
      "Accuracy ---->  0.7411215305328369\n",
      "Iter:812 train_rmse:3.736 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  813\n",
      "Accuracy ---->  0.7411417365074158\n",
      "Iter:813 train_rmse:3.736 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  814\n",
      "Accuracy ---->  0.7411611378192902\n",
      "Iter:814 train_rmse:3.735 test_loss:110.6 test_rmse:0.04883 test_acc:0.706\n",
      "Epoch  815\n",
      "Accuracy ---->  0.7411794364452362\n",
      "Iter:815 train_rmse:3.735 test_loss:110.6 test_rmse:0.04883 test_acc:0.706\n",
      "Epoch  816\n",
      "Accuracy ---->  0.7411960661411285\n",
      "Iter:816 train_rmse:3.735 test_loss:110.7 test_rmse:0.04884 test_acc:0.7059\n",
      "Epoch  817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.7412100434303284\n",
      "Iter:817 train_rmse:3.735 test_loss:110.7 test_rmse:0.04886 test_acc:0.7059\n",
      "Epoch  818\n",
      "Accuracy ---->  0.741220235824585\n",
      "Iter:818 train_rmse:3.734 test_loss:110.8 test_rmse:0.04888 test_acc:0.7057\n",
      "Epoch  819\n",
      "Accuracy ---->  0.7412268221378326\n",
      "Iter:819 train_rmse:3.734 test_loss:111.0 test_rmse:0.04892 test_acc:0.7055\n",
      "Epoch  820\n",
      "Accuracy ---->  0.7412347495555878\n",
      "Iter:820 train_rmse:3.734 test_loss:111.3 test_rmse:0.04899 test_acc:0.705\n",
      "Epoch  821\n",
      "Accuracy ---->  0.7412481904029846\n",
      "Iter:821 train_rmse:3.734 test_loss:111.9 test_rmse:0.04912 test_acc:0.7043\n",
      "Epoch  822\n",
      "Accuracy ---->  0.7411273717880249\n",
      "Iter:822 train_rmse:3.736 test_loss:112.7 test_rmse:0.04929 test_acc:0.7033\n",
      "Epoch  823\n",
      "Accuracy ---->  0.7398398220539093\n",
      "Iter:823 train_rmse:3.754 test_loss:112.6 test_rmse:0.04928 test_acc:0.7033\n",
      "Epoch  824\n",
      "Accuracy ---->  0.736881673336029\n",
      "Iter:824 train_rmse:3.797 test_loss:110.1 test_rmse:0.04872 test_acc:0.7067\n",
      "Epoch  825\n",
      "Accuracy ---->  0.7402477860450745\n",
      "Iter:825 train_rmse:3.748 test_loss:109.6 test_rmse:0.0486 test_acc:0.7074\n",
      "Epoch  826\n",
      "Accuracy ---->  0.7411274611949921\n",
      "Iter:826 train_rmse:3.736 test_loss:110.0 test_rmse:0.04869 test_acc:0.7069\n",
      "Epoch  827\n",
      "Accuracy ---->  0.7411929070949554\n",
      "Iter:827 train_rmse:3.735 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  828\n",
      "Accuracy ---->  0.7411871552467346\n",
      "Iter:828 train_rmse:3.735 test_loss:110.9 test_rmse:0.04889 test_acc:0.7057\n",
      "Epoch  829\n",
      "Accuracy ---->  0.7411081790924072\n",
      "Iter:829 train_rmse:3.736 test_loss:111.0 test_rmse:0.04892 test_acc:0.7055\n",
      "Epoch  830\n",
      "Accuracy ---->  0.7409782409667969\n",
      "Iter:830 train_rmse:3.738 test_loss:111.0 test_rmse:0.04892 test_acc:0.7055\n",
      "Epoch  831\n",
      "Accuracy ---->  0.7408253252506256\n",
      "Iter:831 train_rmse:3.74 test_loss:111.0 test_rmse:0.04891 test_acc:0.7055\n",
      "Epoch  832\n",
      "Accuracy ---->  0.7406748533248901\n",
      "Iter:832 train_rmse:3.742 test_loss:110.9 test_rmse:0.04889 test_acc:0.7057\n",
      "Epoch  833\n",
      "Accuracy ---->  0.7405438125133514\n",
      "Iter:833 train_rmse:3.744 test_loss:110.8 test_rmse:0.04887 test_acc:0.7058\n",
      "Epoch  834\n",
      "Accuracy ---->  0.7404406368732452\n",
      "Iter:834 train_rmse:3.746 test_loss:110.7 test_rmse:0.04885 test_acc:0.7059\n",
      "Epoch  835\n",
      "Accuracy ---->  0.740366667509079\n",
      "Iter:835 train_rmse:3.747 test_loss:110.6 test_rmse:0.04883 test_acc:0.706\n",
      "Epoch  836\n",
      "Accuracy ---->  0.74031862616539\n",
      "Iter:836 train_rmse:3.747 test_loss:110.6 test_rmse:0.04882 test_acc:0.7061\n",
      "Epoch  837\n",
      "Accuracy ---->  0.7402917444705963\n",
      "Iter:837 train_rmse:3.748 test_loss:110.5 test_rmse:0.04881 test_acc:0.7062\n",
      "Epoch  838\n",
      "Accuracy ---->  0.7402811944484711\n",
      "Iter:838 train_rmse:3.748 test_loss:110.5 test_rmse:0.0488 test_acc:0.7062\n",
      "Epoch  839\n",
      "Accuracy ---->  0.7402826547622681\n",
      "Iter:839 train_rmse:3.748 test_loss:110.4 test_rmse:0.04879 test_acc:0.7063\n",
      "Epoch  840\n",
      "Accuracy ---->  0.7402918636798859\n",
      "Iter:840 train_rmse:3.748 test_loss:110.4 test_rmse:0.04879 test_acc:0.7063\n",
      "Epoch  841\n",
      "Accuracy ---->  0.7403068542480469\n",
      "Iter:841 train_rmse:3.748 test_loss:110.4 test_rmse:0.04878 test_acc:0.7063\n",
      "Epoch  842\n",
      "Accuracy ---->  0.740325540304184\n",
      "Iter:842 train_rmse:3.747 test_loss:110.4 test_rmse:0.04878 test_acc:0.7063\n",
      "Epoch  843\n",
      "Accuracy ---->  0.7403458058834076\n",
      "Iter:843 train_rmse:3.747 test_loss:110.4 test_rmse:0.04878 test_acc:0.7063\n",
      "Epoch  844\n",
      "Accuracy ---->  0.7403672933578491\n",
      "Iter:844 train_rmse:3.747 test_loss:110.4 test_rmse:0.04878 test_acc:0.7063\n",
      "Epoch  845\n",
      "Accuracy ---->  0.7403890788555145\n",
      "Iter:845 train_rmse:3.746 test_loss:110.4 test_rmse:0.04877 test_acc:0.7064\n",
      "Epoch  846\n",
      "Accuracy ---->  0.7404105067253113\n",
      "Iter:846 train_rmse:3.746 test_loss:110.3 test_rmse:0.04877 test_acc:0.7064\n",
      "Epoch  847\n",
      "Accuracy ---->  0.7404311895370483\n",
      "Iter:847 train_rmse:3.746 test_loss:110.3 test_rmse:0.04877 test_acc:0.7064\n",
      "Epoch  848\n",
      "Accuracy ---->  0.7404509484767914\n",
      "Iter:848 train_rmse:3.745 test_loss:110.3 test_rmse:0.04877 test_acc:0.7064\n",
      "Epoch  849\n",
      "Accuracy ---->  0.7404701113700867\n",
      "Iter:849 train_rmse:3.745 test_loss:110.3 test_rmse:0.04877 test_acc:0.7064\n",
      "Epoch  850\n",
      "Accuracy ---->  0.7404882907867432\n",
      "Iter:850 train_rmse:3.745 test_loss:110.3 test_rmse:0.04876 test_acc:0.7064\n",
      "Epoch  851\n",
      "Accuracy ---->  0.7405054569244385\n",
      "Iter:851 train_rmse:3.745 test_loss:110.3 test_rmse:0.04876 test_acc:0.7064\n",
      "Epoch  852\n",
      "Accuracy ---->  0.7405218482017517\n",
      "Iter:852 train_rmse:3.744 test_loss:110.3 test_rmse:0.04876 test_acc:0.7064\n",
      "Epoch  853\n",
      "Accuracy ---->  0.7405374348163605\n",
      "Iter:853 train_rmse:3.744 test_loss:110.3 test_rmse:0.04876 test_acc:0.7064\n",
      "Epoch  854\n",
      "Accuracy ---->  0.7405523061752319\n",
      "Iter:854 train_rmse:3.744 test_loss:110.3 test_rmse:0.04876 test_acc:0.7065\n",
      "Epoch  855\n",
      "Accuracy ---->  0.7405667901039124\n",
      "Iter:855 train_rmse:3.744 test_loss:110.3 test_rmse:0.04875 test_acc:0.7065\n",
      "Epoch  856\n",
      "Accuracy ---->  0.7405807375907898\n",
      "Iter:856 train_rmse:3.744 test_loss:110.3 test_rmse:0.04875 test_acc:0.7065\n",
      "Epoch  857\n",
      "Accuracy ---->  0.7405943870544434\n",
      "Iter:857 train_rmse:3.743 test_loss:110.2 test_rmse:0.04875 test_acc:0.7065\n",
      "Epoch  858\n",
      "Accuracy ---->  0.7406077086925507\n",
      "Iter:858 train_rmse:3.743 test_loss:110.2 test_rmse:0.04875 test_acc:0.7065\n",
      "Epoch  859\n",
      "Accuracy ---->  0.7406208515167236\n",
      "Iter:859 train_rmse:3.743 test_loss:110.2 test_rmse:0.04874 test_acc:0.7065\n",
      "Epoch  860\n",
      "Accuracy ---->  0.7406337559223175\n",
      "Iter:860 train_rmse:3.743 test_loss:110.2 test_rmse:0.04874 test_acc:0.7066\n",
      "Epoch  861\n",
      "Accuracy ---->  0.7406467497348785\n",
      "Iter:861 train_rmse:3.743 test_loss:110.2 test_rmse:0.04874 test_acc:0.7066\n",
      "Epoch  862\n",
      "Accuracy ---->  0.7406594753265381\n",
      "Iter:862 train_rmse:3.742 test_loss:110.2 test_rmse:0.04873 test_acc:0.7066\n",
      "Epoch  863\n",
      "Accuracy ---->  0.7406726181507111\n",
      "Iter:863 train_rmse:3.742 test_loss:110.2 test_rmse:0.04873 test_acc:0.7066\n",
      "Epoch  864\n",
      "Accuracy ---->  0.7406854331493378\n",
      "Iter:864 train_rmse:3.742 test_loss:110.2 test_rmse:0.04873 test_acc:0.7066\n",
      "Epoch  865\n",
      "Accuracy ---->  0.7406986653804779\n",
      "Iter:865 train_rmse:3.742 test_loss:110.1 test_rmse:0.04873 test_acc:0.7066\n",
      "Epoch  866\n",
      "Accuracy ---->  0.7407119870185852\n",
      "Iter:866 train_rmse:3.742 test_loss:110.1 test_rmse:0.04872 test_acc:0.7067\n",
      "Epoch  867\n",
      "Accuracy ---->  0.7407252788543701\n",
      "Iter:867 train_rmse:3.742 test_loss:110.1 test_rmse:0.04872 test_acc:0.7067\n",
      "Epoch  868\n",
      "Accuracy ---->  0.7407388091087341\n",
      "Iter:868 train_rmse:3.741 test_loss:110.1 test_rmse:0.04872 test_acc:0.7067\n",
      "Epoch  869\n",
      "Accuracy ---->  0.7407525479793549\n",
      "Iter:869 train_rmse:3.741 test_loss:110.1 test_rmse:0.04872 test_acc:0.7067\n",
      "Epoch  870\n",
      "Accuracy ---->  0.7407663464546204\n",
      "Iter:870 train_rmse:3.741 test_loss:110.1 test_rmse:0.04871 test_acc:0.7067\n",
      "Epoch  871\n",
      "Accuracy ---->  0.7407801151275635\n",
      "Iter:871 train_rmse:3.741 test_loss:110.1 test_rmse:0.04871 test_acc:0.7067\n",
      "Epoch  872\n",
      "Accuracy ---->  0.74079430103302\n",
      "Iter:872 train_rmse:3.741 test_loss:110.1 test_rmse:0.04871 test_acc:0.7068\n",
      "Epoch  873\n",
      "Accuracy ---->  0.7408081591129303\n",
      "Iter:873 train_rmse:3.74 test_loss:110.0 test_rmse:0.04871 test_acc:0.7068\n",
      "Epoch  874\n",
      "Accuracy ---->  0.7408226132392883\n",
      "Iter:874 train_rmse:3.74 test_loss:110.0 test_rmse:0.0487 test_acc:0.7068\n",
      "Epoch  875\n",
      "Accuracy ---->  0.7408370077610016\n",
      "Iter:875 train_rmse:3.74 test_loss:110.0 test_rmse:0.0487 test_acc:0.7068\n",
      "Epoch  876\n",
      "Accuracy ---->  0.7408516108989716\n",
      "Iter:876 train_rmse:3.74 test_loss:110.0 test_rmse:0.0487 test_acc:0.7068\n",
      "Epoch  877\n",
      "Accuracy ---->  0.7408660650253296\n",
      "Iter:877 train_rmse:3.739 test_loss:110.0 test_rmse:0.0487 test_acc:0.7068\n",
      "Epoch  878\n",
      "Accuracy ---->  0.7408807277679443\n",
      "Iter:878 train_rmse:3.739 test_loss:110.0 test_rmse:0.04869 test_acc:0.7068\n",
      "Epoch  879\n",
      "Accuracy ---->  0.740895539522171\n",
      "Iter:879 train_rmse:3.739 test_loss:110.0 test_rmse:0.04869 test_acc:0.7069\n",
      "Epoch  880\n",
      "Accuracy ---->  0.740910530090332\n",
      "Iter:880 train_rmse:3.739 test_loss:110.0 test_rmse:0.04869 test_acc:0.7069\n",
      "Epoch  881\n",
      "Accuracy ---->  0.7409254908561707\n",
      "Iter:881 train_rmse:3.739 test_loss:110.0 test_rmse:0.04869 test_acc:0.7069\n",
      "Epoch  882\n",
      "Accuracy ---->  0.7409405410289764\n",
      "Iter:882 train_rmse:3.738 test_loss:109.9 test_rmse:0.04868 test_acc:0.7069\n",
      "Epoch  883\n",
      "Accuracy ---->  0.740955650806427\n",
      "Iter:883 train_rmse:3.738 test_loss:109.9 test_rmse:0.04868 test_acc:0.7069\n",
      "Epoch  884\n",
      "Accuracy ---->  0.7409710586071014\n",
      "Iter:884 train_rmse:3.738 test_loss:109.9 test_rmse:0.04868 test_acc:0.7069\n",
      "Epoch  885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.7409862577915192\n",
      "Iter:885 train_rmse:3.738 test_loss:109.9 test_rmse:0.04868 test_acc:0.7069\n",
      "Epoch  886\n",
      "Accuracy ---->  0.7410017549991608\n",
      "Iter:886 train_rmse:3.738 test_loss:109.9 test_rmse:0.04867 test_acc:0.707\n",
      "Epoch  887\n",
      "Accuracy ---->  0.7410171031951904\n",
      "Iter:887 train_rmse:3.737 test_loss:109.9 test_rmse:0.04867 test_acc:0.707\n",
      "Epoch  888\n",
      "Accuracy ---->  0.7410323917865753\n",
      "Iter:888 train_rmse:3.737 test_loss:109.9 test_rmse:0.04867 test_acc:0.707\n",
      "Epoch  889\n",
      "Accuracy ---->  0.7410477101802826\n",
      "Iter:889 train_rmse:3.737 test_loss:109.9 test_rmse:0.04867 test_acc:0.707\n",
      "Epoch  890\n",
      "Accuracy ---->  0.74106365442276\n",
      "Iter:890 train_rmse:3.737 test_loss:109.9 test_rmse:0.04866 test_acc:0.707\n",
      "Epoch  891\n",
      "Accuracy ---->  0.7410790622234344\n",
      "Iter:891 train_rmse:3.736 test_loss:109.8 test_rmse:0.04866 test_acc:0.707\n",
      "Epoch  892\n",
      "Accuracy ---->  0.7410947680473328\n",
      "Iter:892 train_rmse:3.736 test_loss:109.8 test_rmse:0.04866 test_acc:0.707\n",
      "Epoch  893\n",
      "Accuracy ---->  0.7411109209060669\n",
      "Iter:893 train_rmse:3.736 test_loss:109.8 test_rmse:0.04866 test_acc:0.7071\n",
      "Epoch  894\n",
      "Accuracy ---->  0.7411268949508667\n",
      "Iter:894 train_rmse:3.736 test_loss:109.8 test_rmse:0.04865 test_acc:0.7071\n",
      "Epoch  895\n",
      "Accuracy ---->  0.7411426305770874\n",
      "Iter:895 train_rmse:3.735 test_loss:109.8 test_rmse:0.04865 test_acc:0.7071\n",
      "Epoch  896\n",
      "Accuracy ---->  0.7411587834358215\n",
      "Iter:896 train_rmse:3.735 test_loss:109.8 test_rmse:0.04865 test_acc:0.7071\n",
      "Epoch  897\n",
      "Accuracy ---->  0.7411746680736542\n",
      "Iter:897 train_rmse:3.735 test_loss:109.8 test_rmse:0.04865 test_acc:0.7071\n",
      "Epoch  898\n",
      "Accuracy ---->  0.7411907315254211\n",
      "Iter:898 train_rmse:3.735 test_loss:109.8 test_rmse:0.04865 test_acc:0.7071\n",
      "Epoch  899\n",
      "Accuracy ---->  0.7412070035934448\n",
      "Iter:899 train_rmse:3.735 test_loss:109.8 test_rmse:0.04864 test_acc:0.7071\n",
      "Epoch  900\n",
      "Accuracy ---->  0.7412231862545013\n",
      "Iter:900 train_rmse:3.734 test_loss:109.8 test_rmse:0.04864 test_acc:0.7072\n",
      "Epoch  901\n",
      "Accuracy ---->  0.7412393689155579\n",
      "Iter:901 train_rmse:3.734 test_loss:109.7 test_rmse:0.04864 test_acc:0.7072\n",
      "Epoch  902\n",
      "Accuracy ---->  0.7412555515766144\n",
      "Iter:902 train_rmse:3.734 test_loss:109.7 test_rmse:0.04864 test_acc:0.7072\n",
      "Epoch  903\n",
      "Accuracy ---->  0.7412721216678619\n",
      "Iter:903 train_rmse:3.734 test_loss:109.7 test_rmse:0.04863 test_acc:0.7072\n",
      "Epoch  904\n",
      "Accuracy ---->  0.7412885427474976\n",
      "Iter:904 train_rmse:3.733 test_loss:109.7 test_rmse:0.04863 test_acc:0.7072\n",
      "Epoch  905\n",
      "Accuracy ---->  0.7413052916526794\n",
      "Iter:905 train_rmse:3.733 test_loss:109.7 test_rmse:0.04863 test_acc:0.7072\n",
      "Epoch  906\n",
      "Accuracy ---->  0.7413216829299927\n",
      "Iter:906 train_rmse:3.733 test_loss:109.7 test_rmse:0.04863 test_acc:0.7072\n",
      "Epoch  907\n",
      "Accuracy ---->  0.7413381040096283\n",
      "Iter:907 train_rmse:3.733 test_loss:109.7 test_rmse:0.04863 test_acc:0.7073\n",
      "Epoch  908\n",
      "Accuracy ---->  0.741354912519455\n",
      "Iter:908 train_rmse:3.732 test_loss:109.7 test_rmse:0.04862 test_acc:0.7073\n",
      "Epoch  909\n",
      "Accuracy ---->  0.7413713932037354\n",
      "Iter:909 train_rmse:3.732 test_loss:109.7 test_rmse:0.04862 test_acc:0.7073\n",
      "Epoch  910\n",
      "Accuracy ---->  0.741388201713562\n",
      "Iter:910 train_rmse:3.732 test_loss:109.7 test_rmse:0.04862 test_acc:0.7073\n",
      "Epoch  911\n",
      "Accuracy ---->  0.7414050996303558\n",
      "Iter:911 train_rmse:3.732 test_loss:109.6 test_rmse:0.04862 test_acc:0.7073\n",
      "Epoch  912\n",
      "Accuracy ---->  0.7414218783378601\n",
      "Iter:912 train_rmse:3.731 test_loss:109.6 test_rmse:0.04861 test_acc:0.7073\n",
      "Epoch  913\n",
      "Accuracy ---->  0.7414388358592987\n",
      "Iter:913 train_rmse:3.731 test_loss:109.6 test_rmse:0.04861 test_acc:0.7073\n",
      "Epoch  914\n",
      "Accuracy ---->  0.7414557933807373\n",
      "Iter:914 train_rmse:3.731 test_loss:109.6 test_rmse:0.04861 test_acc:0.7073\n",
      "Epoch  915\n",
      "Accuracy ---->  0.7414727509021759\n",
      "Iter:915 train_rmse:3.731 test_loss:109.6 test_rmse:0.04861 test_acc:0.7074\n",
      "Epoch  916\n",
      "Accuracy ---->  0.7414899170398712\n",
      "Iter:916 train_rmse:3.73 test_loss:109.6 test_rmse:0.04861 test_acc:0.7074\n",
      "Epoch  917\n",
      "Accuracy ---->  0.7415068447589874\n",
      "Iter:917 train_rmse:3.73 test_loss:109.6 test_rmse:0.0486 test_acc:0.7074\n",
      "Epoch  918\n",
      "Accuracy ---->  0.7415239214897156\n",
      "Iter:918 train_rmse:3.73 test_loss:109.6 test_rmse:0.0486 test_acc:0.7074\n",
      "Epoch  919\n",
      "Accuracy ---->  0.7415409982204437\n",
      "Iter:919 train_rmse:3.73 test_loss:109.6 test_rmse:0.0486 test_acc:0.7074\n",
      "Epoch  920\n",
      "Accuracy ---->  0.7415581941604614\n",
      "Iter:920 train_rmse:3.729 test_loss:109.6 test_rmse:0.0486 test_acc:0.7074\n",
      "Epoch  921\n",
      "Accuracy ---->  0.7415754497051239\n",
      "Iter:921 train_rmse:3.729 test_loss:109.5 test_rmse:0.0486 test_acc:0.7074\n",
      "Epoch  922\n",
      "Accuracy ---->  0.7415926158428192\n",
      "Iter:922 train_rmse:3.729 test_loss:109.5 test_rmse:0.04859 test_acc:0.7074\n",
      "Epoch  923\n",
      "Accuracy ---->  0.7416100203990936\n",
      "Iter:923 train_rmse:3.729 test_loss:109.5 test_rmse:0.04859 test_acc:0.7075\n",
      "Epoch  924\n",
      "Accuracy ---->  0.7416272759437561\n",
      "Iter:924 train_rmse:3.728 test_loss:109.5 test_rmse:0.04859 test_acc:0.7075\n",
      "Epoch  925\n",
      "Accuracy ---->  0.7416448593139648\n",
      "Iter:925 train_rmse:3.728 test_loss:109.5 test_rmse:0.04859 test_acc:0.7075\n",
      "Epoch  926\n",
      "Accuracy ---->  0.7416622042655945\n",
      "Iter:926 train_rmse:3.728 test_loss:109.5 test_rmse:0.04858 test_acc:0.7075\n",
      "Epoch  927\n",
      "Accuracy ---->  0.7416798174381256\n",
      "Iter:927 train_rmse:3.728 test_loss:109.5 test_rmse:0.04858 test_acc:0.7075\n",
      "Epoch  928\n",
      "Accuracy ---->  0.7416975498199463\n",
      "Iter:928 train_rmse:3.727 test_loss:109.5 test_rmse:0.04858 test_acc:0.7075\n",
      "Epoch  929\n",
      "Accuracy ---->  0.7417147159576416\n",
      "Iter:929 train_rmse:3.727 test_loss:109.5 test_rmse:0.04858 test_acc:0.7075\n",
      "Epoch  930\n",
      "Accuracy ---->  0.7417322099208832\n",
      "Iter:930 train_rmse:3.727 test_loss:109.5 test_rmse:0.04858 test_acc:0.7075\n",
      "Epoch  931\n",
      "Accuracy ---->  0.7417498230934143\n",
      "Iter:931 train_rmse:3.727 test_loss:109.5 test_rmse:0.04857 test_acc:0.7076\n",
      "Epoch  932\n",
      "Accuracy ---->  0.7417676150798798\n",
      "Iter:932 train_rmse:3.726 test_loss:109.4 test_rmse:0.04857 test_acc:0.7076\n",
      "Epoch  933\n",
      "Accuracy ---->  0.7417849600315094\n",
      "Iter:933 train_rmse:3.726 test_loss:109.4 test_rmse:0.04857 test_acc:0.7076\n",
      "Epoch  934\n",
      "Accuracy ---->  0.7418026924133301\n",
      "Iter:934 train_rmse:3.726 test_loss:109.4 test_rmse:0.04857 test_acc:0.7076\n",
      "Epoch  935\n",
      "Accuracy ---->  0.7418203055858612\n",
      "Iter:935 train_rmse:3.726 test_loss:109.4 test_rmse:0.04857 test_acc:0.7076\n",
      "Epoch  936\n",
      "Accuracy ---->  0.7418379485607147\n",
      "Iter:936 train_rmse:3.725 test_loss:109.4 test_rmse:0.04856 test_acc:0.7076\n",
      "Epoch  937\n",
      "Accuracy ---->  0.7418559789657593\n",
      "Iter:937 train_rmse:3.725 test_loss:109.4 test_rmse:0.04856 test_acc:0.7076\n",
      "Epoch  938\n",
      "Accuracy ---->  0.7418733239173889\n",
      "Iter:938 train_rmse:3.725 test_loss:109.4 test_rmse:0.04856 test_acc:0.7076\n",
      "Epoch  939\n",
      "Accuracy ---->  0.7418912649154663\n",
      "Iter:939 train_rmse:3.725 test_loss:109.4 test_rmse:0.04856 test_acc:0.7077\n",
      "Epoch  940\n",
      "Accuracy ---->  0.741908848285675\n",
      "Iter:940 train_rmse:3.724 test_loss:109.4 test_rmse:0.04856 test_acc:0.7077\n",
      "Epoch  941\n",
      "Accuracy ---->  0.7419266998767853\n",
      "Iter:941 train_rmse:3.724 test_loss:109.4 test_rmse:0.04856 test_acc:0.7077\n",
      "Epoch  942\n",
      "Accuracy ---->  0.7419446408748627\n",
      "Iter:942 train_rmse:3.724 test_loss:109.4 test_rmse:0.04855 test_acc:0.7077\n",
      "Epoch  943\n",
      "Accuracy ---->  0.7419624924659729\n",
      "Iter:943 train_rmse:3.724 test_loss:109.3 test_rmse:0.04855 test_acc:0.7077\n",
      "Epoch  944\n",
      "Accuracy ---->  0.7419803440570831\n",
      "Iter:944 train_rmse:3.723 test_loss:109.3 test_rmse:0.04855 test_acc:0.7077\n",
      "Epoch  945\n",
      "Accuracy ---->  0.7419981062412262\n",
      "Iter:945 train_rmse:3.723 test_loss:109.3 test_rmse:0.04855 test_acc:0.7077\n",
      "Epoch  946\n",
      "Accuracy ---->  0.7420161068439484\n",
      "Iter:946 train_rmse:3.723 test_loss:109.3 test_rmse:0.04855 test_acc:0.7077\n",
      "Epoch  947\n",
      "Accuracy ---->  0.7420339286327362\n",
      "Iter:947 train_rmse:3.723 test_loss:109.3 test_rmse:0.04854 test_acc:0.7077\n",
      "Epoch  948\n",
      "Accuracy ---->  0.7420516908168793\n",
      "Iter:948 train_rmse:3.722 test_loss:109.3 test_rmse:0.04854 test_acc:0.7078\n",
      "Epoch  949\n",
      "Accuracy ---->  0.7420695424079895\n",
      "Iter:949 train_rmse:3.722 test_loss:109.3 test_rmse:0.04854 test_acc:0.7078\n",
      "Epoch  950\n",
      "Accuracy ---->  0.742087185382843\n",
      "Iter:950 train_rmse:3.722 test_loss:109.3 test_rmse:0.04854 test_acc:0.7078\n",
      "Epoch  951\n",
      "Accuracy ---->  0.7421053946018219\n",
      "Iter:951 train_rmse:3.722 test_loss:109.3 test_rmse:0.04854 test_acc:0.7078\n",
      "Epoch  952\n",
      "Accuracy ---->  0.742123156785965\n",
      "Iter:952 train_rmse:3.721 test_loss:109.3 test_rmse:0.04853 test_acc:0.7078\n",
      "Epoch  953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.74214106798172\n",
      "Iter:953 train_rmse:3.721 test_loss:109.3 test_rmse:0.04853 test_acc:0.7078\n",
      "Epoch  954\n",
      "Accuracy ---->  0.7421589493751526\n",
      "Iter:954 train_rmse:3.721 test_loss:109.3 test_rmse:0.04853 test_acc:0.7078\n",
      "Epoch  955\n",
      "Accuracy ---->  0.7421765625476837\n",
      "Iter:955 train_rmse:3.721 test_loss:109.2 test_rmse:0.04853 test_acc:0.7078\n",
      "Epoch  956\n",
      "Accuracy ---->  0.7421945631504059\n",
      "Iter:956 train_rmse:3.72 test_loss:109.2 test_rmse:0.04853 test_acc:0.7078\n",
      "Epoch  957\n",
      "Accuracy ---->  0.7422124147415161\n",
      "Iter:957 train_rmse:3.72 test_loss:109.2 test_rmse:0.04852 test_acc:0.7079\n",
      "Epoch  958\n",
      "Accuracy ---->  0.7422302663326263\n",
      "Iter:958 train_rmse:3.72 test_loss:109.2 test_rmse:0.04852 test_acc:0.7079\n",
      "Epoch  959\n",
      "Accuracy ---->  0.7422482669353485\n",
      "Iter:959 train_rmse:3.72 test_loss:109.2 test_rmse:0.04852 test_acc:0.7079\n",
      "Epoch  960\n",
      "Accuracy ---->  0.7422662079334259\n",
      "Iter:960 train_rmse:3.719 test_loss:109.2 test_rmse:0.04852 test_acc:0.7079\n",
      "Epoch  961\n",
      "Accuracy ---->  0.7422837615013123\n",
      "Iter:961 train_rmse:3.719 test_loss:109.2 test_rmse:0.04852 test_acc:0.7079\n",
      "Epoch  962\n",
      "Accuracy ---->  0.7423015534877777\n",
      "Iter:962 train_rmse:3.719 test_loss:109.2 test_rmse:0.04852 test_acc:0.7079\n",
      "Epoch  963\n",
      "Accuracy ---->  0.7423192858695984\n",
      "Iter:963 train_rmse:3.719 test_loss:109.2 test_rmse:0.04851 test_acc:0.7079\n",
      "Epoch  964\n",
      "Accuracy ---->  0.7423370480537415\n",
      "Iter:964 train_rmse:3.718 test_loss:109.2 test_rmse:0.04851 test_acc:0.7079\n",
      "Epoch  965\n",
      "Accuracy ---->  0.7423549294471741\n",
      "Iter:965 train_rmse:3.718 test_loss:109.2 test_rmse:0.04851 test_acc:0.7079\n",
      "Epoch  966\n",
      "Accuracy ---->  0.74237260222435\n",
      "Iter:966 train_rmse:3.718 test_loss:109.2 test_rmse:0.04851 test_acc:0.708\n",
      "Epoch  967\n",
      "Accuracy ---->  0.742390364408493\n",
      "Iter:967 train_rmse:3.717 test_loss:109.1 test_rmse:0.04851 test_acc:0.708\n",
      "Epoch  968\n",
      "Accuracy ---->  0.7424080073833466\n",
      "Iter:968 train_rmse:3.717 test_loss:109.1 test_rmse:0.0485 test_acc:0.708\n",
      "Epoch  969\n",
      "Accuracy ---->  0.7424256801605225\n",
      "Iter:969 train_rmse:3.717 test_loss:109.1 test_rmse:0.0485 test_acc:0.708\n",
      "Epoch  970\n",
      "Accuracy ---->  0.7424431443214417\n",
      "Iter:970 train_rmse:3.717 test_loss:109.1 test_rmse:0.0485 test_acc:0.708\n",
      "Epoch  971\n",
      "Accuracy ---->  0.7424609959125519\n",
      "Iter:971 train_rmse:3.716 test_loss:109.1 test_rmse:0.0485 test_acc:0.708\n",
      "Epoch  972\n",
      "Accuracy ---->  0.7424786388874054\n",
      "Iter:972 train_rmse:3.716 test_loss:109.1 test_rmse:0.0485 test_acc:0.708\n",
      "Epoch  973\n",
      "Accuracy ---->  0.7424962818622589\n",
      "Iter:973 train_rmse:3.716 test_loss:109.1 test_rmse:0.0485 test_acc:0.708\n",
      "Epoch  974\n",
      "Accuracy ---->  0.7425139248371124\n",
      "Iter:974 train_rmse:3.716 test_loss:109.1 test_rmse:0.04849 test_acc:0.708\n",
      "Epoch  975\n",
      "Accuracy ---->  0.7425313889980316\n",
      "Iter:975 train_rmse:3.715 test_loss:109.1 test_rmse:0.04849 test_acc:0.7081\n",
      "Epoch  976\n",
      "Accuracy ---->  0.7425490319728851\n",
      "Iter:976 train_rmse:3.715 test_loss:109.1 test_rmse:0.04849 test_acc:0.7081\n",
      "Epoch  977\n",
      "Accuracy ---->  0.7425666749477386\n",
      "Iter:977 train_rmse:3.715 test_loss:109.1 test_rmse:0.04849 test_acc:0.7081\n",
      "Epoch  978\n",
      "Accuracy ---->  0.7425841987133026\n",
      "Iter:978 train_rmse:3.715 test_loss:109.1 test_rmse:0.04849 test_acc:0.7081\n",
      "Epoch  979\n",
      "Accuracy ---->  0.7426015138626099\n",
      "Iter:979 train_rmse:3.714 test_loss:109.0 test_rmse:0.04848 test_acc:0.7081\n",
      "Epoch  980\n",
      "Accuracy ---->  0.742618978023529\n",
      "Iter:980 train_rmse:3.714 test_loss:109.0 test_rmse:0.04848 test_acc:0.7081\n",
      "Epoch  981\n",
      "Accuracy ---->  0.7426363229751587\n",
      "Iter:981 train_rmse:3.714 test_loss:109.0 test_rmse:0.04848 test_acc:0.7081\n",
      "Epoch  982\n",
      "Accuracy ---->  0.7426539957523346\n",
      "Iter:982 train_rmse:3.714 test_loss:109.0 test_rmse:0.04848 test_acc:0.7081\n",
      "Epoch  983\n",
      "Accuracy ---->  0.7426712214946747\n",
      "Iter:983 train_rmse:3.713 test_loss:109.0 test_rmse:0.04848 test_acc:0.7081\n",
      "Epoch  984\n",
      "Accuracy ---->  0.7426885664463043\n",
      "Iter:984 train_rmse:3.713 test_loss:109.0 test_rmse:0.04848 test_acc:0.7081\n",
      "Epoch  985\n",
      "Accuracy ---->  0.7427060604095459\n",
      "Iter:985 train_rmse:3.713 test_loss:109.0 test_rmse:0.04847 test_acc:0.7082\n",
      "Epoch  986\n",
      "Accuracy ---->  0.7427232563495636\n",
      "Iter:986 train_rmse:3.713 test_loss:109.0 test_rmse:0.04847 test_acc:0.7082\n",
      "Epoch  987\n",
      "Accuracy ---->  0.7427406013011932\n",
      "Iter:987 train_rmse:3.712 test_loss:109.0 test_rmse:0.04847 test_acc:0.7082\n",
      "Epoch  988\n",
      "Accuracy ---->  0.7427577078342438\n",
      "Iter:988 train_rmse:3.712 test_loss:109.0 test_rmse:0.04847 test_acc:0.7082\n",
      "Epoch  989\n",
      "Accuracy ---->  0.7427747845649719\n",
      "Iter:989 train_rmse:3.712 test_loss:109.0 test_rmse:0.04847 test_acc:0.7082\n",
      "Epoch  990\n",
      "Accuracy ---->  0.7427921295166016\n",
      "Iter:990 train_rmse:3.712 test_loss:109.0 test_rmse:0.04847 test_acc:0.7082\n",
      "Epoch  991\n",
      "Accuracy ---->  0.7428090274333954\n",
      "Iter:991 train_rmse:3.711 test_loss:109.0 test_rmse:0.04846 test_acc:0.7082\n",
      "Epoch  992\n",
      "Accuracy ---->  0.7428261637687683\n",
      "Iter:992 train_rmse:3.711 test_loss:108.9 test_rmse:0.04846 test_acc:0.7082\n",
      "Epoch  993\n",
      "Accuracy ---->  0.7428432703018188\n",
      "Iter:993 train_rmse:3.711 test_loss:108.9 test_rmse:0.04846 test_acc:0.7082\n",
      "Epoch  994\n",
      "Accuracy ---->  0.7428602874279022\n",
      "Iter:994 train_rmse:3.711 test_loss:108.9 test_rmse:0.04846 test_acc:0.7083\n",
      "Epoch  995\n",
      "Accuracy ---->  0.742877334356308\n",
      "Iter:995 train_rmse:3.71 test_loss:108.9 test_rmse:0.04846 test_acc:0.7083\n",
      "Epoch  996\n",
      "Accuracy ---->  0.7428940534591675\n",
      "Iter:996 train_rmse:3.71 test_loss:108.9 test_rmse:0.04846 test_acc:0.7083\n",
      "Epoch  997\n",
      "Accuracy ---->  0.7429109811782837\n",
      "Iter:997 train_rmse:3.71 test_loss:108.9 test_rmse:0.04845 test_acc:0.7083\n",
      "Epoch  998\n",
      "Accuracy ---->  0.7429278790950775\n",
      "Iter:998 train_rmse:3.71 test_loss:108.9 test_rmse:0.04845 test_acc:0.7083\n",
      "Epoch  999\n",
      "Accuracy ---->  0.7429444491863251\n",
      "Iter:999 train_rmse:3.709 test_loss:108.9 test_rmse:0.04845 test_acc:0.7083\n",
      "Time taken :  31327.595640182495 s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epoch):\n",
    "    print(\"Epoch \", epoch)\n",
    "    for m in range(totalbatch):\n",
    "        mini_batch = trainX[m * batch_size : (m+1) * batch_size]\n",
    "        mini_label = trainY[m * batch_size : (m+1) * batch_size]\n",
    "        _, loss1, rmse1, train_output = sess.run([optimizer, loss, error, y_pred],\n",
    "                                                 feed_dict = {inputs:mini_batch, labels:mini_label})\n",
    "        batch_loss.append(loss1)\n",
    "        batch_rmse.append(rmse1 * max_value)\n",
    "        train_label=np.reshape(mini_label,[-1,num_nodes])\n",
    "     #print(mini_label.shape,train_output.shape) (32, 1, 156) (32, 156)\n",
    "     # Test completely at every epoch\n",
    "    print(\"Accuracy ----> \", evaluation(train_label,train_output)[2])\n",
    "    loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n",
    "                                         feed_dict = {inputs:testX, labels:testY})\n",
    "    #train_label=np.reshape(trainY,[-1,num_nodes])\n",
    "    #train_acc=acc(train_label,train_output)\n",
    "    test_label = np.reshape(testY,[-1,num_nodes])\n",
    "    rmse, mae, acc, r2_score, var_score = evaluation(test_label, test_output)\n",
    "    test_label1 = test_label * max_value#Inverse normalization\n",
    "    test_output1 = test_output * max_value\n",
    "    test_loss.append(loss2)\n",
    "    test_rmse.append(rmse * max_value)\n",
    "    test_mae.append(mae * max_value)\n",
    "    test_acc.append(acc)\n",
    "    test_r2.append(r2_score)\n",
    "    test_var.append(var_score)\n",
    "    test_pred.append(test_output1)\n",
    "    #print(mini_label.shape,train_output.shape)\n",
    "    print('Iter:{}'.format(epoch),\n",
    "          'train_rmse:{:.4}'.format(batch_rmse[-1]),\n",
    "          'test_loss:{:.4}'.format(loss2),\n",
    "          'test_rmse:{:.4}'.format(rmse),\n",
    "          'test_acc:{:.4}'.format(acc))\n",
    "    if (epoch % 500 == 0):        \n",
    "        saver.save(sess, path+'/model_10015TGCN_pre_%r'%epoch, global_step = epoch)\n",
    "        \n",
    "time_end = time.time()\n",
    "print('Time taken : ',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = int(len(batch_rmse)/totalbatch)\n",
    "batch_rmse1 = [i for i in batch_rmse]\n",
    "train_rmse = [(sum(batch_rmse1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "batch_loss1 = [i for i in batch_loss]\n",
    "train_loss = [(sum(batch_loss1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "index = test_rmse.index(np.min(test_rmse))\n",
    "test_result = test_pred[index]\n",
    "var = pd.DataFrame(test_result)\n",
    "var.to_csv(path+'/test_result15.csv',index = False,header = False)\n",
    "#plot_result(test_result,test_label1,path)\n",
    "#plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:\n",
      "min_rmse:4.187500315909929 min_mae:2.9068809 max_acc:0.7083063423633575 r2:0.839141309261322 var:0.8405239880084991\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing:\")\n",
    "print('min_rmse:%r'%(np.min(test_rmse)),\n",
    "      'min_mae:%r'%(test_mae[index]),\n",
    "      'max_acc:%r'%(test_acc[index]),\n",
    "      'r2:%r'%(test_r2[index]),\n",
    "      'var:%r'%test_var[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "min_rmse:0.04291936626660577 min_mae:0.029366774 max_acc:0.7429444491863251 r2:0.866444393992424 var:0.8671977669000626\n"
     ]
    }
   ],
   "source": [
    "print(\"Training:\")\n",
    "rmse, mae, acc, r2_score, var_score = evaluation(train_label,train_output)\n",
    "print('min_rmse:%r'%(rmse),\n",
    "      'min_mae:%r'%(mae),\n",
    "      'max_acc:%r'%(acc),\n",
    "      'r2:%r'%(r2_score),\n",
    "      'var:%r'%(var_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_rmse:3.7094864573989685 min_mae:2.5381467 max_acc:0.7429444491863251 r2:0.866444393992424 var:0.8671977669000626\n"
     ]
    }
   ],
   "source": [
    "#inverse normalisation\n",
    "print('min_rmse:%r'%(rmse*max_value),\n",
    "      'min_mae:%r'%(mae*max_value),\n",
    "      'max_acc:%r'%(acc),\n",
    "      'r2:%r'%(r2_score),\n",
    "      'var:%r'%(var_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction \n",
    "tX=np.reshape(testX[20],[1,4,num_nodes])\n",
    "tY=np.reshape(testY[20],[1,1,num_nodes])\n",
    "loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n",
    "                                         feed_dict = {inputs:tX, labels:tY})\n",
    "    #train_label=np.reshape(trainY,[-1,num_nodes])\n",
    "    #train_acc=acc(train_label,train_output)\n",
    "test_label = np.reshape(tY,[-1,num_nodes])\n",
    "rmse, mae, acc, r2_score, var_score = evaluation(test_label, test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[29.29551   , 15.119696  ,  7.132342  ,  9.38544   ,\n",
       "          8.0976305 , 21.049278  , 24.49573   , 11.40165   ,\n",
       "          0.        ,  3.2964108 , 14.262126  , 14.995415  ,\n",
       "         24.753956  ,  0.26715812, 21.715477  ,  6.0242944 ,\n",
       "         16.283947  , 15.732587  , 17.235863  ,  7.5746217 ,\n",
       "          3.4769073 ,  7.4714866 ,  4.2187324 , 28.780151  ,\n",
       "         34.35125   , 27.660067  , 24.260397  , 41.93124   ,\n",
       "         15.507386  , 12.059961  , 24.756199  , 13.668964  ,\n",
       "         29.932894  ,  0.10825434, 11.158833  , 16.929441  ,\n",
       "         22.844582  ,  8.912826  , 19.689503  , 16.925692  ,\n",
       "         18.794537  ,  0.        ,  0.        ,  9.682071  ,\n",
       "          4.9009476 ,  2.9654264 ,  2.2652206 ,  6.6382546 ,\n",
       "         16.18701   ,  0.        ,  6.9714546 , 19.95379   ,\n",
       "          8.640154  ,  3.6994228 ,  1.8609215 ,  8.1337805 ,\n",
       "         19.895718  ,  0.        ,  3.146843  , 18.550016  ,\n",
       "         13.431131  ,  0.        ,  0.        ,  0.        ,\n",
       "         10.838272  , 25.969774  , 43.96957   , 14.485154  ,\n",
       "         16.475756  ,  0.        ,  0.        , 26.047277  ,\n",
       "         15.725354  , 10.113353  , 12.573445  ,  0.        ,\n",
       "          6.4625754 , 16.696527  , 11.483507  ,  7.6143827 ,\n",
       "          6.6301155 ,  0.8926615 ,  8.019481  ,  8.335503  ,\n",
       "          8.162969  ,  0.        ,  0.        ,  7.958084  ,\n",
       "         15.692771  ,  1.5992426 ,  0.        ,  0.        ,\n",
       "          0.10261232,  8.521705  ,  0.        ,  0.        ,\n",
       "         17.038729  , 18.914003  ,  0.        ,  2.0680726 ,\n",
       "          0.        ,  5.7484355 , 19.932917  ,  0.        ,\n",
       "          3.9561787 ,  0.7949471 ,  1.2717402 ,  0.99544317,\n",
       "          0.        ,  0.        ,  0.        ,  0.17434761,\n",
       "          0.        ,  0.        ,  0.        , 11.778504  ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         15.379426  ,  4.281718  , 27.985474  , 28.161993  ,\n",
       "          2.2335038 ,  7.476153  , 28.930946  , 10.756957  ,\n",
       "         31.604687  ,  7.156183  ,  4.65813   ,  0.        ,\n",
       "          3.0190878 , 18.90656   , 17.338524  ,  0.        ,\n",
       "          0.        ,  1.1071846 ,  0.23985961,  0.27643245,\n",
       "          0.        ,  2.1364892 ,  1.9957958 ,  0.        ,\n",
       "          8.015218  ,  1.2732906 ,  0.79421157, 13.326923  ,\n",
       "         40.870804  , 51.644688  , 28.571379  ,  0.        ,\n",
       "         25.19429   , 13.646876  , 17.831161  ,  0.        ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Y Output \n",
    "tY*max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31.967962  , 14.1435    , 10.183433  ,  9.745589  ,  7.82525   ,\n",
       "        21.418728  , 19.374247  , 14.935576  ,  0.6756145 , 13.512066  ,\n",
       "        12.117996  , 16.809835  , 26.378202  ,  1.6544124 , 15.090639  ,\n",
       "         3.621889  , 13.106642  , 10.491616  , 13.353155  , 10.3491335 ,\n",
       "         1.0337447 , 11.996146  ,  6.0781684 , 22.107927  , 36.583748  ,\n",
       "        25.278082  , 21.816801  , 37.144905  , 17.741226  , 14.239907  ,\n",
       "        20.404701  , 13.323987  , 32.629642  ,  4.7194386 , 16.126942  ,\n",
       "        13.801301  , 14.587566  , 16.365835  , 18.409277  , 11.552677  ,\n",
       "        12.769955  ,  3.3508334 ,  2.5234535 ,  8.464407  ,  5.7526298 ,\n",
       "         5.6795287 ,  4.301393  ,  8.620469  , 18.305277  ,  0.4188545 ,\n",
       "         7.4478245 , 20.67942   , 11.679771  ,  3.8727298 , -0.06691904,\n",
       "         9.492405  , 18.558647  ,  2.4823387 ,  5.258217  , 12.610915  ,\n",
       "        13.344449  ,  0.5247092 ,  3.881575  , -0.33263248,  7.2102747 ,\n",
       "        28.701942  , 40.504017  , 17.192032  , 21.96373   ,  3.8877106 ,\n",
       "         2.7373729 , 18.208006  ,  8.124022  ,  8.290546  ,  9.786695  ,\n",
       "         2.9870596 , 11.469984  , 17.418308  , 11.2062845 ,  9.38961   ,\n",
       "        11.038724  ,  5.660772  ,  8.814951  , 10.422404  ,  8.010068  ,\n",
       "         1.7112602 ,  1.4970677 ,  8.810635  , 12.594641  ,  1.4778214 ,\n",
       "         1.004015  ,  5.2868185 ,  5.551012  , 11.129722  ,  2.2624948 ,\n",
       "         1.2431668 , 19.20207   , 23.85991   ,  1.4989841 , -0.05639952,\n",
       "         0.756672  ,  2.4735088 , 23.474714  ,  0.25597438,  5.1382775 ,\n",
       "         1.8639736 , -0.39667436,  4.2278285 ,  0.80809253,  0.52151525,\n",
       "        -0.07340489,  0.0759137 ,  0.67581284,  0.51689166,  3.4189117 ,\n",
       "        11.871771  ,  0.23108965,  2.2380247 ,  1.144004  ,  0.7170564 ,\n",
       "        15.570262  ,  1.7060982 , 20.084974  , 22.242323  ,  2.716514  ,\n",
       "        13.809482  , 19.540302  ,  8.388709  , 27.745932  ,  4.5758386 ,\n",
       "         5.6484904 ,  5.232696  ,  4.3098826 , 18.529072  , 17.734688  ,\n",
       "        -0.30221498,  0.1801843 ,  1.755234  ,  0.7162167 ,  1.7409487 ,\n",
       "         0.74672693,  2.607996  ,  0.9105292 ,  0.98169315,  9.58534   ,\n",
       "         3.1955545 ,  3.563187  , 14.845846  , 38.517864  , 41.692924  ,\n",
       "        26.737139  , -0.46793878, 28.13752   , 22.658648  , 13.22657   ,\n",
       "         1.9712089 ]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Predicted output\n",
    "test_output*max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 156)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.781780</td>\n",
       "      <td>13.543207</td>\n",
       "      <td>9.240832</td>\n",
       "      <td>10.412905</td>\n",
       "      <td>9.884580</td>\n",
       "      <td>22.752977</td>\n",
       "      <td>19.330700</td>\n",
       "      <td>13.266711</td>\n",
       "      <td>2.156584</td>\n",
       "      <td>12.188547</td>\n",
       "      <td>...</td>\n",
       "      <td>2.532453</td>\n",
       "      <td>14.238125</td>\n",
       "      <td>39.016079</td>\n",
       "      <td>45.888344</td>\n",
       "      <td>26.260866</td>\n",
       "      <td>-0.624887</td>\n",
       "      <td>29.851168</td>\n",
       "      <td>17.570044</td>\n",
       "      <td>12.993816</td>\n",
       "      <td>1.143283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.424919</td>\n",
       "      <td>12.902494</td>\n",
       "      <td>9.220963</td>\n",
       "      <td>10.195102</td>\n",
       "      <td>10.080056</td>\n",
       "      <td>22.279537</td>\n",
       "      <td>18.414099</td>\n",
       "      <td>13.781658</td>\n",
       "      <td>1.430684</td>\n",
       "      <td>12.345279</td>\n",
       "      <td>...</td>\n",
       "      <td>2.574634</td>\n",
       "      <td>13.570778</td>\n",
       "      <td>38.510880</td>\n",
       "      <td>45.941608</td>\n",
       "      <td>27.839052</td>\n",
       "      <td>-0.053028</td>\n",
       "      <td>30.581821</td>\n",
       "      <td>17.779610</td>\n",
       "      <td>13.447145</td>\n",
       "      <td>1.213957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.481411</td>\n",
       "      <td>12.803867</td>\n",
       "      <td>10.173083</td>\n",
       "      <td>10.076785</td>\n",
       "      <td>9.152813</td>\n",
       "      <td>23.610252</td>\n",
       "      <td>20.509722</td>\n",
       "      <td>13.846017</td>\n",
       "      <td>2.063973</td>\n",
       "      <td>13.358719</td>\n",
       "      <td>...</td>\n",
       "      <td>2.760864</td>\n",
       "      <td>14.444487</td>\n",
       "      <td>39.379204</td>\n",
       "      <td>45.416634</td>\n",
       "      <td>26.891563</td>\n",
       "      <td>-0.303168</td>\n",
       "      <td>30.511915</td>\n",
       "      <td>17.774212</td>\n",
       "      <td>13.883696</td>\n",
       "      <td>2.170426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.822714</td>\n",
       "      <td>13.202147</td>\n",
       "      <td>9.865823</td>\n",
       "      <td>10.210397</td>\n",
       "      <td>9.217444</td>\n",
       "      <td>23.386333</td>\n",
       "      <td>20.641256</td>\n",
       "      <td>13.607705</td>\n",
       "      <td>1.865169</td>\n",
       "      <td>13.013542</td>\n",
       "      <td>...</td>\n",
       "      <td>2.748382</td>\n",
       "      <td>14.293195</td>\n",
       "      <td>38.329445</td>\n",
       "      <td>45.913223</td>\n",
       "      <td>25.951399</td>\n",
       "      <td>0.136386</td>\n",
       "      <td>28.585686</td>\n",
       "      <td>17.366350</td>\n",
       "      <td>13.199390</td>\n",
       "      <td>2.314845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.453362</td>\n",
       "      <td>13.277498</td>\n",
       "      <td>9.639323</td>\n",
       "      <td>10.370049</td>\n",
       "      <td>9.584814</td>\n",
       "      <td>22.759216</td>\n",
       "      <td>20.029161</td>\n",
       "      <td>14.898588</td>\n",
       "      <td>1.978947</td>\n",
       "      <td>13.488507</td>\n",
       "      <td>...</td>\n",
       "      <td>2.650280</td>\n",
       "      <td>13.781360</td>\n",
       "      <td>37.322933</td>\n",
       "      <td>45.921909</td>\n",
       "      <td>24.889179</td>\n",
       "      <td>0.163643</td>\n",
       "      <td>27.572567</td>\n",
       "      <td>19.249397</td>\n",
       "      <td>13.762200</td>\n",
       "      <td>2.318848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31.288458</td>\n",
       "      <td>14.950979</td>\n",
       "      <td>9.898715</td>\n",
       "      <td>11.001036</td>\n",
       "      <td>9.904197</td>\n",
       "      <td>24.030148</td>\n",
       "      <td>19.955633</td>\n",
       "      <td>13.809528</td>\n",
       "      <td>1.205771</td>\n",
       "      <td>13.184121</td>\n",
       "      <td>...</td>\n",
       "      <td>3.348103</td>\n",
       "      <td>14.668760</td>\n",
       "      <td>38.568119</td>\n",
       "      <td>45.894268</td>\n",
       "      <td>25.916553</td>\n",
       "      <td>0.499026</td>\n",
       "      <td>28.156574</td>\n",
       "      <td>19.832701</td>\n",
       "      <td>13.869998</td>\n",
       "      <td>2.995385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33.057083</td>\n",
       "      <td>15.376768</td>\n",
       "      <td>9.884590</td>\n",
       "      <td>10.598233</td>\n",
       "      <td>9.265668</td>\n",
       "      <td>24.438787</td>\n",
       "      <td>21.173065</td>\n",
       "      <td>15.237820</td>\n",
       "      <td>2.307499</td>\n",
       "      <td>12.956710</td>\n",
       "      <td>...</td>\n",
       "      <td>2.588054</td>\n",
       "      <td>14.594851</td>\n",
       "      <td>37.796043</td>\n",
       "      <td>44.322704</td>\n",
       "      <td>27.244667</td>\n",
       "      <td>-0.423854</td>\n",
       "      <td>29.893215</td>\n",
       "      <td>21.061317</td>\n",
       "      <td>13.713863</td>\n",
       "      <td>2.261284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32.050316</td>\n",
       "      <td>15.731661</td>\n",
       "      <td>9.605585</td>\n",
       "      <td>10.558293</td>\n",
       "      <td>9.045577</td>\n",
       "      <td>23.455345</td>\n",
       "      <td>21.410032</td>\n",
       "      <td>15.941216</td>\n",
       "      <td>2.256328</td>\n",
       "      <td>14.698202</td>\n",
       "      <td>...</td>\n",
       "      <td>2.571507</td>\n",
       "      <td>14.472841</td>\n",
       "      <td>37.708397</td>\n",
       "      <td>44.082932</td>\n",
       "      <td>28.286137</td>\n",
       "      <td>0.078791</td>\n",
       "      <td>32.442043</td>\n",
       "      <td>23.391960</td>\n",
       "      <td>14.989348</td>\n",
       "      <td>2.487361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32.575962</td>\n",
       "      <td>15.821782</td>\n",
       "      <td>9.415033</td>\n",
       "      <td>10.422915</td>\n",
       "      <td>9.917982</td>\n",
       "      <td>24.006132</td>\n",
       "      <td>21.368078</td>\n",
       "      <td>14.883246</td>\n",
       "      <td>0.932176</td>\n",
       "      <td>11.598557</td>\n",
       "      <td>...</td>\n",
       "      <td>2.162590</td>\n",
       "      <td>16.434641</td>\n",
       "      <td>39.290337</td>\n",
       "      <td>45.825577</td>\n",
       "      <td>25.466949</td>\n",
       "      <td>0.766499</td>\n",
       "      <td>29.971420</td>\n",
       "      <td>21.363508</td>\n",
       "      <td>14.194089</td>\n",
       "      <td>2.463561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32.088402</td>\n",
       "      <td>15.679485</td>\n",
       "      <td>10.068712</td>\n",
       "      <td>10.327548</td>\n",
       "      <td>9.688314</td>\n",
       "      <td>23.674112</td>\n",
       "      <td>21.473042</td>\n",
       "      <td>15.185500</td>\n",
       "      <td>1.448936</td>\n",
       "      <td>14.227770</td>\n",
       "      <td>...</td>\n",
       "      <td>2.152926</td>\n",
       "      <td>13.750476</td>\n",
       "      <td>38.427174</td>\n",
       "      <td>44.594986</td>\n",
       "      <td>26.771948</td>\n",
       "      <td>-0.286848</td>\n",
       "      <td>29.790285</td>\n",
       "      <td>21.802341</td>\n",
       "      <td>14.719844</td>\n",
       "      <td>1.133119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32.142452</td>\n",
       "      <td>16.328430</td>\n",
       "      <td>10.522639</td>\n",
       "      <td>10.626881</td>\n",
       "      <td>9.315237</td>\n",
       "      <td>23.740805</td>\n",
       "      <td>21.379902</td>\n",
       "      <td>15.602315</td>\n",
       "      <td>1.497485</td>\n",
       "      <td>12.352130</td>\n",
       "      <td>...</td>\n",
       "      <td>2.459481</td>\n",
       "      <td>16.536421</td>\n",
       "      <td>40.480095</td>\n",
       "      <td>45.965164</td>\n",
       "      <td>26.774982</td>\n",
       "      <td>0.115086</td>\n",
       "      <td>29.890087</td>\n",
       "      <td>21.311323</td>\n",
       "      <td>14.317273</td>\n",
       "      <td>1.658492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31.705427</td>\n",
       "      <td>14.819589</td>\n",
       "      <td>9.852552</td>\n",
       "      <td>10.198965</td>\n",
       "      <td>9.176675</td>\n",
       "      <td>21.909031</td>\n",
       "      <td>20.233288</td>\n",
       "      <td>14.621185</td>\n",
       "      <td>1.225188</td>\n",
       "      <td>11.086762</td>\n",
       "      <td>...</td>\n",
       "      <td>1.800733</td>\n",
       "      <td>14.776584</td>\n",
       "      <td>37.498360</td>\n",
       "      <td>45.133099</td>\n",
       "      <td>26.510418</td>\n",
       "      <td>0.306967</td>\n",
       "      <td>28.946545</td>\n",
       "      <td>19.771706</td>\n",
       "      <td>13.997062</td>\n",
       "      <td>2.148985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>31.654108</td>\n",
       "      <td>13.674428</td>\n",
       "      <td>9.995523</td>\n",
       "      <td>10.510631</td>\n",
       "      <td>8.928997</td>\n",
       "      <td>21.175322</td>\n",
       "      <td>20.019487</td>\n",
       "      <td>14.790354</td>\n",
       "      <td>1.285487</td>\n",
       "      <td>10.622425</td>\n",
       "      <td>...</td>\n",
       "      <td>2.180281</td>\n",
       "      <td>15.341624</td>\n",
       "      <td>37.137039</td>\n",
       "      <td>44.377518</td>\n",
       "      <td>26.134094</td>\n",
       "      <td>0.179677</td>\n",
       "      <td>28.635550</td>\n",
       "      <td>20.946009</td>\n",
       "      <td>14.778438</td>\n",
       "      <td>2.056277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33.681114</td>\n",
       "      <td>14.566197</td>\n",
       "      <td>9.647560</td>\n",
       "      <td>10.599469</td>\n",
       "      <td>9.014348</td>\n",
       "      <td>22.363663</td>\n",
       "      <td>20.482346</td>\n",
       "      <td>14.730296</td>\n",
       "      <td>1.312373</td>\n",
       "      <td>10.051073</td>\n",
       "      <td>...</td>\n",
       "      <td>1.719987</td>\n",
       "      <td>17.660181</td>\n",
       "      <td>40.053410</td>\n",
       "      <td>46.023632</td>\n",
       "      <td>26.839706</td>\n",
       "      <td>0.932042</td>\n",
       "      <td>29.924484</td>\n",
       "      <td>20.535357</td>\n",
       "      <td>14.725108</td>\n",
       "      <td>1.947110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>34.314930</td>\n",
       "      <td>14.067004</td>\n",
       "      <td>10.021678</td>\n",
       "      <td>10.661773</td>\n",
       "      <td>9.528600</td>\n",
       "      <td>23.235136</td>\n",
       "      <td>19.654718</td>\n",
       "      <td>15.621077</td>\n",
       "      <td>1.370236</td>\n",
       "      <td>8.938208</td>\n",
       "      <td>...</td>\n",
       "      <td>1.472469</td>\n",
       "      <td>16.354595</td>\n",
       "      <td>43.642738</td>\n",
       "      <td>46.681873</td>\n",
       "      <td>27.538479</td>\n",
       "      <td>0.483097</td>\n",
       "      <td>30.233673</td>\n",
       "      <td>19.460865</td>\n",
       "      <td>15.858761</td>\n",
       "      <td>0.625085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>33.594650</td>\n",
       "      <td>14.891633</td>\n",
       "      <td>9.892827</td>\n",
       "      <td>10.524937</td>\n",
       "      <td>9.475529</td>\n",
       "      <td>23.522650</td>\n",
       "      <td>19.507353</td>\n",
       "      <td>13.438053</td>\n",
       "      <td>1.019480</td>\n",
       "      <td>9.601454</td>\n",
       "      <td>...</td>\n",
       "      <td>1.302400</td>\n",
       "      <td>16.315819</td>\n",
       "      <td>41.787819</td>\n",
       "      <td>46.293606</td>\n",
       "      <td>27.231325</td>\n",
       "      <td>-0.319246</td>\n",
       "      <td>29.524923</td>\n",
       "      <td>18.576797</td>\n",
       "      <td>15.613200</td>\n",
       "      <td>1.031303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>35.615860</td>\n",
       "      <td>13.534027</td>\n",
       "      <td>9.483946</td>\n",
       "      <td>9.641203</td>\n",
       "      <td>9.010510</td>\n",
       "      <td>24.175474</td>\n",
       "      <td>20.551794</td>\n",
       "      <td>12.490316</td>\n",
       "      <td>0.718726</td>\n",
       "      <td>10.360333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.393325</td>\n",
       "      <td>16.920496</td>\n",
       "      <td>42.040787</td>\n",
       "      <td>46.269127</td>\n",
       "      <td>26.936609</td>\n",
       "      <td>-0.105708</td>\n",
       "      <td>29.881310</td>\n",
       "      <td>17.727764</td>\n",
       "      <td>16.989269</td>\n",
       "      <td>2.418840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>34.037189</td>\n",
       "      <td>13.954793</td>\n",
       "      <td>10.273467</td>\n",
       "      <td>10.399820</td>\n",
       "      <td>9.250853</td>\n",
       "      <td>23.128880</td>\n",
       "      <td>20.997887</td>\n",
       "      <td>14.168969</td>\n",
       "      <td>0.663356</td>\n",
       "      <td>11.970490</td>\n",
       "      <td>...</td>\n",
       "      <td>1.144256</td>\n",
       "      <td>17.546192</td>\n",
       "      <td>41.322636</td>\n",
       "      <td>45.069851</td>\n",
       "      <td>25.560261</td>\n",
       "      <td>-0.289645</td>\n",
       "      <td>29.106857</td>\n",
       "      <td>19.593925</td>\n",
       "      <td>15.682839</td>\n",
       "      <td>2.911563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>34.399284</td>\n",
       "      <td>14.076210</td>\n",
       "      <td>10.188951</td>\n",
       "      <td>11.111928</td>\n",
       "      <td>8.540196</td>\n",
       "      <td>24.196941</td>\n",
       "      <td>21.505682</td>\n",
       "      <td>16.076384</td>\n",
       "      <td>1.735421</td>\n",
       "      <td>14.621701</td>\n",
       "      <td>...</td>\n",
       "      <td>2.037139</td>\n",
       "      <td>16.658972</td>\n",
       "      <td>43.606613</td>\n",
       "      <td>43.793587</td>\n",
       "      <td>28.035353</td>\n",
       "      <td>-0.232530</td>\n",
       "      <td>29.316629</td>\n",
       "      <td>23.281504</td>\n",
       "      <td>14.009090</td>\n",
       "      <td>2.334761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>29.381689</td>\n",
       "      <td>14.556379</td>\n",
       "      <td>10.094573</td>\n",
       "      <td>10.538021</td>\n",
       "      <td>8.235656</td>\n",
       "      <td>22.715500</td>\n",
       "      <td>20.756931</td>\n",
       "      <td>16.333366</td>\n",
       "      <td>1.807651</td>\n",
       "      <td>15.541000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.305068</td>\n",
       "      <td>15.354663</td>\n",
       "      <td>41.044361</td>\n",
       "      <td>42.400471</td>\n",
       "      <td>25.056564</td>\n",
       "      <td>-0.628642</td>\n",
       "      <td>25.033150</td>\n",
       "      <td>23.866545</td>\n",
       "      <td>13.812799</td>\n",
       "      <td>2.746749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31.967962</td>\n",
       "      <td>14.143500</td>\n",
       "      <td>10.183433</td>\n",
       "      <td>9.745589</td>\n",
       "      <td>7.825250</td>\n",
       "      <td>21.418728</td>\n",
       "      <td>19.374247</td>\n",
       "      <td>14.935576</td>\n",
       "      <td>0.675614</td>\n",
       "      <td>13.512066</td>\n",
       "      <td>...</td>\n",
       "      <td>3.563187</td>\n",
       "      <td>14.845846</td>\n",
       "      <td>38.517864</td>\n",
       "      <td>41.692924</td>\n",
       "      <td>26.737139</td>\n",
       "      <td>-0.467939</td>\n",
       "      <td>28.137520</td>\n",
       "      <td>22.658648</td>\n",
       "      <td>13.226570</td>\n",
       "      <td>1.971209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>31.060810</td>\n",
       "      <td>14.566368</td>\n",
       "      <td>10.130197</td>\n",
       "      <td>9.597451</td>\n",
       "      <td>7.892380</td>\n",
       "      <td>19.765875</td>\n",
       "      <td>19.067314</td>\n",
       "      <td>13.548018</td>\n",
       "      <td>0.055717</td>\n",
       "      <td>12.015326</td>\n",
       "      <td>...</td>\n",
       "      <td>2.893146</td>\n",
       "      <td>16.464586</td>\n",
       "      <td>40.258747</td>\n",
       "      <td>43.115482</td>\n",
       "      <td>24.223961</td>\n",
       "      <td>0.217327</td>\n",
       "      <td>27.783529</td>\n",
       "      <td>19.896894</td>\n",
       "      <td>12.726367</td>\n",
       "      <td>1.823863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31.950653</td>\n",
       "      <td>13.695735</td>\n",
       "      <td>10.442800</td>\n",
       "      <td>10.741561</td>\n",
       "      <td>8.712795</td>\n",
       "      <td>21.456635</td>\n",
       "      <td>18.664627</td>\n",
       "      <td>12.981586</td>\n",
       "      <td>-0.125593</td>\n",
       "      <td>10.257301</td>\n",
       "      <td>...</td>\n",
       "      <td>2.748253</td>\n",
       "      <td>15.396751</td>\n",
       "      <td>41.962952</td>\n",
       "      <td>44.298378</td>\n",
       "      <td>26.758188</td>\n",
       "      <td>0.409623</td>\n",
       "      <td>28.686369</td>\n",
       "      <td>19.329536</td>\n",
       "      <td>16.481907</td>\n",
       "      <td>2.044789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>31.506828</td>\n",
       "      <td>13.918685</td>\n",
       "      <td>9.688413</td>\n",
       "      <td>9.960030</td>\n",
       "      <td>8.464375</td>\n",
       "      <td>21.176882</td>\n",
       "      <td>18.386055</td>\n",
       "      <td>11.987475</td>\n",
       "      <td>0.293519</td>\n",
       "      <td>10.096902</td>\n",
       "      <td>...</td>\n",
       "      <td>2.535312</td>\n",
       "      <td>15.294781</td>\n",
       "      <td>41.445568</td>\n",
       "      <td>44.094109</td>\n",
       "      <td>27.801306</td>\n",
       "      <td>0.058164</td>\n",
       "      <td>30.304033</td>\n",
       "      <td>18.604404</td>\n",
       "      <td>15.763044</td>\n",
       "      <td>2.766160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31.712078</td>\n",
       "      <td>14.266314</td>\n",
       "      <td>9.369452</td>\n",
       "      <td>9.006596</td>\n",
       "      <td>9.066339</td>\n",
       "      <td>18.956118</td>\n",
       "      <td>16.020433</td>\n",
       "      <td>10.536100</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>8.991507</td>\n",
       "      <td>...</td>\n",
       "      <td>2.046592</td>\n",
       "      <td>13.399684</td>\n",
       "      <td>39.816753</td>\n",
       "      <td>44.169960</td>\n",
       "      <td>29.887001</td>\n",
       "      <td>-0.439409</td>\n",
       "      <td>32.039719</td>\n",
       "      <td>18.192736</td>\n",
       "      <td>14.330987</td>\n",
       "      <td>1.724685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32.493500</td>\n",
       "      <td>13.225560</td>\n",
       "      <td>9.901771</td>\n",
       "      <td>9.351921</td>\n",
       "      <td>8.735823</td>\n",
       "      <td>18.912119</td>\n",
       "      <td>16.792238</td>\n",
       "      <td>11.548710</td>\n",
       "      <td>1.016744</td>\n",
       "      <td>11.751858</td>\n",
       "      <td>...</td>\n",
       "      <td>1.826104</td>\n",
       "      <td>13.377995</td>\n",
       "      <td>40.585152</td>\n",
       "      <td>44.770378</td>\n",
       "      <td>28.452915</td>\n",
       "      <td>-0.815199</td>\n",
       "      <td>31.857786</td>\n",
       "      <td>19.923029</td>\n",
       "      <td>12.158446</td>\n",
       "      <td>2.076889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32.162338</td>\n",
       "      <td>12.451381</td>\n",
       "      <td>11.095897</td>\n",
       "      <td>10.212534</td>\n",
       "      <td>8.618316</td>\n",
       "      <td>19.959425</td>\n",
       "      <td>18.056461</td>\n",
       "      <td>13.265006</td>\n",
       "      <td>1.435331</td>\n",
       "      <td>11.968399</td>\n",
       "      <td>...</td>\n",
       "      <td>2.191944</td>\n",
       "      <td>13.306806</td>\n",
       "      <td>39.731674</td>\n",
       "      <td>43.895599</td>\n",
       "      <td>24.030102</td>\n",
       "      <td>-0.282466</td>\n",
       "      <td>27.672522</td>\n",
       "      <td>20.704298</td>\n",
       "      <td>11.427772</td>\n",
       "      <td>2.593999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31.139938</td>\n",
       "      <td>12.914868</td>\n",
       "      <td>10.437339</td>\n",
       "      <td>10.958607</td>\n",
       "      <td>8.555090</td>\n",
       "      <td>18.644644</td>\n",
       "      <td>15.969020</td>\n",
       "      <td>13.067201</td>\n",
       "      <td>1.424389</td>\n",
       "      <td>11.677191</td>\n",
       "      <td>...</td>\n",
       "      <td>1.568046</td>\n",
       "      <td>13.117223</td>\n",
       "      <td>38.510696</td>\n",
       "      <td>42.606102</td>\n",
       "      <td>19.630770</td>\n",
       "      <td>-0.230018</td>\n",
       "      <td>23.275143</td>\n",
       "      <td>20.917747</td>\n",
       "      <td>10.967828</td>\n",
       "      <td>2.557552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.866302</td>\n",
       "      <td>12.102680</td>\n",
       "      <td>9.831889</td>\n",
       "      <td>11.238369</td>\n",
       "      <td>9.074184</td>\n",
       "      <td>16.860491</td>\n",
       "      <td>15.138467</td>\n",
       "      <td>12.414568</td>\n",
       "      <td>0.628150</td>\n",
       "      <td>11.953717</td>\n",
       "      <td>...</td>\n",
       "      <td>1.653495</td>\n",
       "      <td>12.276567</td>\n",
       "      <td>36.639637</td>\n",
       "      <td>39.444633</td>\n",
       "      <td>28.802435</td>\n",
       "      <td>1.536694</td>\n",
       "      <td>29.831448</td>\n",
       "      <td>23.005117</td>\n",
       "      <td>13.511803</td>\n",
       "      <td>2.526359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31.818592</td>\n",
       "      <td>11.842531</td>\n",
       "      <td>9.791294</td>\n",
       "      <td>11.004981</td>\n",
       "      <td>9.659589</td>\n",
       "      <td>17.020319</td>\n",
       "      <td>16.117577</td>\n",
       "      <td>11.883063</td>\n",
       "      <td>-0.127813</td>\n",
       "      <td>9.859614</td>\n",
       "      <td>...</td>\n",
       "      <td>1.221839</td>\n",
       "      <td>12.903056</td>\n",
       "      <td>36.750175</td>\n",
       "      <td>38.671898</td>\n",
       "      <td>27.657759</td>\n",
       "      <td>2.006209</td>\n",
       "      <td>29.255573</td>\n",
       "      <td>19.764988</td>\n",
       "      <td>12.416608</td>\n",
       "      <td>1.794474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>31.756578</td>\n",
       "      <td>11.284259</td>\n",
       "      <td>9.828773</td>\n",
       "      <td>11.455976</td>\n",
       "      <td>8.492395</td>\n",
       "      <td>20.421398</td>\n",
       "      <td>18.467779</td>\n",
       "      <td>13.334274</td>\n",
       "      <td>1.144226</td>\n",
       "      <td>10.440152</td>\n",
       "      <td>...</td>\n",
       "      <td>2.145034</td>\n",
       "      <td>13.378692</td>\n",
       "      <td>37.368427</td>\n",
       "      <td>44.584023</td>\n",
       "      <td>24.051682</td>\n",
       "      <td>-1.690115</td>\n",
       "      <td>26.545675</td>\n",
       "      <td>13.484314</td>\n",
       "      <td>10.874492</td>\n",
       "      <td>0.477598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>33.197460</td>\n",
       "      <td>11.923230</td>\n",
       "      <td>9.923149</td>\n",
       "      <td>12.374813</td>\n",
       "      <td>8.624905</td>\n",
       "      <td>22.251518</td>\n",
       "      <td>20.470642</td>\n",
       "      <td>13.699542</td>\n",
       "      <td>2.223652</td>\n",
       "      <td>9.930403</td>\n",
       "      <td>...</td>\n",
       "      <td>1.806333</td>\n",
       "      <td>13.250093</td>\n",
       "      <td>39.237637</td>\n",
       "      <td>45.777420</td>\n",
       "      <td>24.325211</td>\n",
       "      <td>-1.255652</td>\n",
       "      <td>28.530478</td>\n",
       "      <td>12.521112</td>\n",
       "      <td>11.595049</td>\n",
       "      <td>0.883566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>35.037788</td>\n",
       "      <td>11.624953</td>\n",
       "      <td>10.480066</td>\n",
       "      <td>12.223717</td>\n",
       "      <td>9.110847</td>\n",
       "      <td>22.082541</td>\n",
       "      <td>19.785759</td>\n",
       "      <td>14.763874</td>\n",
       "      <td>1.784557</td>\n",
       "      <td>10.636200</td>\n",
       "      <td>...</td>\n",
       "      <td>2.052290</td>\n",
       "      <td>13.644827</td>\n",
       "      <td>39.087391</td>\n",
       "      <td>45.065533</td>\n",
       "      <td>24.665070</td>\n",
       "      <td>-0.998992</td>\n",
       "      <td>28.254641</td>\n",
       "      <td>14.139111</td>\n",
       "      <td>12.435262</td>\n",
       "      <td>1.021180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>33.766441</td>\n",
       "      <td>11.471992</td>\n",
       "      <td>10.524483</td>\n",
       "      <td>12.387187</td>\n",
       "      <td>9.261294</td>\n",
       "      <td>22.309158</td>\n",
       "      <td>20.277967</td>\n",
       "      <td>14.432493</td>\n",
       "      <td>1.422931</td>\n",
       "      <td>10.389841</td>\n",
       "      <td>...</td>\n",
       "      <td>1.267003</td>\n",
       "      <td>15.087630</td>\n",
       "      <td>42.071960</td>\n",
       "      <td>47.747601</td>\n",
       "      <td>24.036304</td>\n",
       "      <td>-0.935661</td>\n",
       "      <td>28.359564</td>\n",
       "      <td>13.434323</td>\n",
       "      <td>12.019987</td>\n",
       "      <td>0.304618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>31.290787</td>\n",
       "      <td>13.494864</td>\n",
       "      <td>10.588075</td>\n",
       "      <td>12.727268</td>\n",
       "      <td>8.812082</td>\n",
       "      <td>19.997614</td>\n",
       "      <td>17.140762</td>\n",
       "      <td>12.718408</td>\n",
       "      <td>-0.363735</td>\n",
       "      <td>8.108701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588895</td>\n",
       "      <td>12.374127</td>\n",
       "      <td>39.636909</td>\n",
       "      <td>46.892666</td>\n",
       "      <td>24.771513</td>\n",
       "      <td>-1.194415</td>\n",
       "      <td>28.522533</td>\n",
       "      <td>13.566863</td>\n",
       "      <td>10.560756</td>\n",
       "      <td>-0.283772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>30.350170</td>\n",
       "      <td>14.057221</td>\n",
       "      <td>10.719115</td>\n",
       "      <td>12.905441</td>\n",
       "      <td>9.577024</td>\n",
       "      <td>16.816950</td>\n",
       "      <td>15.045238</td>\n",
       "      <td>11.115370</td>\n",
       "      <td>-0.902683</td>\n",
       "      <td>7.519416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287515</td>\n",
       "      <td>11.698462</td>\n",
       "      <td>36.238323</td>\n",
       "      <td>44.518650</td>\n",
       "      <td>23.726822</td>\n",
       "      <td>-0.315877</td>\n",
       "      <td>28.383543</td>\n",
       "      <td>12.508492</td>\n",
       "      <td>11.408809</td>\n",
       "      <td>-0.058635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>29.930832</td>\n",
       "      <td>12.973221</td>\n",
       "      <td>10.475307</td>\n",
       "      <td>12.093650</td>\n",
       "      <td>9.057755</td>\n",
       "      <td>16.721470</td>\n",
       "      <td>17.159298</td>\n",
       "      <td>12.562021</td>\n",
       "      <td>0.444991</td>\n",
       "      <td>8.062181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647252</td>\n",
       "      <td>12.807989</td>\n",
       "      <td>35.109875</td>\n",
       "      <td>45.465427</td>\n",
       "      <td>23.383907</td>\n",
       "      <td>-0.373018</td>\n",
       "      <td>29.016657</td>\n",
       "      <td>13.467850</td>\n",
       "      <td>11.133261</td>\n",
       "      <td>1.173765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>31.361580</td>\n",
       "      <td>13.910000</td>\n",
       "      <td>9.983809</td>\n",
       "      <td>11.355268</td>\n",
       "      <td>9.579843</td>\n",
       "      <td>19.488451</td>\n",
       "      <td>17.582531</td>\n",
       "      <td>12.956504</td>\n",
       "      <td>1.554539</td>\n",
       "      <td>9.934797</td>\n",
       "      <td>...</td>\n",
       "      <td>1.034852</td>\n",
       "      <td>12.388017</td>\n",
       "      <td>35.683071</td>\n",
       "      <td>44.678391</td>\n",
       "      <td>21.511875</td>\n",
       "      <td>0.328223</td>\n",
       "      <td>26.996960</td>\n",
       "      <td>13.947426</td>\n",
       "      <td>11.294073</td>\n",
       "      <td>1.796539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>30.906921</td>\n",
       "      <td>15.133026</td>\n",
       "      <td>9.978611</td>\n",
       "      <td>11.097390</td>\n",
       "      <td>9.880603</td>\n",
       "      <td>21.212130</td>\n",
       "      <td>20.057810</td>\n",
       "      <td>14.724084</td>\n",
       "      <td>1.931140</td>\n",
       "      <td>11.545140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.831664</td>\n",
       "      <td>14.285478</td>\n",
       "      <td>36.245979</td>\n",
       "      <td>43.707924</td>\n",
       "      <td>23.404839</td>\n",
       "      <td>0.393617</td>\n",
       "      <td>27.341244</td>\n",
       "      <td>17.067446</td>\n",
       "      <td>13.640881</td>\n",
       "      <td>1.941948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>31.291489</td>\n",
       "      <td>14.617837</td>\n",
       "      <td>10.136744</td>\n",
       "      <td>10.891616</td>\n",
       "      <td>9.386535</td>\n",
       "      <td>21.201496</td>\n",
       "      <td>19.692722</td>\n",
       "      <td>14.204758</td>\n",
       "      <td>1.011114</td>\n",
       "      <td>9.907206</td>\n",
       "      <td>...</td>\n",
       "      <td>1.113739</td>\n",
       "      <td>14.174333</td>\n",
       "      <td>35.919411</td>\n",
       "      <td>43.732407</td>\n",
       "      <td>24.376257</td>\n",
       "      <td>-1.171392</td>\n",
       "      <td>27.414480</td>\n",
       "      <td>17.084208</td>\n",
       "      <td>13.385888</td>\n",
       "      <td>1.459204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>32.045670</td>\n",
       "      <td>14.187469</td>\n",
       "      <td>9.851630</td>\n",
       "      <td>11.752486</td>\n",
       "      <td>9.770060</td>\n",
       "      <td>20.743212</td>\n",
       "      <td>18.915503</td>\n",
       "      <td>14.246563</td>\n",
       "      <td>1.801933</td>\n",
       "      <td>11.740432</td>\n",
       "      <td>...</td>\n",
       "      <td>1.413154</td>\n",
       "      <td>12.928036</td>\n",
       "      <td>36.510242</td>\n",
       "      <td>43.811203</td>\n",
       "      <td>25.751183</td>\n",
       "      <td>-0.438147</td>\n",
       "      <td>28.001728</td>\n",
       "      <td>18.776787</td>\n",
       "      <td>13.790354</td>\n",
       "      <td>0.950727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>33.078056</td>\n",
       "      <td>13.012218</td>\n",
       "      <td>10.194946</td>\n",
       "      <td>11.354506</td>\n",
       "      <td>9.722691</td>\n",
       "      <td>21.546236</td>\n",
       "      <td>19.225107</td>\n",
       "      <td>13.782493</td>\n",
       "      <td>2.274910</td>\n",
       "      <td>11.372928</td>\n",
       "      <td>...</td>\n",
       "      <td>1.135272</td>\n",
       "      <td>12.326888</td>\n",
       "      <td>36.517841</td>\n",
       "      <td>44.706303</td>\n",
       "      <td>24.715025</td>\n",
       "      <td>-0.100938</td>\n",
       "      <td>26.533508</td>\n",
       "      <td>17.616098</td>\n",
       "      <td>14.884014</td>\n",
       "      <td>1.766315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>30.997034</td>\n",
       "      <td>13.994182</td>\n",
       "      <td>10.045185</td>\n",
       "      <td>11.523880</td>\n",
       "      <td>9.976860</td>\n",
       "      <td>21.860435</td>\n",
       "      <td>17.039812</td>\n",
       "      <td>13.663836</td>\n",
       "      <td>3.481441</td>\n",
       "      <td>14.624957</td>\n",
       "      <td>...</td>\n",
       "      <td>2.333932</td>\n",
       "      <td>9.822688</td>\n",
       "      <td>35.962822</td>\n",
       "      <td>42.911034</td>\n",
       "      <td>26.520649</td>\n",
       "      <td>-0.869123</td>\n",
       "      <td>29.136118</td>\n",
       "      <td>19.402121</td>\n",
       "      <td>15.398647</td>\n",
       "      <td>2.388580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>33.210648</td>\n",
       "      <td>13.470467</td>\n",
       "      <td>10.229843</td>\n",
       "      <td>11.769641</td>\n",
       "      <td>10.337141</td>\n",
       "      <td>21.010384</td>\n",
       "      <td>17.969028</td>\n",
       "      <td>15.668698</td>\n",
       "      <td>4.019704</td>\n",
       "      <td>16.031132</td>\n",
       "      <td>...</td>\n",
       "      <td>2.121125</td>\n",
       "      <td>11.293754</td>\n",
       "      <td>38.535179</td>\n",
       "      <td>45.702217</td>\n",
       "      <td>27.377615</td>\n",
       "      <td>0.201934</td>\n",
       "      <td>28.668287</td>\n",
       "      <td>20.327820</td>\n",
       "      <td>14.755957</td>\n",
       "      <td>1.943143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>32.761063</td>\n",
       "      <td>13.826317</td>\n",
       "      <td>9.784741</td>\n",
       "      <td>12.132776</td>\n",
       "      <td>9.758242</td>\n",
       "      <td>20.907465</td>\n",
       "      <td>17.814779</td>\n",
       "      <td>15.225538</td>\n",
       "      <td>3.106329</td>\n",
       "      <td>17.225733</td>\n",
       "      <td>...</td>\n",
       "      <td>1.696831</td>\n",
       "      <td>9.480551</td>\n",
       "      <td>37.210995</td>\n",
       "      <td>44.904011</td>\n",
       "      <td>28.177481</td>\n",
       "      <td>-0.756711</td>\n",
       "      <td>30.258121</td>\n",
       "      <td>21.869564</td>\n",
       "      <td>14.042684</td>\n",
       "      <td>1.959185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>33.526688</td>\n",
       "      <td>12.869153</td>\n",
       "      <td>9.772631</td>\n",
       "      <td>11.083734</td>\n",
       "      <td>9.887362</td>\n",
       "      <td>19.560183</td>\n",
       "      <td>16.091736</td>\n",
       "      <td>13.224041</td>\n",
       "      <td>2.728399</td>\n",
       "      <td>15.248633</td>\n",
       "      <td>...</td>\n",
       "      <td>2.795797</td>\n",
       "      <td>11.983220</td>\n",
       "      <td>41.797325</td>\n",
       "      <td>47.848324</td>\n",
       "      <td>25.542673</td>\n",
       "      <td>1.072619</td>\n",
       "      <td>29.550541</td>\n",
       "      <td>21.579489</td>\n",
       "      <td>15.318009</td>\n",
       "      <td>1.946121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>31.970888</td>\n",
       "      <td>13.011966</td>\n",
       "      <td>9.247462</td>\n",
       "      <td>10.285872</td>\n",
       "      <td>9.401242</td>\n",
       "      <td>20.106699</td>\n",
       "      <td>19.179758</td>\n",
       "      <td>15.082566</td>\n",
       "      <td>2.721985</td>\n",
       "      <td>14.211651</td>\n",
       "      <td>...</td>\n",
       "      <td>1.799533</td>\n",
       "      <td>12.420786</td>\n",
       "      <td>39.353600</td>\n",
       "      <td>46.237518</td>\n",
       "      <td>25.738331</td>\n",
       "      <td>0.155704</td>\n",
       "      <td>28.975451</td>\n",
       "      <td>20.581802</td>\n",
       "      <td>14.136082</td>\n",
       "      <td>1.705923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>32.325157</td>\n",
       "      <td>12.451711</td>\n",
       "      <td>9.139846</td>\n",
       "      <td>10.009516</td>\n",
       "      <td>9.535349</td>\n",
       "      <td>20.341352</td>\n",
       "      <td>19.148308</td>\n",
       "      <td>15.327406</td>\n",
       "      <td>1.751927</td>\n",
       "      <td>13.437290</td>\n",
       "      <td>...</td>\n",
       "      <td>2.964903</td>\n",
       "      <td>14.600487</td>\n",
       "      <td>40.179024</td>\n",
       "      <td>46.841530</td>\n",
       "      <td>26.960379</td>\n",
       "      <td>0.430167</td>\n",
       "      <td>29.336597</td>\n",
       "      <td>21.550083</td>\n",
       "      <td>13.457083</td>\n",
       "      <td>1.090294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>30.623894</td>\n",
       "      <td>12.539824</td>\n",
       "      <td>9.593932</td>\n",
       "      <td>9.970858</td>\n",
       "      <td>8.685935</td>\n",
       "      <td>22.434816</td>\n",
       "      <td>21.263676</td>\n",
       "      <td>16.231230</td>\n",
       "      <td>2.680438</td>\n",
       "      <td>13.580133</td>\n",
       "      <td>...</td>\n",
       "      <td>2.054345</td>\n",
       "      <td>16.898252</td>\n",
       "      <td>40.560101</td>\n",
       "      <td>47.032612</td>\n",
       "      <td>28.708706</td>\n",
       "      <td>0.475269</td>\n",
       "      <td>31.200603</td>\n",
       "      <td>22.760303</td>\n",
       "      <td>14.115383</td>\n",
       "      <td>2.452861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>30.773043</td>\n",
       "      <td>13.157724</td>\n",
       "      <td>9.633651</td>\n",
       "      <td>10.548943</td>\n",
       "      <td>9.431029</td>\n",
       "      <td>24.269098</td>\n",
       "      <td>20.855356</td>\n",
       "      <td>15.087692</td>\n",
       "      <td>1.693338</td>\n",
       "      <td>12.098204</td>\n",
       "      <td>...</td>\n",
       "      <td>3.056683</td>\n",
       "      <td>15.900931</td>\n",
       "      <td>39.404030</td>\n",
       "      <td>47.063873</td>\n",
       "      <td>26.858881</td>\n",
       "      <td>0.024467</td>\n",
       "      <td>28.332769</td>\n",
       "      <td>22.877935</td>\n",
       "      <td>15.608363</td>\n",
       "      <td>3.012622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>30.978714</td>\n",
       "      <td>15.064196</td>\n",
       "      <td>9.222776</td>\n",
       "      <td>10.661768</td>\n",
       "      <td>9.667857</td>\n",
       "      <td>22.119602</td>\n",
       "      <td>20.069870</td>\n",
       "      <td>15.745667</td>\n",
       "      <td>2.612231</td>\n",
       "      <td>13.600842</td>\n",
       "      <td>...</td>\n",
       "      <td>3.736280</td>\n",
       "      <td>15.022478</td>\n",
       "      <td>38.354828</td>\n",
       "      <td>47.073761</td>\n",
       "      <td>27.383076</td>\n",
       "      <td>0.387850</td>\n",
       "      <td>30.246973</td>\n",
       "      <td>23.784124</td>\n",
       "      <td>15.426615</td>\n",
       "      <td>1.870222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>32.780434</td>\n",
       "      <td>14.593522</td>\n",
       "      <td>9.262407</td>\n",
       "      <td>10.485851</td>\n",
       "      <td>9.709379</td>\n",
       "      <td>22.462202</td>\n",
       "      <td>19.657450</td>\n",
       "      <td>14.474680</td>\n",
       "      <td>2.143900</td>\n",
       "      <td>15.126963</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079294</td>\n",
       "      <td>14.556431</td>\n",
       "      <td>37.954224</td>\n",
       "      <td>46.055656</td>\n",
       "      <td>26.410009</td>\n",
       "      <td>0.311903</td>\n",
       "      <td>28.816292</td>\n",
       "      <td>23.392115</td>\n",
       "      <td>15.776675</td>\n",
       "      <td>2.331289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>32.263363</td>\n",
       "      <td>14.122775</td>\n",
       "      <td>9.249658</td>\n",
       "      <td>10.515839</td>\n",
       "      <td>10.015399</td>\n",
       "      <td>22.208239</td>\n",
       "      <td>19.373695</td>\n",
       "      <td>13.464707</td>\n",
       "      <td>1.457483</td>\n",
       "      <td>14.428403</td>\n",
       "      <td>...</td>\n",
       "      <td>1.924814</td>\n",
       "      <td>14.363400</td>\n",
       "      <td>38.832836</td>\n",
       "      <td>46.645317</td>\n",
       "      <td>26.336697</td>\n",
       "      <td>0.201146</td>\n",
       "      <td>29.587067</td>\n",
       "      <td>20.978891</td>\n",
       "      <td>14.774311</td>\n",
       "      <td>3.225650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>32.078011</td>\n",
       "      <td>13.444307</td>\n",
       "      <td>9.483199</td>\n",
       "      <td>9.139258</td>\n",
       "      <td>9.284956</td>\n",
       "      <td>21.028717</td>\n",
       "      <td>19.762562</td>\n",
       "      <td>13.862543</td>\n",
       "      <td>0.465396</td>\n",
       "      <td>12.965622</td>\n",
       "      <td>...</td>\n",
       "      <td>2.471443</td>\n",
       "      <td>13.424318</td>\n",
       "      <td>39.487000</td>\n",
       "      <td>46.389984</td>\n",
       "      <td>26.811930</td>\n",
       "      <td>0.891309</td>\n",
       "      <td>30.762672</td>\n",
       "      <td>19.241795</td>\n",
       "      <td>14.613530</td>\n",
       "      <td>1.390069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>31.553240</td>\n",
       "      <td>13.257810</td>\n",
       "      <td>9.593613</td>\n",
       "      <td>9.749351</td>\n",
       "      <td>9.342597</td>\n",
       "      <td>19.660860</td>\n",
       "      <td>19.104200</td>\n",
       "      <td>14.253172</td>\n",
       "      <td>1.169499</td>\n",
       "      <td>12.323843</td>\n",
       "      <td>...</td>\n",
       "      <td>1.020557</td>\n",
       "      <td>13.405181</td>\n",
       "      <td>38.203850</td>\n",
       "      <td>46.120987</td>\n",
       "      <td>26.111027</td>\n",
       "      <td>0.863032</td>\n",
       "      <td>29.380375</td>\n",
       "      <td>19.346830</td>\n",
       "      <td>13.431325</td>\n",
       "      <td>0.955951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>30.925386</td>\n",
       "      <td>12.881517</td>\n",
       "      <td>8.998224</td>\n",
       "      <td>9.893764</td>\n",
       "      <td>8.776958</td>\n",
       "      <td>20.475124</td>\n",
       "      <td>19.384977</td>\n",
       "      <td>14.534433</td>\n",
       "      <td>1.011268</td>\n",
       "      <td>9.754497</td>\n",
       "      <td>...</td>\n",
       "      <td>1.914099</td>\n",
       "      <td>12.559636</td>\n",
       "      <td>35.769867</td>\n",
       "      <td>45.963245</td>\n",
       "      <td>25.180656</td>\n",
       "      <td>0.782353</td>\n",
       "      <td>28.548677</td>\n",
       "      <td>18.736496</td>\n",
       "      <td>13.379887</td>\n",
       "      <td>1.733288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>31.444721</td>\n",
       "      <td>12.127511</td>\n",
       "      <td>9.400603</td>\n",
       "      <td>9.740578</td>\n",
       "      <td>8.897140</td>\n",
       "      <td>19.338469</td>\n",
       "      <td>18.350080</td>\n",
       "      <td>13.394939</td>\n",
       "      <td>0.700631</td>\n",
       "      <td>8.946136</td>\n",
       "      <td>...</td>\n",
       "      <td>1.977169</td>\n",
       "      <td>13.102896</td>\n",
       "      <td>37.951683</td>\n",
       "      <td>47.787956</td>\n",
       "      <td>24.923552</td>\n",
       "      <td>1.572904</td>\n",
       "      <td>28.751392</td>\n",
       "      <td>17.604280</td>\n",
       "      <td>13.367682</td>\n",
       "      <td>1.876760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>32.734116</td>\n",
       "      <td>12.606186</td>\n",
       "      <td>8.957918</td>\n",
       "      <td>10.411437</td>\n",
       "      <td>8.907505</td>\n",
       "      <td>18.092764</td>\n",
       "      <td>16.683395</td>\n",
       "      <td>13.485654</td>\n",
       "      <td>0.687157</td>\n",
       "      <td>10.016682</td>\n",
       "      <td>...</td>\n",
       "      <td>1.669878</td>\n",
       "      <td>11.490157</td>\n",
       "      <td>37.381439</td>\n",
       "      <td>44.825344</td>\n",
       "      <td>26.101645</td>\n",
       "      <td>0.294542</td>\n",
       "      <td>30.104986</td>\n",
       "      <td>18.590263</td>\n",
       "      <td>14.979117</td>\n",
       "      <td>1.093544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>31.576834</td>\n",
       "      <td>12.524677</td>\n",
       "      <td>9.471484</td>\n",
       "      <td>9.504980</td>\n",
       "      <td>8.711183</td>\n",
       "      <td>18.825649</td>\n",
       "      <td>16.101667</td>\n",
       "      <td>11.679004</td>\n",
       "      <td>0.390361</td>\n",
       "      <td>8.565460</td>\n",
       "      <td>...</td>\n",
       "      <td>1.883833</td>\n",
       "      <td>12.649892</td>\n",
       "      <td>38.860641</td>\n",
       "      <td>46.500744</td>\n",
       "      <td>25.780609</td>\n",
       "      <td>0.492594</td>\n",
       "      <td>28.784414</td>\n",
       "      <td>16.819815</td>\n",
       "      <td>15.362391</td>\n",
       "      <td>1.674452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>33.067032</td>\n",
       "      <td>13.175719</td>\n",
       "      <td>9.502821</td>\n",
       "      <td>9.706896</td>\n",
       "      <td>8.423117</td>\n",
       "      <td>17.444088</td>\n",
       "      <td>15.150140</td>\n",
       "      <td>11.023738</td>\n",
       "      <td>0.563673</td>\n",
       "      <td>8.948584</td>\n",
       "      <td>...</td>\n",
       "      <td>1.371333</td>\n",
       "      <td>11.461452</td>\n",
       "      <td>38.067421</td>\n",
       "      <td>44.595955</td>\n",
       "      <td>26.769156</td>\n",
       "      <td>0.723194</td>\n",
       "      <td>29.461811</td>\n",
       "      <td>15.864216</td>\n",
       "      <td>14.248510</td>\n",
       "      <td>1.167403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>591 rows  156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5    \\\n",
       "0    31.781780  13.543207   9.240832  10.412905   9.884580  22.752977   \n",
       "1    32.424919  12.902494   9.220963  10.195102  10.080056  22.279537   \n",
       "2    32.481411  12.803867  10.173083  10.076785   9.152813  23.610252   \n",
       "3    30.822714  13.202147   9.865823  10.210397   9.217444  23.386333   \n",
       "4    31.453362  13.277498   9.639323  10.370049   9.584814  22.759216   \n",
       "5    31.288458  14.950979   9.898715  11.001036   9.904197  24.030148   \n",
       "6    33.057083  15.376768   9.884590  10.598233   9.265668  24.438787   \n",
       "7    32.050316  15.731661   9.605585  10.558293   9.045577  23.455345   \n",
       "8    32.575962  15.821782   9.415033  10.422915   9.917982  24.006132   \n",
       "9    32.088402  15.679485  10.068712  10.327548   9.688314  23.674112   \n",
       "10   32.142452  16.328430  10.522639  10.626881   9.315237  23.740805   \n",
       "11   31.705427  14.819589   9.852552  10.198965   9.176675  21.909031   \n",
       "12   31.654108  13.674428   9.995523  10.510631   8.928997  21.175322   \n",
       "13   33.681114  14.566197   9.647560  10.599469   9.014348  22.363663   \n",
       "14   34.314930  14.067004  10.021678  10.661773   9.528600  23.235136   \n",
       "15   33.594650  14.891633   9.892827  10.524937   9.475529  23.522650   \n",
       "16   35.615860  13.534027   9.483946   9.641203   9.010510  24.175474   \n",
       "17   34.037189  13.954793  10.273467  10.399820   9.250853  23.128880   \n",
       "18   34.399284  14.076210  10.188951  11.111928   8.540196  24.196941   \n",
       "19   29.381689  14.556379  10.094573  10.538021   8.235656  22.715500   \n",
       "20   31.967962  14.143500  10.183433   9.745589   7.825250  21.418728   \n",
       "21   31.060810  14.566368  10.130197   9.597451   7.892380  19.765875   \n",
       "22   31.950653  13.695735  10.442800  10.741561   8.712795  21.456635   \n",
       "23   31.506828  13.918685   9.688413   9.960030   8.464375  21.176882   \n",
       "24   31.712078  14.266314   9.369452   9.006596   9.066339  18.956118   \n",
       "25   32.493500  13.225560   9.901771   9.351921   8.735823  18.912119   \n",
       "26   32.162338  12.451381  11.095897  10.212534   8.618316  19.959425   \n",
       "27   31.139938  12.914868  10.437339  10.958607   8.555090  18.644644   \n",
       "28   29.866302  12.102680   9.831889  11.238369   9.074184  16.860491   \n",
       "29   31.818592  11.842531   9.791294  11.004981   9.659589  17.020319   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "561  31.756578  11.284259   9.828773  11.455976   8.492395  20.421398   \n",
       "562  33.197460  11.923230   9.923149  12.374813   8.624905  22.251518   \n",
       "563  35.037788  11.624953  10.480066  12.223717   9.110847  22.082541   \n",
       "564  33.766441  11.471992  10.524483  12.387187   9.261294  22.309158   \n",
       "565  31.290787  13.494864  10.588075  12.727268   8.812082  19.997614   \n",
       "566  30.350170  14.057221  10.719115  12.905441   9.577024  16.816950   \n",
       "567  29.930832  12.973221  10.475307  12.093650   9.057755  16.721470   \n",
       "568  31.361580  13.910000   9.983809  11.355268   9.579843  19.488451   \n",
       "569  30.906921  15.133026   9.978611  11.097390   9.880603  21.212130   \n",
       "570  31.291489  14.617837  10.136744  10.891616   9.386535  21.201496   \n",
       "571  32.045670  14.187469   9.851630  11.752486   9.770060  20.743212   \n",
       "572  33.078056  13.012218  10.194946  11.354506   9.722691  21.546236   \n",
       "573  30.997034  13.994182  10.045185  11.523880   9.976860  21.860435   \n",
       "574  33.210648  13.470467  10.229843  11.769641  10.337141  21.010384   \n",
       "575  32.761063  13.826317   9.784741  12.132776   9.758242  20.907465   \n",
       "576  33.526688  12.869153   9.772631  11.083734   9.887362  19.560183   \n",
       "577  31.970888  13.011966   9.247462  10.285872   9.401242  20.106699   \n",
       "578  32.325157  12.451711   9.139846  10.009516   9.535349  20.341352   \n",
       "579  30.623894  12.539824   9.593932   9.970858   8.685935  22.434816   \n",
       "580  30.773043  13.157724   9.633651  10.548943   9.431029  24.269098   \n",
       "581  30.978714  15.064196   9.222776  10.661768   9.667857  22.119602   \n",
       "582  32.780434  14.593522   9.262407  10.485851   9.709379  22.462202   \n",
       "583  32.263363  14.122775   9.249658  10.515839  10.015399  22.208239   \n",
       "584  32.078011  13.444307   9.483199   9.139258   9.284956  21.028717   \n",
       "585  31.553240  13.257810   9.593613   9.749351   9.342597  19.660860   \n",
       "586  30.925386  12.881517   8.998224   9.893764   8.776958  20.475124   \n",
       "587  31.444721  12.127511   9.400603   9.740578   8.897140  19.338469   \n",
       "588  32.734116  12.606186   8.957918  10.411437   8.907505  18.092764   \n",
       "589  31.576834  12.524677   9.471484   9.504980   8.711183  18.825649   \n",
       "590  33.067032  13.175719   9.502821   9.706896   8.423117  17.444088   \n",
       "\n",
       "           6          7         8          9    ...       146        147  \\\n",
       "0    19.330700  13.266711  2.156584  12.188547  ...  2.532453  14.238125   \n",
       "1    18.414099  13.781658  1.430684  12.345279  ...  2.574634  13.570778   \n",
       "2    20.509722  13.846017  2.063973  13.358719  ...  2.760864  14.444487   \n",
       "3    20.641256  13.607705  1.865169  13.013542  ...  2.748382  14.293195   \n",
       "4    20.029161  14.898588  1.978947  13.488507  ...  2.650280  13.781360   \n",
       "5    19.955633  13.809528  1.205771  13.184121  ...  3.348103  14.668760   \n",
       "6    21.173065  15.237820  2.307499  12.956710  ...  2.588054  14.594851   \n",
       "7    21.410032  15.941216  2.256328  14.698202  ...  2.571507  14.472841   \n",
       "8    21.368078  14.883246  0.932176  11.598557  ...  2.162590  16.434641   \n",
       "9    21.473042  15.185500  1.448936  14.227770  ...  2.152926  13.750476   \n",
       "10   21.379902  15.602315  1.497485  12.352130  ...  2.459481  16.536421   \n",
       "11   20.233288  14.621185  1.225188  11.086762  ...  1.800733  14.776584   \n",
       "12   20.019487  14.790354  1.285487  10.622425  ...  2.180281  15.341624   \n",
       "13   20.482346  14.730296  1.312373  10.051073  ...  1.719987  17.660181   \n",
       "14   19.654718  15.621077  1.370236   8.938208  ...  1.472469  16.354595   \n",
       "15   19.507353  13.438053  1.019480   9.601454  ...  1.302400  16.315819   \n",
       "16   20.551794  12.490316  0.718726  10.360333  ...  1.393325  16.920496   \n",
       "17   20.997887  14.168969  0.663356  11.970490  ...  1.144256  17.546192   \n",
       "18   21.505682  16.076384  1.735421  14.621701  ...  2.037139  16.658972   \n",
       "19   20.756931  16.333366  1.807651  15.541000  ...  2.305068  15.354663   \n",
       "20   19.374247  14.935576  0.675614  13.512066  ...  3.563187  14.845846   \n",
       "21   19.067314  13.548018  0.055717  12.015326  ...  2.893146  16.464586   \n",
       "22   18.664627  12.981586 -0.125593  10.257301  ...  2.748253  15.396751   \n",
       "23   18.386055  11.987475  0.293519  10.096902  ...  2.535312  15.294781   \n",
       "24   16.020433  10.536100  0.024514   8.991507  ...  2.046592  13.399684   \n",
       "25   16.792238  11.548710  1.016744  11.751858  ...  1.826104  13.377995   \n",
       "26   18.056461  13.265006  1.435331  11.968399  ...  2.191944  13.306806   \n",
       "27   15.969020  13.067201  1.424389  11.677191  ...  1.568046  13.117223   \n",
       "28   15.138467  12.414568  0.628150  11.953717  ...  1.653495  12.276567   \n",
       "29   16.117577  11.883063 -0.127813   9.859614  ...  1.221839  12.903056   \n",
       "..         ...        ...       ...        ...  ...       ...        ...   \n",
       "561  18.467779  13.334274  1.144226  10.440152  ...  2.145034  13.378692   \n",
       "562  20.470642  13.699542  2.223652   9.930403  ...  1.806333  13.250093   \n",
       "563  19.785759  14.763874  1.784557  10.636200  ...  2.052290  13.644827   \n",
       "564  20.277967  14.432493  1.422931  10.389841  ...  1.267003  15.087630   \n",
       "565  17.140762  12.718408 -0.363735   8.108701  ...  0.588895  12.374127   \n",
       "566  15.045238  11.115370 -0.902683   7.519416  ...  0.287515  11.698462   \n",
       "567  17.159298  12.562021  0.444991   8.062181  ...  0.647252  12.807989   \n",
       "568  17.582531  12.956504  1.554539   9.934797  ...  1.034852  12.388017   \n",
       "569  20.057810  14.724084  1.931140  11.545140  ...  0.831664  14.285478   \n",
       "570  19.692722  14.204758  1.011114   9.907206  ...  1.113739  14.174333   \n",
       "571  18.915503  14.246563  1.801933  11.740432  ...  1.413154  12.928036   \n",
       "572  19.225107  13.782493  2.274910  11.372928  ...  1.135272  12.326888   \n",
       "573  17.039812  13.663836  3.481441  14.624957  ...  2.333932   9.822688   \n",
       "574  17.969028  15.668698  4.019704  16.031132  ...  2.121125  11.293754   \n",
       "575  17.814779  15.225538  3.106329  17.225733  ...  1.696831   9.480551   \n",
       "576  16.091736  13.224041  2.728399  15.248633  ...  2.795797  11.983220   \n",
       "577  19.179758  15.082566  2.721985  14.211651  ...  1.799533  12.420786   \n",
       "578  19.148308  15.327406  1.751927  13.437290  ...  2.964903  14.600487   \n",
       "579  21.263676  16.231230  2.680438  13.580133  ...  2.054345  16.898252   \n",
       "580  20.855356  15.087692  1.693338  12.098204  ...  3.056683  15.900931   \n",
       "581  20.069870  15.745667  2.612231  13.600842  ...  3.736280  15.022478   \n",
       "582  19.657450  14.474680  2.143900  15.126963  ...  2.079294  14.556431   \n",
       "583  19.373695  13.464707  1.457483  14.428403  ...  1.924814  14.363400   \n",
       "584  19.762562  13.862543  0.465396  12.965622  ...  2.471443  13.424318   \n",
       "585  19.104200  14.253172  1.169499  12.323843  ...  1.020557  13.405181   \n",
       "586  19.384977  14.534433  1.011268   9.754497  ...  1.914099  12.559636   \n",
       "587  18.350080  13.394939  0.700631   8.946136  ...  1.977169  13.102896   \n",
       "588  16.683395  13.485654  0.687157  10.016682  ...  1.669878  11.490157   \n",
       "589  16.101667  11.679004  0.390361   8.565460  ...  1.883833  12.649892   \n",
       "590  15.150140  11.023738  0.563673   8.948584  ...  1.371333  11.461452   \n",
       "\n",
       "           148        149        150       151        152        153  \\\n",
       "0    39.016079  45.888344  26.260866 -0.624887  29.851168  17.570044   \n",
       "1    38.510880  45.941608  27.839052 -0.053028  30.581821  17.779610   \n",
       "2    39.379204  45.416634  26.891563 -0.303168  30.511915  17.774212   \n",
       "3    38.329445  45.913223  25.951399  0.136386  28.585686  17.366350   \n",
       "4    37.322933  45.921909  24.889179  0.163643  27.572567  19.249397   \n",
       "5    38.568119  45.894268  25.916553  0.499026  28.156574  19.832701   \n",
       "6    37.796043  44.322704  27.244667 -0.423854  29.893215  21.061317   \n",
       "7    37.708397  44.082932  28.286137  0.078791  32.442043  23.391960   \n",
       "8    39.290337  45.825577  25.466949  0.766499  29.971420  21.363508   \n",
       "9    38.427174  44.594986  26.771948 -0.286848  29.790285  21.802341   \n",
       "10   40.480095  45.965164  26.774982  0.115086  29.890087  21.311323   \n",
       "11   37.498360  45.133099  26.510418  0.306967  28.946545  19.771706   \n",
       "12   37.137039  44.377518  26.134094  0.179677  28.635550  20.946009   \n",
       "13   40.053410  46.023632  26.839706  0.932042  29.924484  20.535357   \n",
       "14   43.642738  46.681873  27.538479  0.483097  30.233673  19.460865   \n",
       "15   41.787819  46.293606  27.231325 -0.319246  29.524923  18.576797   \n",
       "16   42.040787  46.269127  26.936609 -0.105708  29.881310  17.727764   \n",
       "17   41.322636  45.069851  25.560261 -0.289645  29.106857  19.593925   \n",
       "18   43.606613  43.793587  28.035353 -0.232530  29.316629  23.281504   \n",
       "19   41.044361  42.400471  25.056564 -0.628642  25.033150  23.866545   \n",
       "20   38.517864  41.692924  26.737139 -0.467939  28.137520  22.658648   \n",
       "21   40.258747  43.115482  24.223961  0.217327  27.783529  19.896894   \n",
       "22   41.962952  44.298378  26.758188  0.409623  28.686369  19.329536   \n",
       "23   41.445568  44.094109  27.801306  0.058164  30.304033  18.604404   \n",
       "24   39.816753  44.169960  29.887001 -0.439409  32.039719  18.192736   \n",
       "25   40.585152  44.770378  28.452915 -0.815199  31.857786  19.923029   \n",
       "26   39.731674  43.895599  24.030102 -0.282466  27.672522  20.704298   \n",
       "27   38.510696  42.606102  19.630770 -0.230018  23.275143  20.917747   \n",
       "28   36.639637  39.444633  28.802435  1.536694  29.831448  23.005117   \n",
       "29   36.750175  38.671898  27.657759  2.006209  29.255573  19.764988   \n",
       "..         ...        ...        ...       ...        ...        ...   \n",
       "561  37.368427  44.584023  24.051682 -1.690115  26.545675  13.484314   \n",
       "562  39.237637  45.777420  24.325211 -1.255652  28.530478  12.521112   \n",
       "563  39.087391  45.065533  24.665070 -0.998992  28.254641  14.139111   \n",
       "564  42.071960  47.747601  24.036304 -0.935661  28.359564  13.434323   \n",
       "565  39.636909  46.892666  24.771513 -1.194415  28.522533  13.566863   \n",
       "566  36.238323  44.518650  23.726822 -0.315877  28.383543  12.508492   \n",
       "567  35.109875  45.465427  23.383907 -0.373018  29.016657  13.467850   \n",
       "568  35.683071  44.678391  21.511875  0.328223  26.996960  13.947426   \n",
       "569  36.245979  43.707924  23.404839  0.393617  27.341244  17.067446   \n",
       "570  35.919411  43.732407  24.376257 -1.171392  27.414480  17.084208   \n",
       "571  36.510242  43.811203  25.751183 -0.438147  28.001728  18.776787   \n",
       "572  36.517841  44.706303  24.715025 -0.100938  26.533508  17.616098   \n",
       "573  35.962822  42.911034  26.520649 -0.869123  29.136118  19.402121   \n",
       "574  38.535179  45.702217  27.377615  0.201934  28.668287  20.327820   \n",
       "575  37.210995  44.904011  28.177481 -0.756711  30.258121  21.869564   \n",
       "576  41.797325  47.848324  25.542673  1.072619  29.550541  21.579489   \n",
       "577  39.353600  46.237518  25.738331  0.155704  28.975451  20.581802   \n",
       "578  40.179024  46.841530  26.960379  0.430167  29.336597  21.550083   \n",
       "579  40.560101  47.032612  28.708706  0.475269  31.200603  22.760303   \n",
       "580  39.404030  47.063873  26.858881  0.024467  28.332769  22.877935   \n",
       "581  38.354828  47.073761  27.383076  0.387850  30.246973  23.784124   \n",
       "582  37.954224  46.055656  26.410009  0.311903  28.816292  23.392115   \n",
       "583  38.832836  46.645317  26.336697  0.201146  29.587067  20.978891   \n",
       "584  39.487000  46.389984  26.811930  0.891309  30.762672  19.241795   \n",
       "585  38.203850  46.120987  26.111027  0.863032  29.380375  19.346830   \n",
       "586  35.769867  45.963245  25.180656  0.782353  28.548677  18.736496   \n",
       "587  37.951683  47.787956  24.923552  1.572904  28.751392  17.604280   \n",
       "588  37.381439  44.825344  26.101645  0.294542  30.104986  18.590263   \n",
       "589  38.860641  46.500744  25.780609  0.492594  28.784414  16.819815   \n",
       "590  38.067421  44.595955  26.769156  0.723194  29.461811  15.864216   \n",
       "\n",
       "           154       155  \n",
       "0    12.993816  1.143283  \n",
       "1    13.447145  1.213957  \n",
       "2    13.883696  2.170426  \n",
       "3    13.199390  2.314845  \n",
       "4    13.762200  2.318848  \n",
       "5    13.869998  2.995385  \n",
       "6    13.713863  2.261284  \n",
       "7    14.989348  2.487361  \n",
       "8    14.194089  2.463561  \n",
       "9    14.719844  1.133119  \n",
       "10   14.317273  1.658492  \n",
       "11   13.997062  2.148985  \n",
       "12   14.778438  2.056277  \n",
       "13   14.725108  1.947110  \n",
       "14   15.858761  0.625085  \n",
       "15   15.613200  1.031303  \n",
       "16   16.989269  2.418840  \n",
       "17   15.682839  2.911563  \n",
       "18   14.009090  2.334761  \n",
       "19   13.812799  2.746749  \n",
       "20   13.226570  1.971209  \n",
       "21   12.726367  1.823863  \n",
       "22   16.481907  2.044789  \n",
       "23   15.763044  2.766160  \n",
       "24   14.330987  1.724685  \n",
       "25   12.158446  2.076889  \n",
       "26   11.427772  2.593999  \n",
       "27   10.967828  2.557552  \n",
       "28   13.511803  2.526359  \n",
       "29   12.416608  1.794474  \n",
       "..         ...       ...  \n",
       "561  10.874492  0.477598  \n",
       "562  11.595049  0.883566  \n",
       "563  12.435262  1.021180  \n",
       "564  12.019987  0.304618  \n",
       "565  10.560756 -0.283772  \n",
       "566  11.408809 -0.058635  \n",
       "567  11.133261  1.173765  \n",
       "568  11.294073  1.796539  \n",
       "569  13.640881  1.941948  \n",
       "570  13.385888  1.459204  \n",
       "571  13.790354  0.950727  \n",
       "572  14.884014  1.766315  \n",
       "573  15.398647  2.388580  \n",
       "574  14.755957  1.943143  \n",
       "575  14.042684  1.959185  \n",
       "576  15.318009  1.946121  \n",
       "577  14.136082  1.705923  \n",
       "578  13.457083  1.090294  \n",
       "579  14.115383  2.452861  \n",
       "580  15.608363  3.012622  \n",
       "581  15.426615  1.870222  \n",
       "582  15.776675  2.331289  \n",
       "583  14.774311  3.225650  \n",
       "584  14.613530  1.390069  \n",
       "585  13.431325  0.955951  \n",
       "586  13.379887  1.733288  \n",
       "587  13.367682  1.876760  \n",
       "588  14.979117  1.093544  \n",
       "589  15.362391  1.674452  \n",
       "590  14.248510  1.167403  \n",
       "\n",
       "[591 rows x 156 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_result(test_result,test_label1,path):\n",
    "    ##all test result visualization\n",
    "    fig1 = plt.figure(figsize=(7,1.5))\n",
    "    ax1 = fig1.add_subplot(1,1,1)\n",
    "    a_pred = test_result[:,0]\n",
    "    a_true = test_label1[:,0]\n",
    "    plt.plot(a_pred,'r-',label='prediction')\n",
    "    plt.plot(a_true,'b-',label='true')\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_all.jpg')\n",
    "    plt.show()\n",
    "    ## oneday test result visualization\n",
    "    fig1 = plt.figure(figsize=(7,1.5))\n",
    "    ax1 = fig1.add_subplot(1,1,1)\n",
    "    a_pred = test_result[0:96,0]\n",
    "    a_true = test_label1[0:96,0]\n",
    "    plt.plot(a_pred,'r-',label=\"prediction\")\n",
    "    plt.plot(a_true,'b-',label=\"true\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_oneday.jpg')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path):\n",
    "    ###train_rmse & test_rmse \n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(train_rmse, 'r-', label=\"train_rmse\")\n",
    "    plt.plot(test_rmse, 'b-', label=\"test_rmse\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/rmse.jpg')\n",
    "    plt.show()\n",
    "    #### train_loss & train_rmse\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(train_loss,'b-', label='train_loss')\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/train_loss.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(train_rmse,'b-', label='train_rmse')\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/train_rmse.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    ### accuracy\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(test_acc, 'b-', label=\"test_acc\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_acc.jpg')\n",
    "    plt.show()\n",
    "    ### rmse\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(test_rmse, 'b-', label=\"test_rmse\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_rmse.jpg')\n",
    "    plt.show()\n",
    "    ### mae\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(test_mae, 'b-', label=\"test_mae\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_mae.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x108 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x108 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result(test_result,test_label1,path)\n",
    "#plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
