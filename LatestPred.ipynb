{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import numpy.linalg as la\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "#import matplotlib.pyplot as plt\n",
    "import time\n",
    "time_start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of Dataset-SZ Traffic\n",
    "def load_sz_data():\n",
    "    sz_adj = pd.read_csv('sz_adj.csv',header=None)\n",
    "    adj = np.mat(sz_adj)\n",
    "    sz_tf = pd.read_csv('sz_speed.csv')\n",
    "    return sz_tf, adj\n",
    "\n",
    "data, adj = load_sz_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          90217      90218      90219      90220      90221      90222  \\\n",
      "0      8.471971  18.455410  20.590635  15.345258   9.585218  21.501821   \n",
      "1      7.807137  15.713816  27.523695  11.087895   9.455280  17.332246   \n",
      "2      8.809457   8.979647  20.280394  16.523419   8.003314  15.789483   \n",
      "3     51.590372  23.631243  20.224094  15.116459   6.642644  17.575806   \n",
      "4     58.770433  20.437740  20.465606  14.820217  11.344404   0.000000   \n",
      "5     58.289126  10.332738  25.331018  18.485616   2.028513  10.718488   \n",
      "6     61.334449  20.818480  15.839392   8.768217  13.933326  13.866124   \n",
      "7     58.903144   8.291826  44.043729  15.250859   0.000000   0.000000   \n",
      "8     57.210441  25.765667  18.677565   5.066051   0.000000  12.645903   \n",
      "9     59.559265  16.803983  20.314610  23.393083   2.612821   0.000000   \n",
      "10    60.217867  26.773576  17.684078  17.735375  12.957171   7.404007   \n",
      "11    56.336405  18.866552  27.502295  18.201105  23.973238  22.215358   \n",
      "12    58.511395  15.802270  15.900551  15.825524  28.359067  13.519283   \n",
      "13    59.327670  28.104659  23.004938  17.177101   5.770516   0.000000   \n",
      "14    59.415925  20.049136  24.239933  16.541053  18.319476  30.199940   \n",
      "15    57.000535  25.916699  23.620492  15.149346  18.415374  10.608687   \n",
      "16    59.468388  23.153764  22.637581  21.761991   7.845323  14.191932   \n",
      "17    57.489538  29.269876  21.906870  15.336707  23.427912  22.823073   \n",
      "18    60.289596  24.220646  18.846753  20.107602  17.853574   0.000000   \n",
      "19    58.943452  28.973177  23.110562  17.220674  21.988291  42.517631   \n",
      "20    59.584971  28.455832  19.782971  18.319035  20.363029   9.263639   \n",
      "21    61.006783  20.343802  23.375934  14.071018  11.776268  14.986706   \n",
      "22    61.391417  16.989282  17.488080  16.687639  18.837553  12.817948   \n",
      "23    62.847104  17.922808  20.711342  12.684088  14.431455  37.580218   \n",
      "24    59.712049  22.491029  20.029159  19.459853  10.929650  12.636402   \n",
      "25    60.128278  11.853744  21.438214  16.230672  18.646959  33.716293   \n",
      "26    57.230341  21.015119  20.436854  18.399933  10.754621  28.184720   \n",
      "27    61.317819  18.813310  22.462014  18.800135  19.252080  31.657746   \n",
      "28    57.949689  23.351677  24.078893  15.910953  19.144494  30.628915   \n",
      "29    60.440370  19.183227  22.276537  18.166327  20.971482   9.271435   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "2946  37.047736  11.747849   9.158301  12.016694  10.121105  16.834794   \n",
      "2947  32.587985  11.186664  10.718418  12.244456  11.516860  22.302047   \n",
      "2948  28.728285  14.481847   9.443261   9.845491   6.633443   5.380683   \n",
      "2949  27.710759  15.547506  10.484638  12.722840  10.626220  10.623221   \n",
      "2950  26.003952  15.903029   8.968620  10.328466   9.006967  27.212662   \n",
      "2951  30.641217  13.588164   9.242868  11.115546   8.813212  22.479598   \n",
      "2952  29.589988  13.610331   9.388144   8.920857   9.951734  25.764707   \n",
      "2953  32.527293  11.195964   9.614881  11.488864   9.484497  26.042143   \n",
      "2954  32.717067  11.197344   9.038040  10.987866   8.974821  12.656677   \n",
      "2955  32.833424  11.759863   7.935098  11.153611   8.666250  26.168973   \n",
      "2956  30.866704  13.977036   9.085705  10.476089  10.399103  13.961974   \n",
      "2957  36.407212  13.445718   8.871406  12.298801   9.390320  13.086711   \n",
      "2958  29.498956  15.263794   9.542898  13.793024  11.678501  10.690330   \n",
      "2959  34.159054   8.463801   9.084855  10.914051   8.824929  10.646655   \n",
      "2960  29.827767  10.139989   8.011831   9.475263   8.314195  34.699582   \n",
      "2961  34.712911  13.224499   7.302345   9.891491   9.414917  22.504949   \n",
      "2962  29.886521  13.882277   8.170605   9.800999   8.409821  22.904986   \n",
      "2963  31.016664  10.664699   6.565841  10.013826  10.228492  28.740343   \n",
      "2964  31.287008  13.376588   6.999327   9.330356  10.903492  22.816266   \n",
      "2965  32.527834  11.507783   7.120969   9.997413  10.916735  14.875885   \n",
      "2966  31.001262  10.235263   7.584673  11.063059   9.750946  18.449771   \n",
      "2967  28.383740  11.333047   8.815655   9.994421   6.719870  17.476265   \n",
      "2968  31.422072  11.200472   8.240685  12.568828   8.239171  18.232270   \n",
      "2969  30.696860   9.195974   7.187724  10.021121   8.282266  14.751376   \n",
      "2970  29.795250   9.772127   6.848839   9.454226   7.350518  15.062446   \n",
      "2971  34.595076  11.353815   8.251759  10.507924   8.129874  17.123473   \n",
      "2972  28.697368  13.005691   8.131853  10.418131   8.087082  10.528642   \n",
      "2973  32.797527  12.241658   9.035159   8.513272   9.346719   6.296222   \n",
      "2974  34.331867  16.075872  10.543411  11.254747   9.930924  26.989660   \n",
      "2975  32.930700  12.101170   9.006258  11.971602   9.063041   3.209035   \n",
      "\n",
      "          90223      90224      90225      90226  ...     112746     116097  \\\n",
      "0     31.611759   0.000000   0.000000  22.008941  ...   0.000000  27.623514   \n",
      "1     33.976531  12.968823   0.000000  17.921757  ...   4.193466  14.889713   \n",
      "2     13.747267  12.221143   0.000000  21.588447  ...  11.076185   0.000000   \n",
      "3     17.657556  15.998745   0.000000  24.228905  ...   0.000000  34.132564   \n",
      "4     33.878502   5.676248   0.000000  26.250064  ...   0.000000  15.904281   \n",
      "5     10.439684   0.000000   2.122070  20.836691  ...  10.167713  15.566301   \n",
      "6     31.703332   4.823769   5.320506  29.906599  ...   0.000000  46.163459   \n",
      "7     28.502493   8.702932  15.535269  21.537570  ...   0.000000  29.304136   \n",
      "8     41.644261   0.000000   0.000000  36.559919  ...   5.753177  14.196596   \n",
      "9     43.230732  17.152937   5.947999  36.524772  ...   0.000000  21.855200   \n",
      "10    34.944085  20.617665   0.000000  31.518823  ...   0.000000  15.034488   \n",
      "11    36.325173   0.000000   0.000000  36.831651  ...   8.816271   4.736363   \n",
      "12    26.471023   0.000000   0.000000  39.778615  ...   0.000000  14.252345   \n",
      "13    42.172147  10.559143   7.318216  19.045834  ...   1.536469  19.284731   \n",
      "14    32.131190   8.375252   0.000000  25.769071  ...   7.341624  14.752431   \n",
      "15    51.908341   8.897444   3.245168  36.120403  ...   0.000000  38.125425   \n",
      "16    29.510518  16.612282   0.000000  24.700598  ...   2.491167  14.258565   \n",
      "17    36.651121   7.383970   0.000000  44.773479  ...   2.860599  26.084234   \n",
      "18    27.925195  24.273771   0.000000  23.039533  ...   0.725449   0.000000   \n",
      "19    27.891778   0.000000   0.000000  34.002083  ...   6.751413  11.150303   \n",
      "20    17.395813   0.000000   0.000000  11.116550  ...   2.888578  23.275499   \n",
      "21    22.301599  19.320293   0.000000  27.543710  ...   0.000000  24.007239   \n",
      "22    21.587101  17.851547   0.000000  21.746151  ...  12.737200  27.440866   \n",
      "23    23.709984  14.883667   0.000000  33.107521  ...   3.938674  36.426266   \n",
      "24    29.708086  22.742133   0.000000  37.725845  ...   0.000000  22.046767   \n",
      "25    30.235825   4.255047   0.000000  26.690446  ...   0.000000  35.862150   \n",
      "26    25.918242  25.447402   0.000000  14.266611  ...   0.000000  47.437139   \n",
      "27    28.459396  29.610378   0.000000  50.132373  ...   0.000000  36.277945   \n",
      "28    38.874517  22.601449   0.000000  33.561759  ...   0.000000  29.070403   \n",
      "29    31.017825  21.766361   0.000000  27.425346  ...   3.894236  11.285954   \n",
      "...         ...        ...        ...        ...  ...        ...        ...   \n",
      "2946  16.621054  16.076322   0.000000  11.406278  ...   0.000000   9.345173   \n",
      "2947  20.332373  20.779564   4.914102   6.002957  ...   0.000000  23.526664   \n",
      "2948  10.166654   0.548397   0.000000   0.508297  ...   1.438116  10.878190   \n",
      "2949  18.220014  10.257785   0.000000   7.386936  ...   1.742917  10.012569   \n",
      "2950  20.167706  10.898557   4.130814   7.967307  ...   1.510360  15.547205   \n",
      "2951  21.556030  14.203493   4.529964  11.716027  ...   1.532794  17.600544   \n",
      "2952  18.924204  23.254352   0.578904  10.899913  ...   0.000000  16.460960   \n",
      "2953  15.332775   0.750467   0.000000   2.450325  ...   0.000000   7.781984   \n",
      "2954  15.669264  17.051805   1.283620  21.034440  ...   2.356488  10.683835   \n",
      "2955  18.567192  20.202813   0.000000  11.974987  ...   0.183038  12.470673   \n",
      "2956  10.783107  14.458228   5.365179  19.067691  ...   6.014137   5.274705   \n",
      "2957  18.530033  19.199612  20.599443  12.775609  ...   2.521016  10.053292   \n",
      "2958  16.016819  22.607717   0.000000  22.509989  ...   0.846702   7.677273   \n",
      "2959  19.582120  12.469384   0.000000   5.861398  ...   2.295160  18.952776   \n",
      "2960  22.171771  16.004300   0.000000   8.106583  ...   0.000000  18.136384   \n",
      "2961  21.443091  17.334934   4.503638   5.556081  ...   9.450010  12.695900   \n",
      "2962  23.441071  21.412342   6.674789  16.785712  ...   0.000000  18.576055   \n",
      "2963  21.795302   6.274750   0.000000   5.124548  ...   1.799690  19.570846   \n",
      "2964  23.759857  10.202150   7.289540  20.279794  ...   3.150157  15.732537   \n",
      "2965  18.442424  18.924473   0.000000  17.888589  ...   0.000000  12.109941   \n",
      "2966  19.474964  15.931830   0.000000   6.037346  ...   0.000000  15.671586   \n",
      "2967  29.395769  15.423292   0.000000   1.334491  ...   7.010127  12.289426   \n",
      "2968  11.817172  25.411707   0.000000   7.466439  ...   0.737591  11.044343   \n",
      "2969  22.074365  15.854219   0.000000   7.530802  ...   3.303066  11.018375   \n",
      "2970  17.397767   6.669475   0.000000   3.884560  ...   2.290444  13.209082   \n",
      "2971  19.888889   6.678460   0.000000  12.145205  ...   1.879424  12.670002   \n",
      "2972  19.072223   3.387923   0.000000   0.000000  ...   0.000000  19.371605   \n",
      "2973  15.410514  13.703730   0.000000   4.225793  ...   0.058773   9.656911   \n",
      "2974  17.512098  18.424676   0.000000   2.560759  ...   0.000000   9.801162   \n",
      "2975   7.570859   0.000000   0.603126   3.032373  ...   0.000000  12.359037   \n",
      "\n",
      "         116098     116099     116100    116157     116219     116220  \\\n",
      "0     31.126936  45.680952  18.801449  0.000000  22.791096  16.589562   \n",
      "1     29.400996  47.128457  18.081014  0.000000  22.492316  39.614822   \n",
      "2     30.198482  44.345346  17.947455  0.000000  22.289410  33.331948   \n",
      "3     60.259055  57.122521  35.479929  0.000000  40.244356  12.942474   \n",
      "4     66.134278  66.451325  37.001600  0.000000  37.958725  38.225794   \n",
      "5     65.929097  66.349569  48.915435  0.000000  50.373773  24.276129   \n",
      "6     68.030431  71.886087  46.005796  0.000000  49.722241   0.000000   \n",
      "7     68.634745  73.103156  56.698859  0.000000  51.135885  27.220163   \n",
      "8     70.592116  60.747958  58.174526  0.000000  57.160444  23.119363   \n",
      "9     64.980035  69.594903  52.444499  0.000000  51.360643  36.325469   \n",
      "10    68.788526  69.007137  58.214619  0.000000  58.809694  30.052202   \n",
      "11    65.816174  72.206940  52.140354  0.000000  59.963354  26.341642   \n",
      "12    67.929099  66.163011  59.177468  0.000000  63.581915  24.927127   \n",
      "13    66.100885  72.714835  59.859032  0.000000  59.160402  40.284147   \n",
      "14    67.667761  68.202443  59.104579  0.000000  58.480160  34.665834   \n",
      "15    68.621476  69.453213  60.992506  0.000000  62.027281  43.359989   \n",
      "16    68.291143  68.354823  59.714517  0.000000  62.954463  43.632490   \n",
      "17    65.407774  71.312358  51.137745  0.000000  58.018560  32.251570   \n",
      "18    67.674847  73.182116  58.340013  0.000000  64.439813  34.048774   \n",
      "19    70.993967  70.873641  62.416882  0.000000  61.103834  18.805094   \n",
      "20    69.600229  72.078667  57.909412  6.701554  61.983545  22.334514   \n",
      "21    69.941719  69.263506  60.122118  0.000000  63.982623  33.036327   \n",
      "22    68.173481  67.246559  44.310628  0.000000  62.433895  31.826195   \n",
      "23    74.798464  73.245167  65.568166  0.000000  53.040289  31.590351   \n",
      "24    70.751899  64.776135  61.661042  1.802561  62.753117   9.259704   \n",
      "25    69.534026  71.664232  62.448415  0.000000  57.813226  25.087152   \n",
      "26    71.973866  70.020455  68.500494  0.000000  66.173209  54.375801   \n",
      "27    71.913695  66.090129  66.616435  0.000000  64.483602  51.170332   \n",
      "28    70.402014  66.678358  50.324825  0.000000  62.840546  47.793719   \n",
      "29    71.731718  66.003012  15.793436  0.000000  59.481418  35.213490   \n",
      "...         ...        ...        ...       ...        ...        ...   \n",
      "2946  37.698755  46.348432  22.095332  0.038467  27.841142   4.972559   \n",
      "2947  42.109588  48.671108  21.874720  0.000000  31.934600   5.136800   \n",
      "2948  35.645960  44.343302  23.498465  0.000000  30.949700   1.280021   \n",
      "2949  34.260596  37.677862  20.907736  0.000000  27.324777   8.351562   \n",
      "2950  34.192267  38.770371  25.385398  0.082594  28.387821  15.862299   \n",
      "2951  34.483287  34.561348  22.088021  0.188568  21.321251  22.688723   \n",
      "2952  33.738771  41.084354  23.877117  1.514338  25.379574  16.285831   \n",
      "2953  37.859767  43.016123  23.172915  0.000000  24.860740  11.838873   \n",
      "2954  35.396790  41.690636  23.696140  0.000000  25.700716  21.654983   \n",
      "2955  35.334183  47.698038  19.111376  0.000000  20.146626  12.261926   \n",
      "2956  36.788607  38.790310  24.760289  0.000000  29.946767  28.026876   \n",
      "2957  42.467836  50.668277  28.074623  0.008285  28.768498   6.275687   \n",
      "2958  35.008602  41.272904  26.981336  0.000000  35.262194  29.381163   \n",
      "2959  45.717488  52.763768  24.494698  0.489733  28.631877  24.373179   \n",
      "2960  38.542626  40.700446  25.678615  0.000000  28.849982  19.085754   \n",
      "2961  39.000975  49.249257  29.728343  0.011392  28.671993  24.607453   \n",
      "2962  39.461325  47.809948  31.135147  3.316910  36.147069  30.515198   \n",
      "2963  37.448850  47.315214  24.331721  2.348221  26.926014  18.197321   \n",
      "2964  39.832593  44.922287  27.598128  0.216922  29.565925  19.243576   \n",
      "2965  35.477865  46.270300  24.320381  0.012342  27.235592  24.085565   \n",
      "2966  38.827909  47.911611  22.792098  0.000000  28.698006  23.857062   \n",
      "2967  39.723126  48.089489  25.060553  5.272592  33.668220   7.470575   \n",
      "2968  37.766642  45.750956  24.560689  0.431034  29.747896  16.170493   \n",
      "2969  31.164363  43.177665  21.279160  0.107526  31.910840  14.589180   \n",
      "2970  42.762818  48.637535  23.263640  2.119708  30.360498  17.211133   \n",
      "2971  34.848367  42.262780  25.921609  0.004418  29.229571  16.286234   \n",
      "2972  38.794024  45.165799  26.114714  0.000000  25.117042   5.176496   \n",
      "2973  37.280788  46.780534  27.677188  0.000000  28.269513  11.758752   \n",
      "2974  39.112511  38.064220  26.452214  0.000000  26.609735  14.093485   \n",
      "2975  45.995439  45.785918  27.200510  0.854993  17.063615  16.468485   \n",
      "\n",
      "         116221     117189  \n",
      "0     32.590224   0.000000  \n",
      "1     32.588299   0.000000  \n",
      "2     37.471168   0.000000  \n",
      "3     47.140562  39.697219  \n",
      "4     39.550687   0.000000  \n",
      "5     44.648038   0.000000  \n",
      "6     38.623554  11.761946  \n",
      "7     35.115407   0.000000  \n",
      "8     32.632697   0.000000  \n",
      "9     43.623113   0.000000  \n",
      "10    35.687806   0.000000  \n",
      "11    42.476165   0.000000  \n",
      "12    34.540010   0.000000  \n",
      "13    36.533095   0.000000  \n",
      "14    37.561942   7.188408  \n",
      "15    45.600814   0.000000  \n",
      "16    36.047202   0.000000  \n",
      "17    48.167841   0.000000  \n",
      "18    43.047044   0.000000  \n",
      "19    41.153677   0.000000  \n",
      "20    34.789538   0.000000  \n",
      "21    43.543865   0.000000  \n",
      "22    42.535417   0.000000  \n",
      "23    51.173023   9.211282  \n",
      "24    39.690650   0.000000  \n",
      "25    41.027179   0.000000  \n",
      "26    40.371523   0.000000  \n",
      "27    36.336421   0.000000  \n",
      "28    39.679429   0.000000  \n",
      "29    37.998813   0.000000  \n",
      "...         ...        ...  \n",
      "2946  13.806343   3.208666  \n",
      "2947  14.615504   0.000000  \n",
      "2948   7.805262   2.934000  \n",
      "2949  15.666566   0.000000  \n",
      "2950  11.369534   3.220515  \n",
      "2951  13.741226   0.000000  \n",
      "2952  13.299441   0.000000  \n",
      "2953  15.991623   0.000000  \n",
      "2954  11.234242   0.000000  \n",
      "2955  14.298898   0.000000  \n",
      "2956  17.569062   0.000000  \n",
      "2957  15.170356   0.000000  \n",
      "2958  12.867905   0.000000  \n",
      "2959  14.427421   1.776432  \n",
      "2960  14.088725   0.000000  \n",
      "2961  12.329245   0.000000  \n",
      "2962  14.083769   0.000000  \n",
      "2963  16.804706   8.421107  \n",
      "2964  16.205415   0.000000  \n",
      "2965  15.549589   0.000000  \n",
      "2966  10.890544   0.000000  \n",
      "2967  16.232275   0.000000  \n",
      "2968  13.814595   0.000000  \n",
      "2969  15.200403   0.000000  \n",
      "2970  12.847533   0.000000  \n",
      "2971  17.721697   0.000000  \n",
      "2972  15.243260   0.000000  \n",
      "2973  17.273817   0.000000  \n",
      "2974  14.656461   0.000000  \n",
      "2975  15.950042   0.000000  \n",
      "\n",
      "[2976 rows x 156 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data) # Time sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(adj) #Adjacency Matrix of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2976, 156)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "time_len=data.shape[0] # Time sequence length\n",
    "num_nodes=data.shape[1] #Number of Roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the variables\n",
    "output_dim=pre_len=1\n",
    "seq_len=4\n",
    "num_units=100\n",
    "train_rate=0.8\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0980221  0.21353212 0.23823702 ... 0.19194394 0.37707424 0.        ]\n",
      " [0.09032986 0.18181142 0.31845367 ... 0.45834997 0.37705195 0.        ]\n",
      " [0.10192686 0.10389599 0.23464748 ... 0.3856561  0.43354756 0.        ]\n",
      " ...\n",
      " [0.37947276 0.141638   0.10453826 ... 0.13605069 0.1998609  0.        ]\n",
      " [0.39722532 0.18600048 0.121989   ... 0.16306393 0.16957766 0.        ]\n",
      " [0.38101357 0.14001252 0.10420388 ... 0.19054307 0.1845446  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Normalization : Traffic Speed Data\n",
    "\n",
    "data1 =np.mat(data,dtype=np.float32)\n",
    "\n",
    "max_value = np.max(data1)\n",
    "data1  = data1/max_value\n",
    "print(data1)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2380, 156) -----> (596, 156)\n",
      "Train Test Split Details :\n",
      "Train x ---->  2375\n",
      "Train y ---->  2375\n",
      "(2375, 4, 156)\n",
      "Test x ---->  591\n",
      "Test y ---->  591\n",
      "Total Batch ---->  74\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data, time_len, rate, seq_len, pre_len):\n",
    "    train_size = int(time_len * rate) #2976 *0.8 =2380\n",
    "    train_data = data[0:train_size] #  [0:2380]\n",
    "    test_data = data[train_size:time_len] #[2380:2976]\n",
    "    print(train_data.shape,'----->',test_data.shape)\n",
    "\n",
    "    trainX, trainY, testX, testY = [], [], [], []\n",
    "    for i in range(len(train_data) - seq_len - pre_len): #(2380-4-1)=2375\n",
    "        a = train_data[i: i + seq_len + pre_len] #[0:0+4+1] \n",
    "        trainX.append(a[0 : seq_len]) #a[0:4] 4 time * 156 roads\n",
    "        trainY.append(a[seq_len : seq_len + pre_len]) #a[4:4+1] 1 time*156 \n",
    "    for i in range(len(test_data) - seq_len -pre_len):\n",
    "        b = test_data[i: i + seq_len + pre_len]\n",
    "        testX.append(b[0 : seq_len])\n",
    "        testY.append(b[seq_len : seq_len + pre_len])\n",
    "      \n",
    "    trainX1 = np.array(trainX) \n",
    "    trainY1 = np.array(trainY)\n",
    "    testX1 = np.array(testX)\n",
    "    testY1 = np.array(testY)\n",
    "    return trainX1, trainY1, testX1, testY1\n",
    "\n",
    "trainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len)\n",
    "\n",
    "totalbatch = int(trainX.shape[0]/batch_size)\n",
    "training_data_count = len(trainX)  \n",
    "print('Train Test Split Details :')\n",
    "print('Train x ----> ',len(trainX))\n",
    "print('Train y ----> ',len(trainY))\n",
    "print(trainX.shape)\n",
    "print('Test x ----> ',len(testX))\n",
    "print('Test y ----> ',len(testY))\n",
    "print('Total Batch ----> ',totalbatch)\n",
    "#print('\\nTrain Sample Details :')\n",
    "#print(trainX[0],'--->',trainY[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "        init_range = np.sqrt(1 / (input_dim))\n",
    "        initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "        return tf.Variable(initial,name=name) \n",
    "#weights =weight_variable_glorot(4, 64, name='weights')\n",
    "#weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stgcnCell(RNNCell):\n",
    "    \"\"\"Temporal Graph Convolutional Network \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, num_units, adj, num_nodes, input_size=None,\n",
    "                 act=tf.nn.tanh, reuse=None):\n",
    "\n",
    "        super(stgcnCell, self).__init__(_reuse=reuse)\n",
    "        self._act = act\n",
    "        self._nodes = num_nodes\n",
    "        self._units = num_units\n",
    "        self._adj = []\n",
    "        self._adj.append(self.calculate_laplacian(adj))\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_sparse_matrix(L):\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
    "        return tf.sparse_reorder(L)\n",
    "\n",
    "    def calculate_laplacian(self,adj, lambda_max=1):  \n",
    "        adj = self.normalized_adj(adj + sp.eye(adj.shape[0])) # normalisation(self identity matrix + adj)\n",
    "        adj = sp.csr_matrix(adj) #compressed sparse matrix\n",
    "        adj = adj.astype(np.float32)\n",
    "        return self.sparse_to_tuple(adj)\n",
    "    \n",
    "    def normalized_adj(self,adj):\n",
    "        adj = sp.coo_matrix(adj)\n",
    "        degree = np.array(adj.sum(1)) # Degree Matrix row wise sum\n",
    "        d_inv_sqrt = np.power(degree, -0.5).flatten() # D inv = Degree ^-0.5 \n",
    "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt) #substitution of the 1D array degree in a 2D matrix diagonals\n",
    "        normalized_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo() # norm= D^-0.5 * adj * D^-0.5\n",
    "        normalized_adj = normalized_adj.astype(np.float32) \n",
    "        return normalized_adj\n",
    "     \n",
    "    ''' def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "        init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "        initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "        return tf.Variable(initial,name=name) '''\n",
    "    \n",
    "    def sparse_to_tuple(self,mx):\n",
    "        mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose() #coordinate stacking row and column wise and transpose\n",
    "        L = tf.SparseTensor(coords, mx.data, mx.shape) # mx.shape= (156,156)\n",
    "        #print('shape ---->',mx.shape)\n",
    "        return tf.sparse_reorder(L) #row major ordering\n",
    "        \n",
    "    def init_state(self,batch_size):       \n",
    "        state = tf.zeros(shape=[batch_size, self._num_nodes*self._num_units], dtype=tf.float32)\n",
    "        return state  \n",
    "               \n",
    "    @staticmethod\n",
    "    def _concat(x, x_):\n",
    "        x_ = tf.expand_dims(x_, 0)\n",
    "        return tf.concat([x, x_], axis=0)   \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._nodes * self._units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "\n",
    "        with tf.variable_scope(scope or \"tgcn\"):\n",
    "            with tf.variable_scope(\"gates\"):  \n",
    "                value = tf.nn.sigmoid(\n",
    "                    self._gc(inputs, state, 2 * self._units, bias=1.0, scope=scope))\n",
    "                r, u = tf.split(value=value, num_or_size_splits=2, axis=1)\n",
    "            with tf.variable_scope(\"candidate\"):\n",
    "                r_state = r * state\n",
    "                c = self._act(self._gc(inputs, r_state, self._units, scope=scope))\n",
    "            new_h = u * state + (1 - u) * c\n",
    "        return new_h, new_h\n",
    "\n",
    "\n",
    "    def _gc(self, inputs, state, output_size, bias=0.0, scope=None):\n",
    "        ## inputs:(-1,num_nodes)\n",
    "        inputs = tf.expand_dims(inputs, 2)\n",
    "        ## state:(batch,num_node,gru_units)\n",
    "        state = tf.reshape(state, (-1, self._nodes, self._units))#(-1,156,64)\n",
    "        ## concat\n",
    "        x_s = tf.concat([inputs, state], axis=2)\n",
    "        input_size = x_s.get_shape()[2].value\n",
    "        ## (num_node,input_size,-1)\n",
    "        x0 = tf.transpose(x_s, perm=[1, 2, 0])  \n",
    "        x0 = tf.reshape(x0, shape=[self._nodes, -1])\n",
    "        \n",
    "        scope = tf.get_variable_scope()\n",
    "        with tf.variable_scope(scope):\n",
    "            for m in self._adj:\n",
    "                x1 = tf.sparse_tensor_dense_matmul(m, x0)\n",
    "#                print(x1)\n",
    "            x = tf.reshape(x1, shape=[self._nodes, input_size,-1])\n",
    "            x = tf.transpose(x,perm=[2,0,1])\n",
    "            x = tf.reshape(x, shape=[-1, input_size])\n",
    "            weights = weight_variable_glorot(input_size, output_size, name='weights')\n",
    "            x = tf.matmul(x, weights)  # (batch_size * self._nodes, output_size)\n",
    "            biases = tf.get_variable(\n",
    "                \"biases\", [output_size], initializer=tf.constant_initializer(bias, dtype=tf.float32))\n",
    "            x = tf.nn.bias_add(x, biases)\n",
    "            x = tf.reshape(x, shape=[-1, self._nodes, output_size])\n",
    "            x = tf.reshape(x, shape=[-1, self._nodes * output_size])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STGCN(_X, _weights, _biases):\n",
    "    ###\n",
    "    cell_1 = stgcnCell(num_units, adj, num_nodes=num_nodes)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True)\n",
    "    _X = tf.unstack(_X, axis=1)\n",
    "    outputs, states = tf.nn.static_rnn(cell, _X, dtype=tf.float32)\n",
    "    m = []\n",
    "    for i in outputs:\n",
    "        o = tf.reshape(i,shape=[-1,num_nodes,num_units])\n",
    "        o = tf.reshape(o,shape=[-1,num_units])\n",
    "        m.append(o)\n",
    "    last_output = m[-1]\n",
    "    output = tf.matmul(last_output, _weights['out']) + _biases['out']\n",
    "    output = tf.reshape(output,shape=[-1,num_nodes,pre_len])\n",
    "    output = tf.transpose(output, perm=[0,2,1])\n",
    "    output = tf.reshape(output, shape=[-1,num_nodes])\n",
    "    return output, m, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Lema Labs ML Workshop x64\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "WARNING:tensorflow:From <ipython-input-11-bbb08c04eed1>:4: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-11-bbb08c04eed1>:6: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.placeholder(tf.float32, shape=[None, seq_len, num_nodes])\n",
    "labels = tf.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_units, pre_len], mean=1.0), name='weight_o')}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([pre_len]),name='bias_o')}\n",
    "print(type(inputs))\n",
    "pred,ttts,ttto = STGCN(inputs, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_loss = 0.0015\n",
    "Lreg = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "label = tf.reshape(labels, [-1,num_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.l2_loss(y_pred-label) + Lreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = tf.sqrt(tf.reduce_mean(tf.square(y_pred-label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = tf.global_variables()\n",
    "training_epoch=1000\n",
    "saver = tf.train.Saver(tf.global_variables()) #\n",
    "#sess = tf.Session()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "out = 'out/%s'%(\"STGCN\")\n",
    "#out = 'out/%s_%s'%(model_name,'perturbation')\n",
    "path1 = '%s_%s_lr%r_batch%r_unit%r_seq%r_pre%r_epoch%r'%(\"STGCN\",\"sz\",lr,batch_size,num_units,seq_len,pre_len,training_epoch)\n",
    "path = os.path.join(out,path1)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(a,b):\n",
    "    rmse = math.sqrt(mean_squared_error(a,b))\n",
    "    mae = mean_absolute_error(a, b)\n",
    "    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n",
    "    r2 = 1-((a-b)**2).sum()/((a-a.mean())**2).sum()\n",
    "    var = 1-(np.var(a-b))/np.var(a)\n",
    "    return rmse, mae, 1-F_norm, r2, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []\n",
    "test_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(a,b):  \n",
    "    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n",
    "    train_acc=1-F_norm\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Accuracy ---->  0.6106643974781036\n",
      "(32, 1, 156) (32, 156)\n",
      "Iter:0 train_rmse:5.618 test_loss:221.3 test_rmse:0.06921 test_acc:0.5833\n",
      "Epoch  1\n",
      "Accuracy ---->  0.6167108416557312\n",
      "(32, 1, 156) (32, 156)\n",
      "Iter:1 train_rmse:5.531 test_loss:213.9 test_rmse:0.06806 test_acc:0.5903\n",
      "Epoch  2\n",
      "Accuracy ---->  0.6187747716903687\n",
      "(32, 1, 156) (32, 156)\n",
      "Iter:2 train_rmse:5.501 test_loss:211.5 test_rmse:0.06767 test_acc:0.5926\n",
      "Epoch  3\n",
      "Accuracy ---->  0.6211485266685486\n",
      "(32, 1, 156) (32, 156)\n",
      "Iter:3 train_rmse:5.467 test_loss:209.4 test_rmse:0.06734 test_acc:0.5946\n",
      "Epoch  4\n",
      "Accuracy ---->  0.6241418719291687\n",
      "(32, 1, 156) (32, 156)\n",
      "Iter:4 train_rmse:5.424 test_loss:206.4 test_rmse:0.06686 test_acc:0.5975\n",
      "Epoch  5\n",
      "Accuracy ---->  0.6281624436378479\n",
      "(32, 1, 156) (32, 156)\n",
      "Iter:5 train_rmse:5.366 test_loss:202.7 test_rmse:0.06626 test_acc:0.6011\n",
      "Epoch  6\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epoch):\n",
    "    print(\"Epoch \", epoch)\n",
    "    for m in range(totalbatch):\n",
    "        mini_batch = trainX[m * batch_size : (m+1) * batch_size]\n",
    "        mini_label = trainY[m * batch_size : (m+1) * batch_size]\n",
    "        _, loss1, rmse1, train_output = sess.run([optimizer, loss, error, y_pred],\n",
    "                                                 feed_dict = {inputs:mini_batch, labels:mini_label})\n",
    "        batch_loss.append(loss1)\n",
    "        batch_rmse.append(rmse1 * max_value)\n",
    "        train_label=np.reshape(mini_label,[-1,num_nodes])\n",
    "        \n",
    "\n",
    "     # Test completely at every epoch\n",
    "    print(\"Accuracy ----> \", evaluation(train_label,train_output)[2])\n",
    "    loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n",
    "                                         feed_dict = {inputs:testX, labels:testY})\n",
    "    #train_label=np.reshape(trainY,[-1,num_nodes])\n",
    "    #train_acc=acc(train_label,train_output)\n",
    "    test_label = np.reshape(testY,[-1,num_nodes])\n",
    "    rmse, mae, acc, r2_score, var_score = evaluation(test_label, test_output)\n",
    "    test_label1 = test_label * max_value#Inverse normalization\n",
    "    test_output1 = test_output * max_value\n",
    "    test_loss.append(loss2)\n",
    "    test_rmse.append(rmse * max_value)\n",
    "    test_mae.append(mae * max_value)\n",
    "    test_acc.append(acc)\n",
    "    test_r2.append(r2_score)\n",
    "    test_var.append(var_score)\n",
    "    test_pred.append(test_output1)\n",
    "    print(mini_label.shape,train_output.shape)\n",
    "    print('Iter:{}'.format(epoch),\n",
    "          'train_rmse:{:.4}'.format(batch_rmse[-1]),\n",
    "          'test_loss:{:.4}'.format(loss2),\n",
    "          'test_rmse:{:.4}'.format(rmse),\n",
    "          'test_acc:{:.4}'.format(acc))\n",
    "    if (epoch % 500 == 0):        \n",
    "        saver.save(sess, path+'/model_100/TGCN_pre_%r'%epoch, global_step = epoch)\n",
    "        \n",
    "time_end = time.time()\n",
    "print(time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = int(len(batch_rmse)/totalbatch)\n",
    "batch_rmse1 = [i for i in batch_rmse]\n",
    "train_rmse = [(sum(batch_rmse1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "batch_loss1 = [i for i in batch_loss]\n",
    "train_loss = [(sum(batch_loss1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "\n",
    "index = test_rmse.index(np.min(test_rmse))\n",
    "test_result = test_pred[index]\n",
    "var = pd.DataFrame(test_result)\n",
    "var.to_csv(path+'/newtest_result.csv',index = False,header = False)\n",
    "#plot_result(test_result,test_label1,path)\n",
    "#plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing:\")\n",
    "print('min_rmse:%r'%(np.min(test_rmse)),\n",
    "      'min_mae:%r'%(test_mae[index]),\n",
    "      'max_acc:%r'%(test_acc[index]),\n",
    "      'r2:%r'%(test_r2[index]),\n",
    "      'var:%r'%test_var[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training:\")\n",
    "rmse, mae, acc, r2_score, var_score = evaluation(train_label,train_output)\n",
    "print('min_rmse:%r'%(rmse),\n",
    "      'min_mae:%r'%(mae),\n",
    "      'max_acc:%r'%(acc),\n",
    "      'r2:%r'%(r2_score),\n",
    "      'var:%r'%(var_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse normalisation\n",
    "print('min_rmse:%r'%(rmse*max_value),\n",
    "      'min_mae:%r'%(mae*max_value),\n",
    "      'max_acc:%r'%(acc),\n",
    "      'r2:%r'%(r2_score),\n",
    "      'var:%r'%(var_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(test_result,test_label1,path):\n",
    "    ##all test result visualization\n",
    "    fig1 = plt.figure(figsize=(7,1.5))\n",
    "#    ax1 = fig1.add_subplot(1,1,1)\n",
    "    a_pred = test_result[:,0]\n",
    "    a_true = test_label1[:,0]\n",
    "    plt.plot(a_pred,'r-',label='prediction')\n",
    "    plt.plot(a_true,'b-',label='true')\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_all.jpg')\n",
    "    plt.show()\n",
    "    ## oneday test result visualization\n",
    "    fig1 = plt.figure(figsize=(7,1.5))\n",
    "#    ax1 = fig1.add_subplot(1,1,1)\n",
    "    a_pred = test_result[0:96,0]\n",
    "    a_true = test_label1[0:96,0]\n",
    "    plt.plot(a_pred,'r-',label=\"prediction\")\n",
    "    plt.plot(a_true,'b-',label=\"true\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_oneday.jpg')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path):\n",
    "    ###train_rmse & test_rmse \n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(train_rmse, 'r-', label=\"train_rmse\")\n",
    "    plt.plot(test_rmse, 'b-', label=\"test_rmse\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/rmse.jpg')\n",
    "    plt.show()\n",
    "    #### train_loss & train_rmse\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(train_loss,'b-', label='train_loss')\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/train_loss.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(train_rmse,'b-', label='train_rmse')\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/train_rmse.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    ### accuracy\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(test_acc, 'b-', label=\"test_acc\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_acc.jpg')\n",
    "    plt.show()\n",
    "    ### rmse\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(test_rmse, 'b-', label=\"test_rmse\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_rmse.jpg')\n",
    "    plt.show()\n",
    "    ### mae\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(test_mae, 'b-', label=\"test_mae\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_mae.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(test_result,test_label1,path)\n",
    "plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
