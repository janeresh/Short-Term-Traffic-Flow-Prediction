{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import numpy.linalg as la\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "#import matplotlib.pyplot as plt\n",
    "import time\n",
    "time_start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of Dataset-SZ Traffic\n",
    "def load_sz_data():\n",
    "    sz_adj = pd.read_csv('sz_adj.csv',header=None)\n",
    "    adj = np.mat(sz_adj)\n",
    "    sz_tf = pd.read_csv('sz_speed.csv')\n",
    "    return sz_tf, adj\n",
    "\n",
    "data, adj = load_sz_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          90217      90218      90219      90220      90221      90222  \\\n",
      "0      8.471971  18.455410  20.590635  15.345258   9.585218  21.501821   \n",
      "1      7.807137  15.713816  27.523695  11.087895   9.455280  17.332246   \n",
      "2      8.809457   8.979647  20.280394  16.523419   8.003314  15.789483   \n",
      "3     51.590372  23.631243  20.224094  15.116459   6.642644  17.575806   \n",
      "4     58.770433  20.437740  20.465606  14.820217  11.344404   0.000000   \n",
      "5     58.289126  10.332738  25.331018  18.485616   2.028513  10.718488   \n",
      "6     61.334449  20.818480  15.839392   8.768217  13.933326  13.866124   \n",
      "7     58.903144   8.291826  44.043729  15.250859   0.000000   0.000000   \n",
      "8     57.210441  25.765667  18.677565   5.066051   0.000000  12.645903   \n",
      "9     59.559265  16.803983  20.314610  23.393083   2.612821   0.000000   \n",
      "10    60.217867  26.773576  17.684078  17.735375  12.957171   7.404007   \n",
      "11    56.336405  18.866552  27.502295  18.201105  23.973238  22.215358   \n",
      "12    58.511395  15.802270  15.900551  15.825524  28.359067  13.519283   \n",
      "13    59.327670  28.104659  23.004938  17.177101   5.770516   0.000000   \n",
      "14    59.415925  20.049136  24.239933  16.541053  18.319476  30.199940   \n",
      "15    57.000535  25.916699  23.620492  15.149346  18.415374  10.608687   \n",
      "16    59.468388  23.153764  22.637581  21.761991   7.845323  14.191932   \n",
      "17    57.489538  29.269876  21.906870  15.336707  23.427912  22.823073   \n",
      "18    60.289596  24.220646  18.846753  20.107602  17.853574   0.000000   \n",
      "19    58.943452  28.973177  23.110562  17.220674  21.988291  42.517631   \n",
      "20    59.584971  28.455832  19.782971  18.319035  20.363029   9.263639   \n",
      "21    61.006783  20.343802  23.375934  14.071018  11.776268  14.986706   \n",
      "22    61.391417  16.989282  17.488080  16.687639  18.837553  12.817948   \n",
      "23    62.847104  17.922808  20.711342  12.684088  14.431455  37.580218   \n",
      "24    59.712049  22.491029  20.029159  19.459853  10.929650  12.636402   \n",
      "25    60.128278  11.853744  21.438214  16.230672  18.646959  33.716293   \n",
      "26    57.230341  21.015119  20.436854  18.399933  10.754621  28.184720   \n",
      "27    61.317819  18.813310  22.462014  18.800135  19.252080  31.657746   \n",
      "28    57.949689  23.351677  24.078893  15.910953  19.144494  30.628915   \n",
      "29    60.440370  19.183227  22.276537  18.166327  20.971482   9.271435   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "2946  37.047736  11.747849   9.158301  12.016694  10.121105  16.834794   \n",
      "2947  32.587985  11.186664  10.718418  12.244456  11.516860  22.302047   \n",
      "2948  28.728285  14.481847   9.443261   9.845491   6.633443   5.380683   \n",
      "2949  27.710759  15.547506  10.484638  12.722840  10.626220  10.623221   \n",
      "2950  26.003952  15.903029   8.968620  10.328466   9.006967  27.212662   \n",
      "2951  30.641217  13.588164   9.242868  11.115546   8.813212  22.479598   \n",
      "2952  29.589988  13.610331   9.388144   8.920857   9.951734  25.764707   \n",
      "2953  32.527293  11.195964   9.614881  11.488864   9.484497  26.042143   \n",
      "2954  32.717067  11.197344   9.038040  10.987866   8.974821  12.656677   \n",
      "2955  32.833424  11.759863   7.935098  11.153611   8.666250  26.168973   \n",
      "2956  30.866704  13.977036   9.085705  10.476089  10.399103  13.961974   \n",
      "2957  36.407212  13.445718   8.871406  12.298801   9.390320  13.086711   \n",
      "2958  29.498956  15.263794   9.542898  13.793024  11.678501  10.690330   \n",
      "2959  34.159054   8.463801   9.084855  10.914051   8.824929  10.646655   \n",
      "2960  29.827767  10.139989   8.011831   9.475263   8.314195  34.699582   \n",
      "2961  34.712911  13.224499   7.302345   9.891491   9.414917  22.504949   \n",
      "2962  29.886521  13.882277   8.170605   9.800999   8.409821  22.904986   \n",
      "2963  31.016664  10.664699   6.565841  10.013826  10.228492  28.740343   \n",
      "2964  31.287008  13.376588   6.999327   9.330356  10.903492  22.816266   \n",
      "2965  32.527834  11.507783   7.120969   9.997413  10.916735  14.875885   \n",
      "2966  31.001262  10.235263   7.584673  11.063059   9.750946  18.449771   \n",
      "2967  28.383740  11.333047   8.815655   9.994421   6.719870  17.476265   \n",
      "2968  31.422072  11.200472   8.240685  12.568828   8.239171  18.232270   \n",
      "2969  30.696860   9.195974   7.187724  10.021121   8.282266  14.751376   \n",
      "2970  29.795250   9.772127   6.848839   9.454226   7.350518  15.062446   \n",
      "2971  34.595076  11.353815   8.251759  10.507924   8.129874  17.123473   \n",
      "2972  28.697368  13.005691   8.131853  10.418131   8.087082  10.528642   \n",
      "2973  32.797527  12.241658   9.035159   8.513272   9.346719   6.296222   \n",
      "2974  34.331867  16.075872  10.543411  11.254747   9.930924  26.989660   \n",
      "2975  32.930700  12.101170   9.006258  11.971602   9.063041   3.209035   \n",
      "\n",
      "          90223      90224      90225      90226  ...     112746     116097  \\\n",
      "0     31.611759   0.000000   0.000000  22.008941  ...   0.000000  27.623514   \n",
      "1     33.976531  12.968823   0.000000  17.921757  ...   4.193466  14.889713   \n",
      "2     13.747267  12.221143   0.000000  21.588447  ...  11.076185   0.000000   \n",
      "3     17.657556  15.998745   0.000000  24.228905  ...   0.000000  34.132564   \n",
      "4     33.878502   5.676248   0.000000  26.250064  ...   0.000000  15.904281   \n",
      "5     10.439684   0.000000   2.122070  20.836691  ...  10.167713  15.566301   \n",
      "6     31.703332   4.823769   5.320506  29.906599  ...   0.000000  46.163459   \n",
      "7     28.502493   8.702932  15.535269  21.537570  ...   0.000000  29.304136   \n",
      "8     41.644261   0.000000   0.000000  36.559919  ...   5.753177  14.196596   \n",
      "9     43.230732  17.152937   5.947999  36.524772  ...   0.000000  21.855200   \n",
      "10    34.944085  20.617665   0.000000  31.518823  ...   0.000000  15.034488   \n",
      "11    36.325173   0.000000   0.000000  36.831651  ...   8.816271   4.736363   \n",
      "12    26.471023   0.000000   0.000000  39.778615  ...   0.000000  14.252345   \n",
      "13    42.172147  10.559143   7.318216  19.045834  ...   1.536469  19.284731   \n",
      "14    32.131190   8.375252   0.000000  25.769071  ...   7.341624  14.752431   \n",
      "15    51.908341   8.897444   3.245168  36.120403  ...   0.000000  38.125425   \n",
      "16    29.510518  16.612282   0.000000  24.700598  ...   2.491167  14.258565   \n",
      "17    36.651121   7.383970   0.000000  44.773479  ...   2.860599  26.084234   \n",
      "18    27.925195  24.273771   0.000000  23.039533  ...   0.725449   0.000000   \n",
      "19    27.891778   0.000000   0.000000  34.002083  ...   6.751413  11.150303   \n",
      "20    17.395813   0.000000   0.000000  11.116550  ...   2.888578  23.275499   \n",
      "21    22.301599  19.320293   0.000000  27.543710  ...   0.000000  24.007239   \n",
      "22    21.587101  17.851547   0.000000  21.746151  ...  12.737200  27.440866   \n",
      "23    23.709984  14.883667   0.000000  33.107521  ...   3.938674  36.426266   \n",
      "24    29.708086  22.742133   0.000000  37.725845  ...   0.000000  22.046767   \n",
      "25    30.235825   4.255047   0.000000  26.690446  ...   0.000000  35.862150   \n",
      "26    25.918242  25.447402   0.000000  14.266611  ...   0.000000  47.437139   \n",
      "27    28.459396  29.610378   0.000000  50.132373  ...   0.000000  36.277945   \n",
      "28    38.874517  22.601449   0.000000  33.561759  ...   0.000000  29.070403   \n",
      "29    31.017825  21.766361   0.000000  27.425346  ...   3.894236  11.285954   \n",
      "...         ...        ...        ...        ...  ...        ...        ...   \n",
      "2946  16.621054  16.076322   0.000000  11.406278  ...   0.000000   9.345173   \n",
      "2947  20.332373  20.779564   4.914102   6.002957  ...   0.000000  23.526664   \n",
      "2948  10.166654   0.548397   0.000000   0.508297  ...   1.438116  10.878190   \n",
      "2949  18.220014  10.257785   0.000000   7.386936  ...   1.742917  10.012569   \n",
      "2950  20.167706  10.898557   4.130814   7.967307  ...   1.510360  15.547205   \n",
      "2951  21.556030  14.203493   4.529964  11.716027  ...   1.532794  17.600544   \n",
      "2952  18.924204  23.254352   0.578904  10.899913  ...   0.000000  16.460960   \n",
      "2953  15.332775   0.750467   0.000000   2.450325  ...   0.000000   7.781984   \n",
      "2954  15.669264  17.051805   1.283620  21.034440  ...   2.356488  10.683835   \n",
      "2955  18.567192  20.202813   0.000000  11.974987  ...   0.183038  12.470673   \n",
      "2956  10.783107  14.458228   5.365179  19.067691  ...   6.014137   5.274705   \n",
      "2957  18.530033  19.199612  20.599443  12.775609  ...   2.521016  10.053292   \n",
      "2958  16.016819  22.607717   0.000000  22.509989  ...   0.846702   7.677273   \n",
      "2959  19.582120  12.469384   0.000000   5.861398  ...   2.295160  18.952776   \n",
      "2960  22.171771  16.004300   0.000000   8.106583  ...   0.000000  18.136384   \n",
      "2961  21.443091  17.334934   4.503638   5.556081  ...   9.450010  12.695900   \n",
      "2962  23.441071  21.412342   6.674789  16.785712  ...   0.000000  18.576055   \n",
      "2963  21.795302   6.274750   0.000000   5.124548  ...   1.799690  19.570846   \n",
      "2964  23.759857  10.202150   7.289540  20.279794  ...   3.150157  15.732537   \n",
      "2965  18.442424  18.924473   0.000000  17.888589  ...   0.000000  12.109941   \n",
      "2966  19.474964  15.931830   0.000000   6.037346  ...   0.000000  15.671586   \n",
      "2967  29.395769  15.423292   0.000000   1.334491  ...   7.010127  12.289426   \n",
      "2968  11.817172  25.411707   0.000000   7.466439  ...   0.737591  11.044343   \n",
      "2969  22.074365  15.854219   0.000000   7.530802  ...   3.303066  11.018375   \n",
      "2970  17.397767   6.669475   0.000000   3.884560  ...   2.290444  13.209082   \n",
      "2971  19.888889   6.678460   0.000000  12.145205  ...   1.879424  12.670002   \n",
      "2972  19.072223   3.387923   0.000000   0.000000  ...   0.000000  19.371605   \n",
      "2973  15.410514  13.703730   0.000000   4.225793  ...   0.058773   9.656911   \n",
      "2974  17.512098  18.424676   0.000000   2.560759  ...   0.000000   9.801162   \n",
      "2975   7.570859   0.000000   0.603126   3.032373  ...   0.000000  12.359037   \n",
      "\n",
      "         116098     116099     116100    116157     116219     116220  \\\n",
      "0     31.126936  45.680952  18.801449  0.000000  22.791096  16.589562   \n",
      "1     29.400996  47.128457  18.081014  0.000000  22.492316  39.614822   \n",
      "2     30.198482  44.345346  17.947455  0.000000  22.289410  33.331948   \n",
      "3     60.259055  57.122521  35.479929  0.000000  40.244356  12.942474   \n",
      "4     66.134278  66.451325  37.001600  0.000000  37.958725  38.225794   \n",
      "5     65.929097  66.349569  48.915435  0.000000  50.373773  24.276129   \n",
      "6     68.030431  71.886087  46.005796  0.000000  49.722241   0.000000   \n",
      "7     68.634745  73.103156  56.698859  0.000000  51.135885  27.220163   \n",
      "8     70.592116  60.747958  58.174526  0.000000  57.160444  23.119363   \n",
      "9     64.980035  69.594903  52.444499  0.000000  51.360643  36.325469   \n",
      "10    68.788526  69.007137  58.214619  0.000000  58.809694  30.052202   \n",
      "11    65.816174  72.206940  52.140354  0.000000  59.963354  26.341642   \n",
      "12    67.929099  66.163011  59.177468  0.000000  63.581915  24.927127   \n",
      "13    66.100885  72.714835  59.859032  0.000000  59.160402  40.284147   \n",
      "14    67.667761  68.202443  59.104579  0.000000  58.480160  34.665834   \n",
      "15    68.621476  69.453213  60.992506  0.000000  62.027281  43.359989   \n",
      "16    68.291143  68.354823  59.714517  0.000000  62.954463  43.632490   \n",
      "17    65.407774  71.312358  51.137745  0.000000  58.018560  32.251570   \n",
      "18    67.674847  73.182116  58.340013  0.000000  64.439813  34.048774   \n",
      "19    70.993967  70.873641  62.416882  0.000000  61.103834  18.805094   \n",
      "20    69.600229  72.078667  57.909412  6.701554  61.983545  22.334514   \n",
      "21    69.941719  69.263506  60.122118  0.000000  63.982623  33.036327   \n",
      "22    68.173481  67.246559  44.310628  0.000000  62.433895  31.826195   \n",
      "23    74.798464  73.245167  65.568166  0.000000  53.040289  31.590351   \n",
      "24    70.751899  64.776135  61.661042  1.802561  62.753117   9.259704   \n",
      "25    69.534026  71.664232  62.448415  0.000000  57.813226  25.087152   \n",
      "26    71.973866  70.020455  68.500494  0.000000  66.173209  54.375801   \n",
      "27    71.913695  66.090129  66.616435  0.000000  64.483602  51.170332   \n",
      "28    70.402014  66.678358  50.324825  0.000000  62.840546  47.793719   \n",
      "29    71.731718  66.003012  15.793436  0.000000  59.481418  35.213490   \n",
      "...         ...        ...        ...       ...        ...        ...   \n",
      "2946  37.698755  46.348432  22.095332  0.038467  27.841142   4.972559   \n",
      "2947  42.109588  48.671108  21.874720  0.000000  31.934600   5.136800   \n",
      "2948  35.645960  44.343302  23.498465  0.000000  30.949700   1.280021   \n",
      "2949  34.260596  37.677862  20.907736  0.000000  27.324777   8.351562   \n",
      "2950  34.192267  38.770371  25.385398  0.082594  28.387821  15.862299   \n",
      "2951  34.483287  34.561348  22.088021  0.188568  21.321251  22.688723   \n",
      "2952  33.738771  41.084354  23.877117  1.514338  25.379574  16.285831   \n",
      "2953  37.859767  43.016123  23.172915  0.000000  24.860740  11.838873   \n",
      "2954  35.396790  41.690636  23.696140  0.000000  25.700716  21.654983   \n",
      "2955  35.334183  47.698038  19.111376  0.000000  20.146626  12.261926   \n",
      "2956  36.788607  38.790310  24.760289  0.000000  29.946767  28.026876   \n",
      "2957  42.467836  50.668277  28.074623  0.008285  28.768498   6.275687   \n",
      "2958  35.008602  41.272904  26.981336  0.000000  35.262194  29.381163   \n",
      "2959  45.717488  52.763768  24.494698  0.489733  28.631877  24.373179   \n",
      "2960  38.542626  40.700446  25.678615  0.000000  28.849982  19.085754   \n",
      "2961  39.000975  49.249257  29.728343  0.011392  28.671993  24.607453   \n",
      "2962  39.461325  47.809948  31.135147  3.316910  36.147069  30.515198   \n",
      "2963  37.448850  47.315214  24.331721  2.348221  26.926014  18.197321   \n",
      "2964  39.832593  44.922287  27.598128  0.216922  29.565925  19.243576   \n",
      "2965  35.477865  46.270300  24.320381  0.012342  27.235592  24.085565   \n",
      "2966  38.827909  47.911611  22.792098  0.000000  28.698006  23.857062   \n",
      "2967  39.723126  48.089489  25.060553  5.272592  33.668220   7.470575   \n",
      "2968  37.766642  45.750956  24.560689  0.431034  29.747896  16.170493   \n",
      "2969  31.164363  43.177665  21.279160  0.107526  31.910840  14.589180   \n",
      "2970  42.762818  48.637535  23.263640  2.119708  30.360498  17.211133   \n",
      "2971  34.848367  42.262780  25.921609  0.004418  29.229571  16.286234   \n",
      "2972  38.794024  45.165799  26.114714  0.000000  25.117042   5.176496   \n",
      "2973  37.280788  46.780534  27.677188  0.000000  28.269513  11.758752   \n",
      "2974  39.112511  38.064220  26.452214  0.000000  26.609735  14.093485   \n",
      "2975  45.995439  45.785918  27.200510  0.854993  17.063615  16.468485   \n",
      "\n",
      "         116221     117189  \n",
      "0     32.590224   0.000000  \n",
      "1     32.588299   0.000000  \n",
      "2     37.471168   0.000000  \n",
      "3     47.140562  39.697219  \n",
      "4     39.550687   0.000000  \n",
      "5     44.648038   0.000000  \n",
      "6     38.623554  11.761946  \n",
      "7     35.115407   0.000000  \n",
      "8     32.632697   0.000000  \n",
      "9     43.623113   0.000000  \n",
      "10    35.687806   0.000000  \n",
      "11    42.476165   0.000000  \n",
      "12    34.540010   0.000000  \n",
      "13    36.533095   0.000000  \n",
      "14    37.561942   7.188408  \n",
      "15    45.600814   0.000000  \n",
      "16    36.047202   0.000000  \n",
      "17    48.167841   0.000000  \n",
      "18    43.047044   0.000000  \n",
      "19    41.153677   0.000000  \n",
      "20    34.789538   0.000000  \n",
      "21    43.543865   0.000000  \n",
      "22    42.535417   0.000000  \n",
      "23    51.173023   9.211282  \n",
      "24    39.690650   0.000000  \n",
      "25    41.027179   0.000000  \n",
      "26    40.371523   0.000000  \n",
      "27    36.336421   0.000000  \n",
      "28    39.679429   0.000000  \n",
      "29    37.998813   0.000000  \n",
      "...         ...        ...  \n",
      "2946  13.806343   3.208666  \n",
      "2947  14.615504   0.000000  \n",
      "2948   7.805262   2.934000  \n",
      "2949  15.666566   0.000000  \n",
      "2950  11.369534   3.220515  \n",
      "2951  13.741226   0.000000  \n",
      "2952  13.299441   0.000000  \n",
      "2953  15.991623   0.000000  \n",
      "2954  11.234242   0.000000  \n",
      "2955  14.298898   0.000000  \n",
      "2956  17.569062   0.000000  \n",
      "2957  15.170356   0.000000  \n",
      "2958  12.867905   0.000000  \n",
      "2959  14.427421   1.776432  \n",
      "2960  14.088725   0.000000  \n",
      "2961  12.329245   0.000000  \n",
      "2962  14.083769   0.000000  \n",
      "2963  16.804706   8.421107  \n",
      "2964  16.205415   0.000000  \n",
      "2965  15.549589   0.000000  \n",
      "2966  10.890544   0.000000  \n",
      "2967  16.232275   0.000000  \n",
      "2968  13.814595   0.000000  \n",
      "2969  15.200403   0.000000  \n",
      "2970  12.847533   0.000000  \n",
      "2971  17.721697   0.000000  \n",
      "2972  15.243260   0.000000  \n",
      "2973  17.273817   0.000000  \n",
      "2974  14.656461   0.000000  \n",
      "2975  15.950042   0.000000  \n",
      "\n",
      "[2976 rows x 156 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data) # Time sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(adj) #Adjacency Matrix of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2976, 156)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "time_len=data.shape[0] # Time sequence length\n",
    "num_nodes=data.shape[1] #Number of Roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the variables\n",
    "output_dim=pre_len=1\n",
    "seq_len=4\n",
    "num_units=100\n",
    "train_rate=0.8\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0980221  0.21353212 0.23823702 ... 0.19194394 0.37707424 0.        ]\n",
      " [0.09032986 0.18181142 0.31845367 ... 0.45834997 0.37705195 0.        ]\n",
      " [0.10192686 0.10389599 0.23464748 ... 0.3856561  0.43354756 0.        ]\n",
      " ...\n",
      " [0.37947276 0.141638   0.10453826 ... 0.13605069 0.1998609  0.        ]\n",
      " [0.39722532 0.18600048 0.121989   ... 0.16306393 0.16957766 0.        ]\n",
      " [0.38101357 0.14001252 0.10420388 ... 0.19054307 0.1845446  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Normalization : Traffic Speed Data\n",
    "\n",
    "data1 =np.mat(data,dtype=np.float32)\n",
    "\n",
    "max_value = np.max(data1)\n",
    "data1  = data1/max_value\n",
    "print(data1)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2380, 156) -----> (596, 156)\n",
      "Train Test Split Details :\n",
      "Train x ---->  2375\n",
      "Train y ---->  2375\n",
      "(2375, 4, 156)\n",
      "Test x ---->  591\n",
      "Test y ---->  591\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(data, time_len, rate, seq_len, pre_len):\n",
    "    train_size = int(time_len * rate) #2976 *0.8 =2380\n",
    "    train_data = data[0:train_size] #  [0:2380]\n",
    "    test_data = data[train_size:time_len] #[2380:2976]\n",
    "    print(train_data.shape,'----->',test_data.shape)\n",
    "\n",
    "    trainX, trainY, testX, testY = [], [], [], []\n",
    "    for i in range(len(train_data) - seq_len - pre_len): #(2380-4-1)=2375\n",
    "        a = train_data[i: i + seq_len + pre_len] #[0:0+4+1] \n",
    "        trainX.append(a[0 : seq_len]) #a[0:4] 4 time * 156 roads\n",
    "        trainY.append(a[seq_len : seq_len + pre_len]) #a[4:4+1] 1 time*156 \n",
    "    for i in range(len(test_data) - seq_len -pre_len):\n",
    "        b = test_data[i: i + seq_len + pre_len]\n",
    "        testX.append(b[0 : seq_len])\n",
    "        testY.append(b[seq_len : seq_len + pre_len])\n",
    "      \n",
    "    trainX1 = np.array(trainX) \n",
    "    trainY1 = np.array(trainY)\n",
    "    testX1 = np.array(testX)\n",
    "    testY1 = np.array(testY)\n",
    "    return trainX1, trainY1, testX1, testY1\n",
    "\n",
    "trainX, trainY, testX, testY = preprocess_data(data1, time_len, train_rate, seq_len, pre_len)\n",
    "\n",
    "totalbatch = int(trainX.shape[0]/batch_size)\n",
    "training_data_count = len(trainX)  \n",
    "print('Train Test Split Details :')\n",
    "print('Train x ----> ',len(trainX))\n",
    "print('Train y ----> ',len(trainY))\n",
    "print(trainX.shape)\n",
    "print('Test x ----> ',len(testX))\n",
    "print('Test y ----> ',len(testY))\n",
    "#print('\\nTrain Sample Details :')\n",
    "#print(trainX[0],'--->',trainY[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "        init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "        initial = tf.random_uniform([input_dim, output_dim], minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "        return tf.Variable(initial,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stgcnCell(RNNCell):\n",
    "    \"\"\"Temporal Graph Convolutional Network \"\"\"\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, num_units, adj, num_nodes, input_size=None,\n",
    "                 act=tf.nn.tanh, reuse=None):\n",
    "\n",
    "        super(stgcnCell, self).__init__(_reuse=reuse)\n",
    "        self._act = act\n",
    "        self._nodes = num_nodes\n",
    "        self._units = num_units\n",
    "        self._adj = []\n",
    "        self._adj.append(self.calculate_laplacian(adj))\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_sparse_matrix(L):\n",
    "        L = L.tocoo()\n",
    "        indices = np.column_stack((L.row, L.col))\n",
    "        L = tf.SparseTensor(indices, L.data, L.shape)\n",
    "        return tf.sparse_reorder(L)\n",
    "\n",
    "    def calculate_laplacian(self,adj, lambda_max=1):  \n",
    "        adj = self.normalized_adj(adj + sp.eye(adj.shape[0])) # normalisation(self identity matrix + adj)\n",
    "        adj = sp.csr_matrix(adj) #compressed sparse matrix\n",
    "        adj = adj.astype(np.float32)\n",
    "        return self.sparse_to_tuple(adj)\n",
    "    \n",
    "    def normalized_adj(self,adj):\n",
    "        adj = sp.coo_matrix(adj)\n",
    "        degree = np.array(adj.sum(1)) # Degree Matrix row wise sum\n",
    "        d_inv_sqrt = np.power(degree, -0.5).flatten() # D inv = Degree ^-0.5 \n",
    "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt) #substitution of the 1D array degree in a 2D matrix diagonals\n",
    "        normalized_adj = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo() # norm= D^-0.5 * adj * D^-0.5\n",
    "        normalized_adj = normalized_adj.astype(np.float32) \n",
    "        return normalized_adj\n",
    "    \n",
    "    def sparse_to_tuple(self,mx):\n",
    "        mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose() #coordinate stacking row and column wise and transpose\n",
    "        L = tf.SparseTensor(coords, mx.data, mx.shape) # mx.shape= (156,156)\n",
    "        #print('shape ---->',mx.shape)\n",
    "        return tf.sparse_reorder(L) #row major ordering\n",
    "        \n",
    "    def init_state(self,batch_size):       \n",
    "        state = tf.zeros(shape=[batch_size, self._num_nodes*self._num_units], dtype=tf.float32)\n",
    "        return state  \n",
    "               \n",
    "    @staticmethod\n",
    "    def _concat(x, x_):\n",
    "        x_ = tf.expand_dims(x_, 0)\n",
    "        return tf.concat([x, x_], axis=0)   \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._nodes * self._units\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._units\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "\n",
    "        with tf.variable_scope(scope or \"tgcn\"):\n",
    "            with tf.variable_scope(\"gates\"):  \n",
    "                value = tf.nn.sigmoid(\n",
    "                    self._gc(inputs, state, 2 * self._units, bias=1.0, scope=scope)) #ut (or) rt = sigma(Wu [f(A;Xt); h{t-1}] + bu)\n",
    "                r, u = tf.split(value=value, num_or_size_splits=2, axis=1)\n",
    "            with tf.variable_scope(\"candidate\"):\n",
    "                r_state = r * state #r* h{t-1}\n",
    "                c = self._act(self._gc(inputs, r_state, self._units, scope=scope))#ct = tanh(Wc [f(A;Xt); r_state] + bc) \n",
    "            new_h = u * state + (1 - u) * c #ht = ut * h{t-1} + (1 - u{t}) * ct\n",
    "        return new_h, new_h\n",
    "\n",
    "\n",
    "    def _gc(self, inputs, state, output_size, bias=0.0, scope=None):\n",
    "        ## inputs:(-1,num_nodes)\n",
    "        inputs = tf.expand_dims(inputs, 2)#None,156,None\n",
    "        ## state:(batch,num_node,gru_units)\n",
    "        state = tf.reshape(state, (-1, self._nodes, self._units)) #32,156,64\n",
    "        ## concat\n",
    "        x_s = tf.concat([inputs, state], axis=2) #32,156,65\n",
    "        input_size = x_s.get_shape()[2].value #65\n",
    "        ## (num_node,input_size,-1)\n",
    "        x0 = tf.transpose(x_s, perm=[1, 2, 0]) #156,65,32\n",
    "        x0 = tf.reshape(x0, shape=[self._nodes, -1]) #156,65*32\n",
    "        scope = tf.get_variable_scope()\n",
    "        with tf.variable_scope(scope):\n",
    "            for m in self._adj:#1,156\n",
    "                x1 = tf.sparse_tensor_dense_matmul(m, x0) #1,65*32\n",
    "#                print(x1)\n",
    "            x = tf.reshape(x1, shape=[self._nodes, input_size,-1]) #156,65,32\n",
    "            x = tf.transpose(x,perm=[2,0,1]) #32,156,65\n",
    "            x = tf.reshape(x, shape=[-1, input_size]) #156*32,65\n",
    "            weights = tf.get_variable( # 65,64\n",
    "                'weights', [input_size, output_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            x = tf.matmul(x, weights)  # (batch_size * self._nodes, output_size) \n",
    "            biases = tf.get_variable( #64\n",
    "                \"biases\", [output_size], initializer=tf.constant_initializer(bias, dtype=tf.float32))\n",
    "            x = tf.nn.bias_add(x, biases) #biases added\n",
    "            x = tf.reshape(x, shape=[-1, self._nodes, output_size]) #32,156,64\n",
    "            x = tf.reshape(x, shape=[-1, self._nodes * output_size])#32,156*64\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tf.convert_to_tensor(trainX, np.float32)\n",
    "#labels = tf.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\n",
    "#g=stgcnCell(num_units, adj, inputs, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STGCN(_X, _weights, _biases):\n",
    "    ###\n",
    "    cell_1 = stgcnCell(num_units, adj, num_nodes=num_nodes)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([cell_1], state_is_tuple=True) #stack rnn cells\n",
    "    _X = tf.unstack(_X, axis=1) # 4 tensorflow arrays of shape None,156 (seq_len=4)\n",
    "    outputs, states = tf.nn.static_rnn(cell, _X, dtype=tf.float32) #Creates a recurrent neural network specified by RNNCell cell\n",
    "    #4 ouputs and 1 state None,9984(156*64)\n",
    "    m = []\n",
    "    for i in outputs:\n",
    "        o = tf.reshape(i,shape=[-1,num_nodes,num_units])#None,156,64\n",
    "        o = tf.reshape(o,shape=[-1,num_units])#None*156,64\n",
    "        m.append(o) #4 objects\n",
    "    last_output = m[-1] #last one\n",
    "    output = tf.matmul(last_output, _weights['out']) + _biases['out'] #multiply with weights and add bias None*156,1+len(1)=156,1\n",
    "    output = tf.reshape(output,shape=[-1,num_nodes,pre_len]) # None,156,1\n",
    "    output = tf.transpose(output, perm=[0,2,1])#None,1,156\n",
    "    output = tf.reshape(output, shape=[-1,num_nodes]) #None*1,156\n",
    "    return output, m, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Lema Labs ML Workshop x64\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "WARNING:tensorflow:From <ipython-input-12-dfd1196d70e3>:4: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-dfd1196d70e3>:6: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.placeholder(tf.float32, shape=[None, seq_len, num_nodes])\n",
    "labels = tf.placeholder(tf.float32, shape=[None, pre_len, num_nodes])\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_units, pre_len], mean=1.0), name='weight_o')} #64,1\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([pre_len]),name='bias_o')} #1\n",
    "print(type(inputs))\n",
    "pred,ttts,ttto = STGCN(inputs, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_loss = 0.0015\n",
    "Lreg = lambda_loss * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "label = tf.reshape(labels, [-1,num_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.l2_loss(y_pred-label) + Lreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = tf.sqrt(tf.reduce_mean(tf.square(y_pred-label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = tf.global_variables()\n",
    "training_epoch=1000\n",
    "saver = tf.train.Saver(tf.global_variables()) #\n",
    "#sess = tf.Session()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "out = 'out/%s'%(\"STGCN\")\n",
    "#out = 'out/%s_%s'%(model_name,'perturbation')\n",
    "path1 = '%s_%s_lr%r_batch%r_unit%r_seq%r_pre%r_epoch%r'%(\"STGCN\",\"sz\",lr,batch_size,num_units,seq_len,pre_len,training_epoch)\n",
    "path = os.path.join(out,path1)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(a,b):\n",
    "    rmse = math.sqrt(mean_squared_error(a,b))\n",
    "    mae = mean_absolute_error(a, b)\n",
    "    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n",
    "    r2 = 1-((a-b)**2).sum()/((a-a.mean())**2).sum()\n",
    "    var = 1-(np.var(a-b))/np.var(a)\n",
    "    return rmse, mae, 1-F_norm, r2, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axe,batch_loss,batch_rmse,batch_pred = [], [], [], []\n",
    "test_loss,test_rmse,test_mae,test_acc,test_r2,test_var,test_pred = [],[],[],[],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(a,b):  \n",
    "    F_norm = la.norm(a-b,'fro')/la.norm(a,'fro')\n",
    "    train_acc=1-F_norm\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Accuracy ---->  0.5859107971191406\n",
      "Iter:0 train_rmse:5.976 test_loss:250.9 test_rmse:0.0737 test_acc:0.5563\n",
      "Epoch  1\n",
      "Accuracy ---->  0.6035177111625671\n",
      "Iter:1 train_rmse:5.722 test_loss:227.8 test_rmse:0.07021 test_acc:0.5773\n",
      "Epoch  2\n",
      "Accuracy ---->  0.6091001033782959\n",
      "Iter:2 train_rmse:5.641 test_loss:221.7 test_rmse:0.06927 test_acc:0.583\n",
      "Epoch  3\n",
      "Accuracy ---->  0.6127183437347412\n",
      "Iter:3 train_rmse:5.589 test_loss:217.8 test_rmse:0.06866 test_acc:0.5867\n",
      "Epoch  4\n",
      "Accuracy ---->  0.614863932132721\n",
      "Iter:4 train_rmse:5.558 test_loss:215.5 test_rmse:0.06829 test_acc:0.5889\n",
      "Epoch  5\n",
      "Accuracy ---->  0.6160442531108856\n",
      "Iter:5 train_rmse:5.541 test_loss:214.2 test_rmse:0.06808 test_acc:0.5901\n",
      "Epoch  6\n",
      "Accuracy ---->  0.6166784465312958\n",
      "Iter:6 train_rmse:5.532 test_loss:213.5 test_rmse:0.06797 test_acc:0.5908\n",
      "Epoch  7\n",
      "Accuracy ---->  0.6170701682567596\n",
      "Iter:7 train_rmse:5.526 test_loss:213.0 test_rmse:0.0679 test_acc:0.5912\n",
      "Epoch  8\n",
      "Accuracy ---->  0.6174040138721466\n",
      "Iter:8 train_rmse:5.521 test_loss:212.7 test_rmse:0.06784 test_acc:0.5915\n",
      "Epoch  9\n",
      "Accuracy ---->  0.6177749633789062\n",
      "Iter:9 train_rmse:5.516 test_loss:212.3 test_rmse:0.06779 test_acc:0.5919\n",
      "Epoch  10\n",
      "Accuracy ---->  0.6182127594947815\n",
      "Iter:10 train_rmse:5.509 test_loss:211.9 test_rmse:0.06772 test_acc:0.5923\n",
      "Epoch  11\n",
      "Accuracy ---->  0.6187050044536591\n",
      "Iter:11 train_rmse:5.502 test_loss:211.5 test_rmse:0.06765 test_acc:0.5927\n",
      "Epoch  12\n",
      "Accuracy ---->  0.619235098361969\n",
      "Iter:12 train_rmse:5.495 test_loss:211.0 test_rmse:0.06758 test_acc:0.5931\n",
      "Epoch  13\n",
      "Accuracy ---->  0.6197950839996338\n",
      "Iter:13 train_rmse:5.487 test_loss:210.5 test_rmse:0.0675 test_acc:0.5936\n",
      "Epoch  14\n",
      "Accuracy ---->  0.6203796565532684\n",
      "Iter:14 train_rmse:5.478 test_loss:210.0 test_rmse:0.06741 test_acc:0.5941\n",
      "Epoch  15\n",
      "Accuracy ---->  0.6209880709648132\n",
      "Iter:15 train_rmse:5.469 test_loss:209.4 test_rmse:0.06733 test_acc:0.5947\n",
      "Epoch  16\n",
      "Accuracy ---->  0.6216301321983337\n",
      "Iter:16 train_rmse:5.46 test_loss:208.9 test_rmse:0.06723 test_acc:0.5952\n",
      "Epoch  17\n",
      "Accuracy ---->  0.6223145127296448\n",
      "Iter:17 train_rmse:5.45 test_loss:208.3 test_rmse:0.06714 test_acc:0.5958\n",
      "Epoch  18\n",
      "Accuracy ---->  0.623043417930603\n",
      "Iter:18 train_rmse:5.44 test_loss:207.7 test_rmse:0.06704 test_acc:0.5964\n",
      "Epoch  19\n",
      "Accuracy ---->  0.6238135099411011\n",
      "Iter:19 train_rmse:5.429 test_loss:207.0 test_rmse:0.06694 test_acc:0.597\n",
      "Epoch  20\n",
      "Accuracy ---->  0.624608725309372\n",
      "Iter:20 train_rmse:5.417 test_loss:206.4 test_rmse:0.06684 test_acc:0.5976\n",
      "Epoch  21\n",
      "Accuracy ---->  0.6254129409790039\n",
      "Iter:21 train_rmse:5.406 test_loss:205.8 test_rmse:0.06673 test_acc:0.5982\n",
      "Epoch  22\n",
      "Accuracy ---->  0.626217395067215\n",
      "Iter:22 train_rmse:5.394 test_loss:205.1 test_rmse:0.06663 test_acc:0.5989\n",
      "Epoch  23\n",
      "Accuracy ---->  0.6270131766796112\n",
      "Iter:23 train_rmse:5.382 test_loss:204.5 test_rmse:0.06653 test_acc:0.5995\n",
      "Epoch  24\n",
      "Accuracy ---->  0.6278104782104492\n",
      "Iter:24 train_rmse:5.371 test_loss:203.9 test_rmse:0.06642 test_acc:0.6001\n",
      "Epoch  25\n",
      "Accuracy ---->  0.6286230981349945\n",
      "Iter:25 train_rmse:5.359 test_loss:203.3 test_rmse:0.06633 test_acc:0.6007\n",
      "Epoch  26\n",
      "Accuracy ---->  0.6294248104095459\n",
      "Iter:26 train_rmse:5.348 test_loss:202.7 test_rmse:0.06624 test_acc:0.6012\n",
      "Epoch  27\n",
      "Accuracy ---->  0.6301896572113037\n",
      "Iter:27 train_rmse:5.337 test_loss:202.2 test_rmse:0.06615 test_acc:0.6017\n",
      "Epoch  28\n",
      "Accuracy ---->  0.6308997273445129\n",
      "Iter:28 train_rmse:5.326 test_loss:201.7 test_rmse:0.06607 test_acc:0.6022\n",
      "Epoch  29\n",
      "Accuracy ---->  0.6315449178218842\n",
      "Iter:29 train_rmse:5.317 test_loss:201.3 test_rmse:0.066 test_acc:0.6026\n",
      "Epoch  30\n",
      "Accuracy ---->  0.6321255266666412\n",
      "Iter:30 train_rmse:5.309 test_loss:200.9 test_rmse:0.06594 test_acc:0.603\n",
      "Epoch  31\n",
      "Accuracy ---->  0.6327056586742401\n",
      "Iter:31 train_rmse:5.3 test_loss:200.5 test_rmse:0.06588 test_acc:0.6034\n",
      "Epoch  32\n",
      "Accuracy ---->  0.633298933506012\n",
      "Iter:32 train_rmse:5.292 test_loss:200.2 test_rmse:0.06582 test_acc:0.6037\n",
      "Epoch  33\n",
      "Accuracy ---->  0.6338337063789368\n",
      "Iter:33 train_rmse:5.284 test_loss:199.9 test_rmse:0.06577 test_acc:0.604\n",
      "Epoch  34\n",
      "Accuracy ---->  0.6343051791191101\n",
      "Iter:34 train_rmse:5.277 test_loss:199.6 test_rmse:0.06573 test_acc:0.6043\n",
      "Epoch  35\n",
      "Accuracy ---->  0.6347420811653137\n",
      "Iter:35 train_rmse:5.271 test_loss:199.3 test_rmse:0.06568 test_acc:0.6046\n",
      "Epoch  36\n",
      "Accuracy ---->  0.6351662278175354\n",
      "Iter:36 train_rmse:5.265 test_loss:199.1 test_rmse:0.06564 test_acc:0.6048\n",
      "Epoch  37\n",
      "Accuracy ---->  0.6355947256088257\n",
      "Iter:37 train_rmse:5.259 test_loss:198.8 test_rmse:0.0656 test_acc:0.6051\n",
      "Epoch  38\n",
      "Accuracy ---->  0.636021614074707\n",
      "Iter:38 train_rmse:5.252 test_loss:198.6 test_rmse:0.06557 test_acc:0.6052\n",
      "Epoch  39\n",
      "Accuracy ---->  0.6363945305347443\n",
      "Iter:39 train_rmse:5.247 test_loss:198.4 test_rmse:0.06553 test_acc:0.6055\n",
      "Epoch  40\n",
      "Accuracy ---->  0.6367390155792236\n",
      "Iter:40 train_rmse:5.242 test_loss:198.1 test_rmse:0.06548 test_acc:0.6058\n",
      "Epoch  41\n",
      "Accuracy ---->  0.6370312571525574\n",
      "Iter:41 train_rmse:5.238 test_loss:197.9 test_rmse:0.06544 test_acc:0.606\n",
      "Epoch  42\n",
      "Accuracy ---->  0.6373278498649597\n",
      "Iter:42 train_rmse:5.234 test_loss:197.7 test_rmse:0.06541 test_acc:0.6062\n",
      "Epoch  43\n",
      "Accuracy ---->  0.6376825571060181\n",
      "Iter:43 train_rmse:5.228 test_loss:197.6 test_rmse:0.06539 test_acc:0.6063\n",
      "Epoch  44\n",
      "Accuracy ---->  0.6379733383655548\n",
      "Iter:44 train_rmse:5.224 test_loss:197.3 test_rmse:0.06534 test_acc:0.6066\n",
      "Epoch  45\n",
      "Accuracy ---->  0.6382681429386139\n",
      "Iter:45 train_rmse:5.22 test_loss:197.0 test_rmse:0.06529 test_acc:0.6069\n",
      "Epoch  46\n",
      "Accuracy ---->  0.6385610997676849\n",
      "Iter:46 train_rmse:5.216 test_loss:196.8 test_rmse:0.06525 test_acc:0.6072\n",
      "Epoch  47\n",
      "Accuracy ---->  0.6388509273529053\n",
      "Iter:47 train_rmse:5.212 test_loss:196.5 test_rmse:0.06522 test_acc:0.6074\n",
      "Epoch  48\n",
      "Accuracy ---->  0.63915154337883\n",
      "Iter:48 train_rmse:5.207 test_loss:196.3 test_rmse:0.06517 test_acc:0.6076\n",
      "Epoch  49\n",
      "Accuracy ---->  0.6394552886486053\n",
      "Iter:49 train_rmse:5.203 test_loss:196.0 test_rmse:0.06513 test_acc:0.6079\n",
      "Epoch  50\n",
      "Accuracy ---->  0.6397404372692108\n",
      "Iter:50 train_rmse:5.199 test_loss:195.8 test_rmse:0.06508 test_acc:0.6082\n",
      "Epoch  51\n",
      "Accuracy ---->  0.6400496065616608\n",
      "Iter:51 train_rmse:5.194 test_loss:195.5 test_rmse:0.06504 test_acc:0.6084\n",
      "Epoch  52\n",
      "Accuracy ---->  0.6403959691524506\n",
      "Iter:52 train_rmse:5.189 test_loss:195.2 test_rmse:0.065 test_acc:0.6087\n",
      "Epoch  53\n",
      "Accuracy ---->  0.6407188475131989\n",
      "Iter:53 train_rmse:5.185 test_loss:194.9 test_rmse:0.06494 test_acc:0.609\n",
      "Epoch  54\n",
      "Accuracy ---->  0.6410403549671173\n",
      "Iter:54 train_rmse:5.18 test_loss:194.7 test_rmse:0.0649 test_acc:0.6093\n",
      "Epoch  55\n",
      "Accuracy ---->  0.6413110196590424\n",
      "Iter:55 train_rmse:5.176 test_loss:194.5 test_rmse:0.06486 test_acc:0.6095\n",
      "Epoch  56\n",
      "Accuracy ---->  0.641408771276474\n",
      "Iter:56 train_rmse:5.175 test_loss:194.2 test_rmse:0.06482 test_acc:0.6098\n",
      "Epoch  57\n",
      "Accuracy ---->  0.6414517760276794\n",
      "Iter:57 train_rmse:5.174 test_loss:193.8 test_rmse:0.06476 test_acc:0.6101\n",
      "Epoch  58\n",
      "Accuracy ---->  0.6416092813014984\n",
      "Iter:58 train_rmse:5.172 test_loss:193.5 test_rmse:0.0647 test_acc:0.6105\n",
      "Epoch  59\n",
      "Accuracy ---->  0.6419106423854828\n",
      "Iter:59 train_rmse:5.167 test_loss:193.2 test_rmse:0.06466 test_acc:0.6107\n",
      "Epoch  60\n",
      "Accuracy ---->  0.6422649323940277\n",
      "Iter:60 train_rmse:5.162 test_loss:193.0 test_rmse:0.06462 test_acc:0.611\n",
      "Epoch  61\n",
      "Accuracy ---->  0.6425991952419281\n",
      "Iter:61 train_rmse:5.158 test_loss:192.7 test_rmse:0.06457 test_acc:0.6112\n",
      "Epoch  62\n",
      "Accuracy ---->  0.642901211977005\n",
      "Iter:62 train_rmse:5.153 test_loss:192.5 test_rmse:0.06453 test_acc:0.6115\n",
      "Epoch  63\n",
      "Accuracy ---->  0.643182098865509\n",
      "Iter:63 train_rmse:5.149 test_loss:192.2 test_rmse:0.06448 test_acc:0.6118\n",
      "Epoch  64\n",
      "Accuracy ---->  0.6434543132781982\n",
      "Iter:64 train_rmse:5.145 test_loss:192.0 test_rmse:0.06444 test_acc:0.612\n",
      "Epoch  65\n",
      "Accuracy ---->  0.6437264084815979\n",
      "Iter:65 train_rmse:5.141 test_loss:191.7 test_rmse:0.0644 test_acc:0.6123\n",
      "Epoch  66\n",
      "Accuracy ---->  0.6440052390098572\n",
      "Iter:66 train_rmse:5.137 test_loss:191.5 test_rmse:0.06435 test_acc:0.6126\n",
      "Epoch  67\n",
      "Accuracy ---->  0.6442961096763611\n",
      "Iter:67 train_rmse:5.133 test_loss:191.2 test_rmse:0.06431 test_acc:0.6128\n",
      "Epoch  68\n",
      "Accuracy ---->  0.6446037590503693\n",
      "Iter:68 train_rmse:5.129 test_loss:190.9 test_rmse:0.06427 test_acc:0.6131\n",
      "Epoch  69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.6449323296546936\n",
      "Iter:69 train_rmse:5.124 test_loss:190.7 test_rmse:0.06422 test_acc:0.6134\n",
      "Epoch  70\n",
      "Accuracy ---->  0.6452835500240326\n",
      "Iter:70 train_rmse:5.119 test_loss:190.4 test_rmse:0.06417 test_acc:0.6136\n",
      "Epoch  71\n",
      "Accuracy ---->  0.6456573605537415\n",
      "Iter:71 train_rmse:5.113 test_loss:190.1 test_rmse:0.06413 test_acc:0.6139\n",
      "Epoch  72\n",
      "Accuracy ---->  0.6460505127906799\n",
      "Iter:72 train_rmse:5.108 test_loss:189.8 test_rmse:0.06408 test_acc:0.6142\n",
      "Epoch  73\n",
      "Accuracy ---->  0.6464574933052063\n",
      "Iter:73 train_rmse:5.102 test_loss:189.5 test_rmse:0.06402 test_acc:0.6146\n",
      "Epoch  74\n",
      "Accuracy ---->  0.6468730270862579\n",
      "Iter:74 train_rmse:5.096 test_loss:189.2 test_rmse:0.06396 test_acc:0.6149\n",
      "Epoch  75\n",
      "Accuracy ---->  0.6472928524017334\n",
      "Iter:75 train_rmse:5.09 test_loss:188.8 test_rmse:0.0639 test_acc:0.6153\n",
      "Epoch  76\n",
      "Accuracy ---->  0.6477153599262238\n",
      "Iter:76 train_rmse:5.084 test_loss:188.4 test_rmse:0.06384 test_acc:0.6157\n",
      "Epoch  77\n",
      "Accuracy ---->  0.6481410264968872\n",
      "Iter:77 train_rmse:5.078 test_loss:188.0 test_rmse:0.06377 test_acc:0.6161\n",
      "Epoch  78\n",
      "Accuracy ---->  0.6485713422298431\n",
      "Iter:78 train_rmse:5.071 test_loss:187.6 test_rmse:0.0637 test_acc:0.6165\n",
      "Epoch  79\n",
      "Accuracy ---->  0.6490077674388885\n",
      "Iter:79 train_rmse:5.065 test_loss:187.2 test_rmse:0.06363 test_acc:0.6169\n",
      "Epoch  80\n",
      "Accuracy ---->  0.6494515538215637\n",
      "Iter:80 train_rmse:5.059 test_loss:186.8 test_rmse:0.06355 test_acc:0.6174\n",
      "Epoch  81\n",
      "Accuracy ---->  0.6499038338661194\n",
      "Iter:81 train_rmse:5.052 test_loss:186.3 test_rmse:0.06348 test_acc:0.6178\n",
      "Epoch  82\n",
      "Accuracy ---->  0.6503648459911346\n",
      "Iter:82 train_rmse:5.045 test_loss:185.9 test_rmse:0.0634 test_acc:0.6183\n",
      "Epoch  83\n",
      "Accuracy ---->  0.6508349180221558\n",
      "Iter:83 train_rmse:5.039 test_loss:185.4 test_rmse:0.06331 test_acc:0.6188\n",
      "Epoch  84\n",
      "Accuracy ---->  0.6513142883777618\n",
      "Iter:84 train_rmse:5.032 test_loss:184.9 test_rmse:0.06323 test_acc:0.6193\n",
      "Epoch  85\n",
      "Accuracy ---->  0.6518026888370514\n",
      "Iter:85 train_rmse:5.025 test_loss:184.4 test_rmse:0.06314 test_acc:0.6199\n",
      "Epoch  86\n",
      "Accuracy ---->  0.6522999703884125\n",
      "Iter:86 train_rmse:5.018 test_loss:183.8 test_rmse:0.06304 test_acc:0.6204\n",
      "Epoch  87\n",
      "Accuracy ---->  0.6528059244155884\n",
      "Iter:87 train_rmse:5.01 test_loss:183.3 test_rmse:0.06295 test_acc:0.621\n",
      "Epoch  88\n",
      "Accuracy ---->  0.6533201634883881\n",
      "Iter:88 train_rmse:5.003 test_loss:182.7 test_rmse:0.06285 test_acc:0.6216\n",
      "Epoch  89\n",
      "Accuracy ---->  0.6538426876068115\n",
      "Iter:89 train_rmse:4.995 test_loss:182.2 test_rmse:0.06275 test_acc:0.6222\n",
      "Epoch  90\n",
      "Accuracy ---->  0.6543731689453125\n",
      "Iter:90 train_rmse:4.988 test_loss:181.6 test_rmse:0.06265 test_acc:0.6228\n",
      "Epoch  91\n",
      "Accuracy ---->  0.6549115478992462\n",
      "Iter:91 train_rmse:4.98 test_loss:181.0 test_rmse:0.06255 test_acc:0.6234\n",
      "Epoch  92\n",
      "Accuracy ---->  0.6554584503173828\n",
      "Iter:92 train_rmse:4.972 test_loss:180.4 test_rmse:0.06245 test_acc:0.624\n",
      "Epoch  93\n",
      "Accuracy ---->  0.6560132503509521\n",
      "Iter:93 train_rmse:4.964 test_loss:179.8 test_rmse:0.06235 test_acc:0.6246\n",
      "Epoch  94\n",
      "Accuracy ---->  0.6565763652324677\n",
      "Iter:94 train_rmse:4.956 test_loss:179.3 test_rmse:0.06225 test_acc:0.6252\n",
      "Epoch  95\n",
      "Accuracy ---->  0.6571471691131592\n",
      "Iter:95 train_rmse:4.948 test_loss:178.7 test_rmse:0.06215 test_acc:0.6258\n",
      "Epoch  96\n",
      "Accuracy ---->  0.6577256619930267\n",
      "Iter:96 train_rmse:4.939 test_loss:178.1 test_rmse:0.06205 test_acc:0.6264\n",
      "Epoch  97\n",
      "Accuracy ---->  0.6583104133605957\n",
      "Iter:97 train_rmse:4.931 test_loss:177.6 test_rmse:0.06196 test_acc:0.627\n",
      "Epoch  98\n",
      "Accuracy ---->  0.6589000821113586\n",
      "Iter:98 train_rmse:4.922 test_loss:177.1 test_rmse:0.06187 test_acc:0.6275\n",
      "Epoch  99\n",
      "Accuracy ---->  0.6594925224781036\n",
      "Iter:99 train_rmse:4.914 test_loss:176.6 test_rmse:0.06178 test_acc:0.6281\n",
      "Epoch  100\n",
      "Accuracy ---->  0.6600854098796844\n",
      "Iter:100 train_rmse:4.905 test_loss:176.1 test_rmse:0.06169 test_acc:0.6286\n",
      "Epoch  101\n",
      "Accuracy ---->  0.6606758236885071\n",
      "Iter:101 train_rmse:4.897 test_loss:175.6 test_rmse:0.06161 test_acc:0.6291\n",
      "Epoch  102\n",
      "Accuracy ---->  0.6612607836723328\n",
      "Iter:102 train_rmse:4.888 test_loss:175.2 test_rmse:0.06153 test_acc:0.6296\n",
      "Epoch  103\n",
      "Accuracy ---->  0.6618377864360809\n",
      "Iter:103 train_rmse:4.88 test_loss:174.7 test_rmse:0.06145 test_acc:0.6301\n",
      "Epoch  104\n",
      "Accuracy ---->  0.6624038815498352\n",
      "Iter:104 train_rmse:4.872 test_loss:174.3 test_rmse:0.06137 test_acc:0.6305\n",
      "Epoch  105\n",
      "Accuracy ---->  0.6629568636417389\n",
      "Iter:105 train_rmse:4.864 test_loss:173.8 test_rmse:0.06129 test_acc:0.631\n",
      "Epoch  106\n",
      "Accuracy ---->  0.6634951233863831\n",
      "Iter:106 train_rmse:4.856 test_loss:173.4 test_rmse:0.06121 test_acc:0.6315\n",
      "Epoch  107\n",
      "Accuracy ---->  0.664017528295517\n",
      "Iter:107 train_rmse:4.848 test_loss:173.0 test_rmse:0.06114 test_acc:0.6319\n",
      "Epoch  108\n",
      "Accuracy ---->  0.6645230054855347\n",
      "Iter:108 train_rmse:4.841 test_loss:172.5 test_rmse:0.06106 test_acc:0.6324\n",
      "Epoch  109\n",
      "Accuracy ---->  0.6650106608867645\n",
      "Iter:109 train_rmse:4.834 test_loss:172.1 test_rmse:0.06099 test_acc:0.6328\n",
      "Epoch  110\n",
      "Accuracy ---->  0.6654811203479767\n",
      "Iter:110 train_rmse:4.827 test_loss:171.7 test_rmse:0.06092 test_acc:0.6333\n",
      "Epoch  111\n",
      "Accuracy ---->  0.6659336388111115\n",
      "Iter:111 train_rmse:4.821 test_loss:171.3 test_rmse:0.06085 test_acc:0.6337\n",
      "Epoch  112\n",
      "Accuracy ---->  0.6663691103458405\n",
      "Iter:112 train_rmse:4.815 test_loss:170.9 test_rmse:0.06078 test_acc:0.6341\n",
      "Epoch  113\n",
      "Accuracy ---->  0.6667880117893219\n",
      "Iter:113 train_rmse:4.808 test_loss:170.6 test_rmse:0.06071 test_acc:0.6345\n",
      "Epoch  114\n",
      "Accuracy ---->  0.6671909391880035\n",
      "Iter:114 train_rmse:4.803 test_loss:170.2 test_rmse:0.06064 test_acc:0.6349\n",
      "Epoch  115\n",
      "Accuracy ---->  0.6675796210765839\n",
      "Iter:115 train_rmse:4.797 test_loss:169.8 test_rmse:0.06058 test_acc:0.6353\n",
      "Epoch  116\n",
      "Accuracy ---->  0.6679546236991882\n",
      "Iter:116 train_rmse:4.792 test_loss:169.5 test_rmse:0.06052 test_acc:0.6357\n",
      "Epoch  117\n",
      "Accuracy ---->  0.6683181524276733\n",
      "Iter:117 train_rmse:4.786 test_loss:169.1 test_rmse:0.06045 test_acc:0.636\n",
      "Epoch  118\n",
      "Accuracy ---->  0.6686718463897705\n",
      "Iter:118 train_rmse:4.781 test_loss:168.8 test_rmse:0.06039 test_acc:0.6364\n",
      "Epoch  119\n",
      "Accuracy ---->  0.6690176427364349\n",
      "Iter:119 train_rmse:4.776 test_loss:168.5 test_rmse:0.06033 test_acc:0.6368\n",
      "Epoch  120\n",
      "Accuracy ---->  0.669357419013977\n",
      "Iter:120 train_rmse:4.771 test_loss:168.1 test_rmse:0.06028 test_acc:0.6371\n",
      "Epoch  121\n",
      "Accuracy ---->  0.669692724943161\n",
      "Iter:121 train_rmse:4.767 test_loss:167.8 test_rmse:0.06022 test_acc:0.6375\n",
      "Epoch  122\n",
      "Accuracy ---->  0.670025646686554\n",
      "Iter:122 train_rmse:4.762 test_loss:167.5 test_rmse:0.06016 test_acc:0.6378\n",
      "Epoch  123\n",
      "Accuracy ---->  0.6703569889068604\n",
      "Iter:123 train_rmse:4.757 test_loss:167.2 test_rmse:0.06011 test_acc:0.6381\n",
      "Epoch  124\n",
      "Accuracy ---->  0.6706880629062653\n",
      "Iter:124 train_rmse:4.752 test_loss:166.9 test_rmse:0.06005 test_acc:0.6384\n",
      "Epoch  125\n",
      "Accuracy ---->  0.6710198521614075\n",
      "Iter:125 train_rmse:4.747 test_loss:166.6 test_rmse:0.06 test_acc:0.6388\n",
      "Epoch  126\n",
      "Accuracy ---->  0.6713525056838989\n",
      "Iter:126 train_rmse:4.743 test_loss:166.3 test_rmse:0.05995 test_acc:0.6391\n",
      "Epoch  127\n",
      "Accuracy ---->  0.6716865003108978\n",
      "Iter:127 train_rmse:4.738 test_loss:166.0 test_rmse:0.0599 test_acc:0.6394\n",
      "Epoch  128\n",
      "Accuracy ---->  0.672021746635437\n",
      "Iter:128 train_rmse:4.733 test_loss:165.8 test_rmse:0.05984 test_acc:0.6397\n",
      "Epoch  129\n",
      "Accuracy ---->  0.6723580360412598\n",
      "Iter:129 train_rmse:4.728 test_loss:165.5 test_rmse:0.05979 test_acc:0.64\n",
      "Epoch  130\n",
      "Accuracy ---->  0.672694981098175\n",
      "Iter:130 train_rmse:4.723 test_loss:165.2 test_rmse:0.05975 test_acc:0.6403\n",
      "Epoch  131\n",
      "Accuracy ---->  0.6730318367481232\n",
      "Iter:131 train_rmse:4.718 test_loss:164.9 test_rmse:0.0597 test_acc:0.6406\n",
      "Epoch  132\n",
      "Accuracy ---->  0.6733682155609131\n",
      "Iter:132 train_rmse:4.714 test_loss:164.7 test_rmse:0.05965 test_acc:0.6409\n",
      "Epoch  133\n",
      "Accuracy ---->  0.6737028956413269\n",
      "Iter:133 train_rmse:4.709 test_loss:164.4 test_rmse:0.0596 test_acc:0.6412\n",
      "Epoch  134\n",
      "Accuracy ---->  0.6740351319313049\n",
      "Iter:134 train_rmse:4.704 test_loss:164.2 test_rmse:0.05956 test_acc:0.6414\n",
      "Epoch  135\n",
      "Accuracy ---->  0.6743642091751099\n",
      "Iter:135 train_rmse:4.699 test_loss:163.9 test_rmse:0.05952 test_acc:0.6417\n",
      "Epoch  136\n",
      "Accuracy ---->  0.6746889352798462\n",
      "Iter:136 train_rmse:4.694 test_loss:163.7 test_rmse:0.05947 test_acc:0.6419\n",
      "Epoch  137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.6750093400478363\n",
      "Iter:137 train_rmse:4.69 test_loss:163.5 test_rmse:0.05943 test_acc:0.6422\n",
      "Epoch  138\n",
      "Accuracy ---->  0.6753245890140533\n",
      "Iter:138 train_rmse:4.685 test_loss:163.3 test_rmse:0.05939 test_acc:0.6424\n",
      "Epoch  139\n",
      "Accuracy ---->  0.6756345629692078\n",
      "Iter:139 train_rmse:4.681 test_loss:163.0 test_rmse:0.05935 test_acc:0.6427\n",
      "Epoch  140\n",
      "Accuracy ---->  0.6759397089481354\n",
      "Iter:140 train_rmse:4.676 test_loss:162.8 test_rmse:0.05931 test_acc:0.6429\n",
      "Epoch  141\n",
      "Accuracy ---->  0.6762398779392242\n",
      "Iter:141 train_rmse:4.672 test_loss:162.6 test_rmse:0.05927 test_acc:0.6432\n",
      "Epoch  142\n",
      "Accuracy ---->  0.6765362322330475\n",
      "Iter:142 train_rmse:4.668 test_loss:162.4 test_rmse:0.05923 test_acc:0.6434\n",
      "Epoch  143\n",
      "Accuracy ---->  0.6768288016319275\n",
      "Iter:143 train_rmse:4.664 test_loss:162.2 test_rmse:0.05919 test_acc:0.6437\n",
      "Epoch  144\n",
      "Accuracy ---->  0.6771181523799896\n",
      "Iter:144 train_rmse:4.659 test_loss:161.9 test_rmse:0.05915 test_acc:0.6439\n",
      "Epoch  145\n",
      "Accuracy ---->  0.6774052381515503\n",
      "Iter:145 train_rmse:4.655 test_loss:161.7 test_rmse:0.0591 test_acc:0.6442\n",
      "Epoch  146\n",
      "Accuracy ---->  0.6776898503303528\n",
      "Iter:146 train_rmse:4.651 test_loss:161.5 test_rmse:0.05906 test_acc:0.6444\n",
      "Epoch  147\n",
      "Accuracy ---->  0.677971750497818\n",
      "Iter:147 train_rmse:4.647 test_loss:161.2 test_rmse:0.05902 test_acc:0.6447\n",
      "Epoch  148\n",
      "Accuracy ---->  0.6782512962818146\n",
      "Iter:148 train_rmse:4.643 test_loss:161.0 test_rmse:0.05897 test_acc:0.645\n",
      "Epoch  149\n",
      "Accuracy ---->  0.6785278916358948\n",
      "Iter:149 train_rmse:4.639 test_loss:160.7 test_rmse:0.05893 test_acc:0.6452\n",
      "Epoch  150\n",
      "Accuracy ---->  0.6788011193275452\n",
      "Iter:150 train_rmse:4.635 test_loss:160.5 test_rmse:0.05888 test_acc:0.6455\n",
      "Epoch  151\n",
      "Accuracy ---->  0.6790704429149628\n",
      "Iter:151 train_rmse:4.631 test_loss:160.2 test_rmse:0.05884 test_acc:0.6458\n",
      "Epoch  152\n",
      "Accuracy ---->  0.6793356835842133\n",
      "Iter:152 train_rmse:4.627 test_loss:160.0 test_rmse:0.05879 test_acc:0.6461\n",
      "Epoch  153\n",
      "Accuracy ---->  0.6795968413352966\n",
      "Iter:153 train_rmse:4.624 test_loss:159.7 test_rmse:0.05874 test_acc:0.6463\n",
      "Epoch  154\n",
      "Accuracy ---->  0.6798529922962189\n",
      "Iter:154 train_rmse:4.62 test_loss:159.5 test_rmse:0.05869 test_acc:0.6466\n",
      "Epoch  155\n",
      "Accuracy ---->  0.6801047027111053\n",
      "Iter:155 train_rmse:4.616 test_loss:159.2 test_rmse:0.05864 test_acc:0.6469\n",
      "Epoch  156\n",
      "Accuracy ---->  0.6803520619869232\n",
      "Iter:156 train_rmse:4.613 test_loss:158.9 test_rmse:0.0586 test_acc:0.6472\n",
      "Epoch  157\n",
      "Accuracy ---->  0.6805950403213501\n",
      "Iter:157 train_rmse:4.609 test_loss:158.7 test_rmse:0.05855 test_acc:0.6475\n",
      "Epoch  158\n",
      "Accuracy ---->  0.6808346509933472\n",
      "Iter:158 train_rmse:4.606 test_loss:158.4 test_rmse:0.0585 test_acc:0.6478\n",
      "Epoch  159\n",
      "Accuracy ---->  0.6810708940029144\n",
      "Iter:159 train_rmse:4.602 test_loss:158.1 test_rmse:0.05845 test_acc:0.6481\n",
      "Epoch  160\n",
      "Accuracy ---->  0.6813053786754608\n",
      "Iter:160 train_rmse:4.599 test_loss:157.9 test_rmse:0.0584 test_acc:0.6484\n",
      "Epoch  161\n",
      "Accuracy ---->  0.6815379858016968\n",
      "Iter:161 train_rmse:4.596 test_loss:157.6 test_rmse:0.05835 test_acc:0.6487\n",
      "Epoch  162\n",
      "Accuracy ---->  0.6817703247070312\n",
      "Iter:162 train_rmse:4.592 test_loss:157.3 test_rmse:0.0583 test_acc:0.649\n",
      "Epoch  163\n",
      "Accuracy ---->  0.6820028126239777\n",
      "Iter:163 train_rmse:4.589 test_loss:157.1 test_rmse:0.05825 test_acc:0.6493\n",
      "Epoch  164\n",
      "Accuracy ---->  0.6822363436222076\n",
      "Iter:164 train_rmse:4.586 test_loss:156.8 test_rmse:0.0582 test_acc:0.6496\n",
      "Epoch  165\n",
      "Accuracy ---->  0.6824721693992615\n",
      "Iter:165 train_rmse:4.582 test_loss:156.6 test_rmse:0.05815 test_acc:0.6499\n",
      "Epoch  166\n",
      "Accuracy ---->  0.6827110350131989\n",
      "Iter:166 train_rmse:4.579 test_loss:156.3 test_rmse:0.0581 test_acc:0.6502\n",
      "Epoch  167\n",
      "Accuracy ---->  0.6829531192779541\n",
      "Iter:167 train_rmse:4.575 test_loss:156.0 test_rmse:0.05805 test_acc:0.6505\n",
      "Epoch  168\n",
      "Accuracy ---->  0.6831995844841003\n",
      "Iter:168 train_rmse:4.572 test_loss:155.8 test_rmse:0.05801 test_acc:0.6508\n",
      "Epoch  169\n",
      "Accuracy ---->  0.6834504902362823\n",
      "Iter:169 train_rmse:4.568 test_loss:155.5 test_rmse:0.05796 test_acc:0.6511\n",
      "Epoch  170\n",
      "Accuracy ---->  0.6837065517902374\n",
      "Iter:170 train_rmse:4.564 test_loss:155.3 test_rmse:0.05791 test_acc:0.6513\n",
      "Epoch  171\n",
      "Accuracy ---->  0.683967262506485\n",
      "Iter:171 train_rmse:4.561 test_loss:155.1 test_rmse:0.05787 test_acc:0.6516\n",
      "Epoch  172\n",
      "Accuracy ---->  0.6842333078384399\n",
      "Iter:172 train_rmse:4.557 test_loss:154.8 test_rmse:0.05783 test_acc:0.6519\n",
      "Epoch  173\n",
      "Accuracy ---->  0.6845037043094635\n",
      "Iter:173 train_rmse:4.553 test_loss:154.6 test_rmse:0.05778 test_acc:0.6521\n",
      "Epoch  174\n",
      "Accuracy ---->  0.6847781240940094\n",
      "Iter:174 train_rmse:4.549 test_loss:154.4 test_rmse:0.05774 test_acc:0.6524\n",
      "Epoch  175\n",
      "Accuracy ---->  0.6850554645061493\n",
      "Iter:175 train_rmse:4.545 test_loss:154.2 test_rmse:0.0577 test_acc:0.6526\n",
      "Epoch  176\n",
      "Accuracy ---->  0.6853342354297638\n",
      "Iter:176 train_rmse:4.541 test_loss:154.0 test_rmse:0.05766 test_acc:0.6528\n",
      "Epoch  177\n",
      "Accuracy ---->  0.6856129169464111\n",
      "Iter:177 train_rmse:4.537 test_loss:153.8 test_rmse:0.05762 test_acc:0.6531\n",
      "Epoch  178\n",
      "Accuracy ---->  0.685889482498169\n",
      "Iter:178 train_rmse:4.533 test_loss:153.6 test_rmse:0.05759 test_acc:0.6533\n",
      "Epoch  179\n",
      "Accuracy ---->  0.6861612796783447\n",
      "Iter:179 train_rmse:4.529 test_loss:153.4 test_rmse:0.05755 test_acc:0.6535\n",
      "Epoch  180\n",
      "Accuracy ---->  0.6864253282546997\n",
      "Iter:180 train_rmse:4.525 test_loss:153.2 test_rmse:0.05752 test_acc:0.6537\n",
      "Epoch  181\n",
      "Accuracy ---->  0.6866785287857056\n",
      "Iter:181 train_rmse:4.521 test_loss:153.0 test_rmse:0.05748 test_acc:0.6539\n",
      "Epoch  182\n",
      "Accuracy ---->  0.6869176030158997\n",
      "Iter:182 train_rmse:4.518 test_loss:152.8 test_rmse:0.05745 test_acc:0.6541\n",
      "Epoch  183\n",
      "Accuracy ---->  0.6871401369571686\n",
      "Iter:183 train_rmse:4.515 test_loss:152.7 test_rmse:0.05742 test_acc:0.6543\n",
      "Epoch  184\n",
      "Accuracy ---->  0.6873439252376556\n",
      "Iter:184 train_rmse:4.512 test_loss:152.5 test_rmse:0.05739 test_acc:0.6545\n",
      "Epoch  185\n",
      "Accuracy ---->  0.6875284314155579\n",
      "Iter:185 train_rmse:4.509 test_loss:152.4 test_rmse:0.05736 test_acc:0.6547\n",
      "Epoch  186\n",
      "Accuracy ---->  0.6876954436302185\n",
      "Iter:186 train_rmse:4.507 test_loss:152.2 test_rmse:0.05733 test_acc:0.6549\n",
      "Epoch  187\n",
      "Accuracy ---->  0.6878471076488495\n",
      "Iter:187 train_rmse:4.505 test_loss:152.0 test_rmse:0.0573 test_acc:0.655\n",
      "Epoch  188\n",
      "Accuracy ---->  0.6879882514476776\n",
      "Iter:188 train_rmse:4.503 test_loss:151.9 test_rmse:0.05727 test_acc:0.6552\n",
      "Epoch  189\n",
      "Accuracy ---->  0.6881230175495148\n",
      "Iter:189 train_rmse:4.501 test_loss:151.7 test_rmse:0.05724 test_acc:0.6554\n",
      "Epoch  190\n",
      "Accuracy ---->  0.6882556974887848\n",
      "Iter:190 train_rmse:4.499 test_loss:151.6 test_rmse:0.05721 test_acc:0.6556\n",
      "Epoch  191\n",
      "Accuracy ---->  0.6883893013000488\n",
      "Iter:191 train_rmse:4.497 test_loss:151.4 test_rmse:0.05717 test_acc:0.6558\n",
      "Epoch  192\n",
      "Accuracy ---->  0.6885257363319397\n",
      "Iter:192 train_rmse:4.495 test_loss:151.2 test_rmse:0.05714 test_acc:0.656\n",
      "Epoch  193\n",
      "Accuracy ---->  0.6886657774448395\n",
      "Iter:193 train_rmse:4.493 test_loss:151.1 test_rmse:0.05711 test_acc:0.6561\n",
      "Epoch  194\n",
      "Accuracy ---->  0.6888099610805511\n",
      "Iter:194 train_rmse:4.491 test_loss:150.9 test_rmse:0.05708 test_acc:0.6563\n",
      "Epoch  195\n",
      "Accuracy ---->  0.6889582574367523\n",
      "Iter:195 train_rmse:4.489 test_loss:150.8 test_rmse:0.05705 test_acc:0.6565\n",
      "Epoch  196\n",
      "Accuracy ---->  0.6891110837459564\n",
      "Iter:196 train_rmse:4.486 test_loss:150.6 test_rmse:0.05703 test_acc:0.6567\n",
      "Epoch  197\n",
      "Accuracy ---->  0.68926802277565\n",
      "Iter:197 train_rmse:4.484 test_loss:150.5 test_rmse:0.057 test_acc:0.6568\n",
      "Epoch  198\n",
      "Accuracy ---->  0.689428836107254\n",
      "Iter:198 train_rmse:4.482 test_loss:150.3 test_rmse:0.05697 test_acc:0.657\n",
      "Epoch  199\n",
      "Accuracy ---->  0.6895938515663147\n",
      "Iter:199 train_rmse:4.479 test_loss:150.2 test_rmse:0.05694 test_acc:0.6572\n",
      "Epoch  200\n",
      "Accuracy ---->  0.6897624135017395\n",
      "Iter:200 train_rmse:4.477 test_loss:150.1 test_rmse:0.05692 test_acc:0.6573\n",
      "Epoch  201\n",
      "Accuracy ---->  0.6899347901344299\n",
      "Iter:201 train_rmse:4.474 test_loss:149.9 test_rmse:0.0569 test_acc:0.6575\n",
      "Epoch  202\n",
      "Accuracy ---->  0.690110057592392\n",
      "Iter:202 train_rmse:4.472 test_loss:149.8 test_rmse:0.05687 test_acc:0.6576\n",
      "Epoch  203\n",
      "Accuracy ---->  0.6902881264686584\n",
      "Iter:203 train_rmse:4.469 test_loss:149.7 test_rmse:0.05685 test_acc:0.6577\n",
      "Epoch  204\n",
      "Accuracy ---->  0.6904688477516174\n",
      "Iter:204 train_rmse:4.467 test_loss:149.6 test_rmse:0.05683 test_acc:0.6579\n",
      "Epoch  205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.6906516849994659\n",
      "Iter:205 train_rmse:4.464 test_loss:149.5 test_rmse:0.05681 test_acc:0.658\n",
      "Epoch  206\n",
      "Accuracy ---->  0.6908357739448547\n",
      "Iter:206 train_rmse:4.461 test_loss:149.4 test_rmse:0.05679 test_acc:0.6581\n",
      "Epoch  207\n",
      "Accuracy ---->  0.6910208761692047\n",
      "Iter:207 train_rmse:4.459 test_loss:149.3 test_rmse:0.05677 test_acc:0.6582\n",
      "Epoch  208\n",
      "Accuracy ---->  0.6912059485912323\n",
      "Iter:208 train_rmse:4.456 test_loss:149.2 test_rmse:0.05675 test_acc:0.6584\n",
      "Epoch  209\n",
      "Accuracy ---->  0.6913906335830688\n",
      "Iter:209 train_rmse:4.453 test_loss:149.1 test_rmse:0.05673 test_acc:0.6585\n",
      "Epoch  210\n",
      "Accuracy ---->  0.6915735006332397\n",
      "Iter:210 train_rmse:4.451 test_loss:149.0 test_rmse:0.05671 test_acc:0.6586\n",
      "Epoch  211\n",
      "Accuracy ---->  0.6917541921138763\n",
      "Iter:211 train_rmse:4.448 test_loss:148.9 test_rmse:0.05669 test_acc:0.6587\n",
      "Epoch  212\n",
      "Accuracy ---->  0.6919315159320831\n",
      "Iter:212 train_rmse:4.446 test_loss:148.8 test_rmse:0.05667 test_acc:0.6588\n",
      "Epoch  213\n",
      "Accuracy ---->  0.6921045184135437\n",
      "Iter:213 train_rmse:4.443 test_loss:148.7 test_rmse:0.05665 test_acc:0.6589\n",
      "Epoch  214\n",
      "Accuracy ---->  0.6922724545001984\n",
      "Iter:214 train_rmse:4.441 test_loss:148.6 test_rmse:0.05663 test_acc:0.659\n",
      "Epoch  215\n",
      "Accuracy ---->  0.6924344003200531\n",
      "Iter:215 train_rmse:4.438 test_loss:148.5 test_rmse:0.05661 test_acc:0.6592\n",
      "Epoch  216\n",
      "Accuracy ---->  0.6925899684429169\n",
      "Iter:216 train_rmse:4.436 test_loss:148.4 test_rmse:0.05659 test_acc:0.6593\n",
      "Epoch  217\n",
      "Accuracy ---->  0.6927389204502106\n",
      "Iter:217 train_rmse:4.434 test_loss:148.2 test_rmse:0.05657 test_acc:0.6594\n",
      "Epoch  218\n",
      "Accuracy ---->  0.6928813457489014\n",
      "Iter:218 train_rmse:4.432 test_loss:148.1 test_rmse:0.05655 test_acc:0.6595\n",
      "Epoch  219\n",
      "Accuracy ---->  0.6930176317691803\n",
      "Iter:219 train_rmse:4.43 test_loss:148.0 test_rmse:0.05653 test_acc:0.6597\n",
      "Epoch  220\n",
      "Accuracy ---->  0.6931485533714294\n",
      "Iter:220 train_rmse:4.428 test_loss:147.9 test_rmse:0.05651 test_acc:0.6598\n",
      "Epoch  221\n",
      "Accuracy ---->  0.6932748258113861\n",
      "Iter:221 train_rmse:4.426 test_loss:147.8 test_rmse:0.05649 test_acc:0.6599\n",
      "Epoch  222\n",
      "Accuracy ---->  0.6933972239494324\n",
      "Iter:222 train_rmse:4.424 test_loss:147.7 test_rmse:0.05647 test_acc:0.66\n",
      "Epoch  223\n",
      "Accuracy ---->  0.6935165822505951\n",
      "Iter:223 train_rmse:4.423 test_loss:147.6 test_rmse:0.05645 test_acc:0.6602\n",
      "Epoch  224\n",
      "Accuracy ---->  0.6936334669589996\n",
      "Iter:224 train_rmse:4.421 test_loss:147.5 test_rmse:0.05643 test_acc:0.6603\n",
      "Epoch  225\n",
      "Accuracy ---->  0.6937483847141266\n",
      "Iter:225 train_rmse:4.419 test_loss:147.4 test_rmse:0.05641 test_acc:0.6604\n",
      "Epoch  226\n",
      "Accuracy ---->  0.6938615441322327\n",
      "Iter:226 train_rmse:4.418 test_loss:147.3 test_rmse:0.05639 test_acc:0.6605\n",
      "Epoch  227\n",
      "Accuracy ---->  0.6939731538295746\n",
      "Iter:227 train_rmse:4.416 test_loss:147.2 test_rmse:0.05637 test_acc:0.6606\n",
      "Epoch  228\n",
      "Accuracy ---->  0.6940833926200867\n",
      "Iter:228 train_rmse:4.415 test_loss:147.1 test_rmse:0.05635 test_acc:0.6608\n",
      "Epoch  229\n",
      "Accuracy ---->  0.6941921412944794\n",
      "Iter:229 train_rmse:4.413 test_loss:147.0 test_rmse:0.05633 test_acc:0.6609\n",
      "Epoch  230\n",
      "Accuracy ---->  0.6942993700504303\n",
      "Iter:230 train_rmse:4.411 test_loss:146.9 test_rmse:0.05631 test_acc:0.661\n",
      "Epoch  231\n",
      "Accuracy ---->  0.6944049596786499\n",
      "Iter:231 train_rmse:4.41 test_loss:146.8 test_rmse:0.05629 test_acc:0.6611\n",
      "Epoch  232\n",
      "Accuracy ---->  0.694508820772171\n",
      "Iter:232 train_rmse:4.408 test_loss:146.7 test_rmse:0.05627 test_acc:0.6612\n",
      "Epoch  233\n",
      "Accuracy ---->  0.6946106255054474\n",
      "Iter:233 train_rmse:4.407 test_loss:146.6 test_rmse:0.05625 test_acc:0.6613\n",
      "Epoch  234\n",
      "Accuracy ---->  0.6947101652622223\n",
      "Iter:234 train_rmse:4.406 test_loss:146.5 test_rmse:0.05623 test_acc:0.6615\n",
      "Epoch  235\n",
      "Accuracy ---->  0.6948072612285614\n",
      "Iter:235 train_rmse:4.404 test_loss:146.4 test_rmse:0.05621 test_acc:0.6616\n",
      "Epoch  236\n",
      "Accuracy ---->  0.6949016451835632\n",
      "Iter:236 train_rmse:4.403 test_loss:146.3 test_rmse:0.05619 test_acc:0.6617\n",
      "Epoch  237\n",
      "Accuracy ---->  0.6949930191040039\n",
      "Iter:237 train_rmse:4.401 test_loss:146.2 test_rmse:0.05617 test_acc:0.6618\n",
      "Epoch  238\n",
      "Accuracy ---->  0.6950812041759491\n",
      "Iter:238 train_rmse:4.4 test_loss:146.1 test_rmse:0.05615 test_acc:0.6619\n",
      "Epoch  239\n",
      "Accuracy ---->  0.6951660811901093\n",
      "Iter:239 train_rmse:4.399 test_loss:145.9 test_rmse:0.05613 test_acc:0.6621\n",
      "Epoch  240\n",
      "Accuracy ---->  0.6952475309371948\n",
      "Iter:240 train_rmse:4.398 test_loss:145.8 test_rmse:0.05611 test_acc:0.6622\n",
      "Epoch  241\n",
      "Accuracy ---->  0.6953254044055939\n",
      "Iter:241 train_rmse:4.397 test_loss:145.7 test_rmse:0.05609 test_acc:0.6623\n",
      "Epoch  242\n",
      "Accuracy ---->  0.6954001188278198\n",
      "Iter:242 train_rmse:4.396 test_loss:145.6 test_rmse:0.05606 test_acc:0.6625\n",
      "Epoch  243\n",
      "Accuracy ---->  0.6954718232154846\n",
      "Iter:243 train_rmse:4.395 test_loss:145.5 test_rmse:0.05604 test_acc:0.6626\n",
      "Epoch  244\n",
      "Accuracy ---->  0.6955409646034241\n",
      "Iter:244 train_rmse:4.394 test_loss:145.4 test_rmse:0.05602 test_acc:0.6628\n",
      "Epoch  245\n",
      "Accuracy ---->  0.6956081986427307\n",
      "Iter:245 train_rmse:4.393 test_loss:145.2 test_rmse:0.05599 test_acc:0.6629\n",
      "Epoch  246\n",
      "Accuracy ---->  0.6956741213798523\n",
      "Iter:246 train_rmse:4.392 test_loss:145.1 test_rmse:0.05597 test_acc:0.6631\n",
      "Epoch  247\n",
      "Accuracy ---->  0.6957394778728485\n",
      "Iter:247 train_rmse:4.391 test_loss:145.0 test_rmse:0.05594 test_acc:0.6632\n",
      "Epoch  248\n",
      "Accuracy ---->  0.6958048939704895\n",
      "Iter:248 train_rmse:4.39 test_loss:144.8 test_rmse:0.05591 test_acc:0.6634\n",
      "Epoch  249\n",
      "Accuracy ---->  0.6958711445331573\n",
      "Iter:249 train_rmse:4.389 test_loss:144.7 test_rmse:0.05589 test_acc:0.6635\n",
      "Epoch  250\n",
      "Accuracy ---->  0.6959386169910431\n",
      "Iter:250 train_rmse:4.388 test_loss:144.6 test_rmse:0.05586 test_acc:0.6637\n",
      "Epoch  251\n",
      "Accuracy ---->  0.6960077881813049\n",
      "Iter:251 train_rmse:4.387 test_loss:144.4 test_rmse:0.05584 test_acc:0.6638\n",
      "Epoch  252\n",
      "Accuracy ---->  0.6960793733596802\n",
      "Iter:252 train_rmse:4.386 test_loss:144.3 test_rmse:0.05581 test_acc:0.664\n",
      "Epoch  253\n",
      "Accuracy ---->  0.6961534023284912\n",
      "Iter:253 train_rmse:4.385 test_loss:144.2 test_rmse:0.05578 test_acc:0.6642\n",
      "Epoch  254\n",
      "Accuracy ---->  0.6962301135063171\n",
      "Iter:254 train_rmse:4.384 test_loss:144.0 test_rmse:0.05576 test_acc:0.6643\n",
      "Epoch  255\n",
      "Accuracy ---->  0.6963095366954803\n",
      "Iter:255 train_rmse:4.382 test_loss:143.9 test_rmse:0.05573 test_acc:0.6645\n",
      "Epoch  256\n",
      "Accuracy ---->  0.6963919699192047\n",
      "Iter:256 train_rmse:4.381 test_loss:143.8 test_rmse:0.05571 test_acc:0.6646\n",
      "Epoch  257\n",
      "Accuracy ---->  0.6964773237705231\n",
      "Iter:257 train_rmse:4.38 test_loss:143.6 test_rmse:0.05568 test_acc:0.6648\n",
      "Epoch  258\n",
      "Accuracy ---->  0.6965655982494354\n",
      "Iter:258 train_rmse:4.379 test_loss:143.5 test_rmse:0.05566 test_acc:0.6649\n",
      "Epoch  259\n",
      "Accuracy ---->  0.6966568231582642\n",
      "Iter:259 train_rmse:4.377 test_loss:143.4 test_rmse:0.05563 test_acc:0.6651\n",
      "Epoch  260\n",
      "Accuracy ---->  0.6967507600784302\n",
      "Iter:260 train_rmse:4.376 test_loss:143.3 test_rmse:0.05561 test_acc:0.6652\n",
      "Epoch  261\n",
      "Accuracy ---->  0.6968477070331573\n",
      "Iter:261 train_rmse:4.375 test_loss:143.1 test_rmse:0.05558 test_acc:0.6654\n",
      "Epoch  262\n",
      "Accuracy ---->  0.696947455406189\n",
      "Iter:262 train_rmse:4.373 test_loss:143.0 test_rmse:0.05556 test_acc:0.6655\n",
      "Epoch  263\n",
      "Accuracy ---->  0.6970499455928802\n",
      "Iter:263 train_rmse:4.372 test_loss:142.9 test_rmse:0.05554 test_acc:0.6656\n",
      "Epoch  264\n",
      "Accuracy ---->  0.6971553564071655\n",
      "Iter:264 train_rmse:4.37 test_loss:142.8 test_rmse:0.05551 test_acc:0.6658\n",
      "Epoch  265\n",
      "Accuracy ---->  0.6972633898258209\n",
      "Iter:265 train_rmse:4.369 test_loss:142.7 test_rmse:0.05549 test_acc:0.6659\n",
      "Epoch  266\n",
      "Accuracy ---->  0.6973739564418793\n",
      "Iter:266 train_rmse:4.367 test_loss:142.5 test_rmse:0.05547 test_acc:0.6661\n",
      "Epoch  267\n",
      "Accuracy ---->  0.6974874436855316\n",
      "Iter:267 train_rmse:4.365 test_loss:142.4 test_rmse:0.05544 test_acc:0.6662\n",
      "Epoch  268\n",
      "Accuracy ---->  0.6976029574871063\n",
      "Iter:268 train_rmse:4.364 test_loss:142.3 test_rmse:0.05542 test_acc:0.6663\n",
      "Epoch  269\n",
      "Accuracy ---->  0.6977212131023407\n",
      "Iter:269 train_rmse:4.362 test_loss:142.2 test_rmse:0.0554 test_acc:0.6665\n",
      "Epoch  270\n",
      "Accuracy ---->  0.6978417038917542\n",
      "Iter:270 train_rmse:4.36 test_loss:142.1 test_rmse:0.05538 test_acc:0.6666\n",
      "Epoch  271\n",
      "Accuracy ---->  0.6979643106460571\n",
      "Iter:271 train_rmse:4.359 test_loss:142.0 test_rmse:0.05535 test_acc:0.6667\n",
      "Epoch  272\n",
      "Accuracy ---->  0.6980891823768616\n",
      "Iter:272 train_rmse:4.357 test_loss:141.8 test_rmse:0.05533 test_acc:0.6669\n",
      "Epoch  273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.6982159912586212\n",
      "Iter:273 train_rmse:4.355 test_loss:141.7 test_rmse:0.05531 test_acc:0.667\n",
      "Epoch  274\n",
      "Accuracy ---->  0.6983446180820465\n",
      "Iter:274 train_rmse:4.353 test_loss:141.6 test_rmse:0.05529 test_acc:0.6671\n",
      "Epoch  275\n",
      "Accuracy ---->  0.6984750032424927\n",
      "Iter:275 train_rmse:4.351 test_loss:141.5 test_rmse:0.05526 test_acc:0.6673\n",
      "Epoch  276\n",
      "Accuracy ---->  0.6986069679260254\n",
      "Iter:276 train_rmse:4.349 test_loss:141.4 test_rmse:0.05524 test_acc:0.6674\n",
      "Epoch  277\n",
      "Accuracy ---->  0.6987402141094208\n",
      "Iter:277 train_rmse:4.347 test_loss:141.3 test_rmse:0.05522 test_acc:0.6675\n",
      "Epoch  278\n",
      "Accuracy ---->  0.6988749206066132\n",
      "Iter:278 train_rmse:4.345 test_loss:141.2 test_rmse:0.0552 test_acc:0.6677\n",
      "Epoch  279\n",
      "Accuracy ---->  0.6990109086036682\n",
      "Iter:279 train_rmse:4.343 test_loss:141.1 test_rmse:0.05518 test_acc:0.6678\n",
      "Epoch  280\n",
      "Accuracy ---->  0.699147492647171\n",
      "Iter:280 train_rmse:4.342 test_loss:140.9 test_rmse:0.05515 test_acc:0.668\n",
      "Epoch  281\n",
      "Accuracy ---->  0.6992850005626678\n",
      "Iter:281 train_rmse:4.34 test_loss:140.8 test_rmse:0.05513 test_acc:0.6681\n",
      "Epoch  282\n",
      "Accuracy ---->  0.6994233727455139\n",
      "Iter:282 train_rmse:4.338 test_loss:140.7 test_rmse:0.05511 test_acc:0.6682\n",
      "Epoch  283\n",
      "Accuracy ---->  0.6995620131492615\n",
      "Iter:283 train_rmse:4.336 test_loss:140.6 test_rmse:0.05509 test_acc:0.6684\n",
      "Epoch  284\n",
      "Accuracy ---->  0.6997007727622986\n",
      "Iter:284 train_rmse:4.334 test_loss:140.5 test_rmse:0.05506 test_acc:0.6685\n",
      "Epoch  285\n",
      "Accuracy ---->  0.6998399198055267\n",
      "Iter:285 train_rmse:4.332 test_loss:140.4 test_rmse:0.05504 test_acc:0.6686\n",
      "Epoch  286\n",
      "Accuracy ---->  0.6999787390232086\n",
      "Iter:286 train_rmse:4.33 test_loss:140.3 test_rmse:0.05502 test_acc:0.6688\n",
      "Epoch  287\n",
      "Accuracy ---->  0.7001176476478577\n",
      "Iter:287 train_rmse:4.328 test_loss:140.2 test_rmse:0.055 test_acc:0.6689\n",
      "Epoch  288\n",
      "Accuracy ---->  0.7002560496330261\n",
      "Iter:288 train_rmse:4.326 test_loss:140.0 test_rmse:0.05498 test_acc:0.669\n",
      "Epoch  289\n",
      "Accuracy ---->  0.7003938555717468\n",
      "Iter:289 train_rmse:4.324 test_loss:139.9 test_rmse:0.05495 test_acc:0.6692\n",
      "Epoch  290\n",
      "Accuracy ---->  0.7005310952663422\n",
      "Iter:290 train_rmse:4.322 test_loss:139.8 test_rmse:0.05493 test_acc:0.6693\n",
      "Epoch  291\n",
      "Accuracy ---->  0.7006672918796539\n",
      "Iter:291 train_rmse:4.32 test_loss:139.7 test_rmse:0.05491 test_acc:0.6694\n",
      "Epoch  292\n",
      "Accuracy ---->  0.7008028626441956\n",
      "Iter:292 train_rmse:4.318 test_loss:139.6 test_rmse:0.05489 test_acc:0.6696\n",
      "Epoch  293\n",
      "Accuracy ---->  0.7009369730949402\n",
      "Iter:293 train_rmse:4.316 test_loss:139.5 test_rmse:0.05487 test_acc:0.6697\n",
      "Epoch  294\n",
      "Accuracy ---->  0.7010698914527893\n",
      "Iter:294 train_rmse:4.314 test_loss:139.4 test_rmse:0.05484 test_acc:0.6698\n",
      "Epoch  295\n",
      "Accuracy ---->  0.7012016773223877\n",
      "Iter:295 train_rmse:4.312 test_loss:139.3 test_rmse:0.05482 test_acc:0.6699\n",
      "Epoch  296\n",
      "Accuracy ---->  0.7013320624828339\n",
      "Iter:296 train_rmse:4.31 test_loss:139.2 test_rmse:0.0548 test_acc:0.6701\n",
      "Epoch  297\n",
      "Accuracy ---->  0.7014607489109039\n",
      "Iter:297 train_rmse:4.308 test_loss:139.1 test_rmse:0.05478 test_acc:0.6702\n",
      "Epoch  298\n",
      "Accuracy ---->  0.7015876173973083\n",
      "Iter:298 train_rmse:4.306 test_loss:138.9 test_rmse:0.05476 test_acc:0.6703\n",
      "Epoch  299\n",
      "Accuracy ---->  0.701712965965271\n",
      "Iter:299 train_rmse:4.304 test_loss:138.8 test_rmse:0.05474 test_acc:0.6704\n",
      "Epoch  300\n",
      "Accuracy ---->  0.7018363177776337\n",
      "Iter:300 train_rmse:4.303 test_loss:138.7 test_rmse:0.05472 test_acc:0.6706\n",
      "Epoch  301\n",
      "Accuracy ---->  0.7019577622413635\n",
      "Iter:301 train_rmse:4.301 test_loss:138.6 test_rmse:0.0547 test_acc:0.6707\n",
      "Epoch  302\n",
      "Accuracy ---->  0.7020773589611053\n",
      "Iter:302 train_rmse:4.299 test_loss:138.5 test_rmse:0.05468 test_acc:0.6708\n",
      "Epoch  303\n",
      "Accuracy ---->  0.7021947205066681\n",
      "Iter:303 train_rmse:4.298 test_loss:138.4 test_rmse:0.05466 test_acc:0.6709\n",
      "Epoch  304\n",
      "Accuracy ---->  0.7023100554943085\n",
      "Iter:304 train_rmse:4.296 test_loss:138.3 test_rmse:0.05464 test_acc:0.6711\n",
      "Epoch  305\n",
      "Accuracy ---->  0.7024235725402832\n",
      "Iter:305 train_rmse:4.294 test_loss:138.2 test_rmse:0.05462 test_acc:0.6712\n",
      "Epoch  306\n",
      "Accuracy ---->  0.7025350034236908\n",
      "Iter:306 train_rmse:4.293 test_loss:138.1 test_rmse:0.05459 test_acc:0.6713\n",
      "Epoch  307\n",
      "Accuracy ---->  0.7026447057723999\n",
      "Iter:307 train_rmse:4.291 test_loss:138.0 test_rmse:0.05457 test_acc:0.6714\n",
      "Epoch  308\n",
      "Accuracy ---->  0.7027525901794434\n",
      "Iter:308 train_rmse:4.289 test_loss:137.9 test_rmse:0.05455 test_acc:0.6716\n",
      "Epoch  309\n",
      "Accuracy ---->  0.7028585970401764\n",
      "Iter:309 train_rmse:4.288 test_loss:137.8 test_rmse:0.05453 test_acc:0.6717\n",
      "Epoch  310\n",
      "Accuracy ---->  0.7029633224010468\n",
      "Iter:310 train_rmse:4.286 test_loss:137.7 test_rmse:0.05451 test_acc:0.6718\n",
      "Epoch  311\n",
      "Accuracy ---->  0.7030663192272186\n",
      "Iter:311 train_rmse:4.285 test_loss:137.6 test_rmse:0.05449 test_acc:0.672\n",
      "Epoch  312\n",
      "Accuracy ---->  0.7031682431697845\n",
      "Iter:312 train_rmse:4.283 test_loss:137.5 test_rmse:0.05447 test_acc:0.6721\n",
      "Epoch  313\n",
      "Accuracy ---->  0.7032693326473236\n",
      "Iter:313 train_rmse:4.282 test_loss:137.4 test_rmse:0.05444 test_acc:0.6722\n",
      "Epoch  314\n",
      "Accuracy ---->  0.703369677066803\n",
      "Iter:314 train_rmse:4.281 test_loss:137.3 test_rmse:0.05442 test_acc:0.6724\n",
      "Epoch  315\n",
      "Accuracy ---->  0.703469455242157\n",
      "Iter:315 train_rmse:4.279 test_loss:137.1 test_rmse:0.0544 test_acc:0.6725\n",
      "Epoch  316\n",
      "Accuracy ---->  0.7035686075687408\n",
      "Iter:316 train_rmse:4.278 test_loss:137.0 test_rmse:0.05438 test_acc:0.6726\n",
      "Epoch  317\n",
      "Accuracy ---->  0.7036678493022919\n",
      "Iter:317 train_rmse:4.276 test_loss:136.9 test_rmse:0.05435 test_acc:0.6728\n",
      "Epoch  318\n",
      "Accuracy ---->  0.7037667334079742\n",
      "Iter:318 train_rmse:4.275 test_loss:136.8 test_rmse:0.05433 test_acc:0.6729\n",
      "Epoch  319\n",
      "Accuracy ---->  0.7038662433624268\n",
      "Iter:319 train_rmse:4.273 test_loss:136.7 test_rmse:0.0543 test_acc:0.6731\n",
      "Epoch  320\n",
      "Accuracy ---->  0.7039652764797211\n",
      "Iter:320 train_rmse:4.272 test_loss:136.5 test_rmse:0.05428 test_acc:0.6732\n",
      "Epoch  321\n",
      "Accuracy ---->  0.7040652334690094\n",
      "Iter:321 train_rmse:4.271 test_loss:136.4 test_rmse:0.05425 test_acc:0.6734\n",
      "Epoch  322\n",
      "Accuracy ---->  0.7041654288768768\n",
      "Iter:322 train_rmse:4.269 test_loss:136.3 test_rmse:0.05423 test_acc:0.6735\n",
      "Epoch  323\n",
      "Accuracy ---->  0.7042660415172577\n",
      "Iter:323 train_rmse:4.268 test_loss:136.2 test_rmse:0.0542 test_acc:0.6737\n",
      "Epoch  324\n",
      "Accuracy ---->  0.7043673694133759\n",
      "Iter:324 train_rmse:4.266 test_loss:136.0 test_rmse:0.05418 test_acc:0.6738\n",
      "Epoch  325\n",
      "Accuracy ---->  0.7044692039489746\n",
      "Iter:325 train_rmse:4.265 test_loss:135.9 test_rmse:0.05415 test_acc:0.674\n",
      "Epoch  326\n",
      "Accuracy ---->  0.7045710384845734\n",
      "Iter:326 train_rmse:4.263 test_loss:135.8 test_rmse:0.05413 test_acc:0.6741\n",
      "Epoch  327\n",
      "Accuracy ---->  0.7046738564968109\n",
      "Iter:327 train_rmse:4.262 test_loss:135.7 test_rmse:0.0541 test_acc:0.6743\n",
      "Epoch  328\n",
      "Accuracy ---->  0.7047770023345947\n",
      "Iter:328 train_rmse:4.26 test_loss:135.5 test_rmse:0.05408 test_acc:0.6744\n",
      "Epoch  329\n",
      "Accuracy ---->  0.7048803865909576\n",
      "Iter:329 train_rmse:4.259 test_loss:135.4 test_rmse:0.05405 test_acc:0.6746\n",
      "Epoch  330\n",
      "Accuracy ---->  0.704984188079834\n",
      "Iter:330 train_rmse:4.257 test_loss:135.3 test_rmse:0.05402 test_acc:0.6747\n",
      "Epoch  331\n",
      "Accuracy ---->  0.7050882577896118\n",
      "Iter:331 train_rmse:4.256 test_loss:135.1 test_rmse:0.054 test_acc:0.6749\n",
      "Epoch  332\n",
      "Accuracy ---->  0.7051925659179688\n",
      "Iter:332 train_rmse:4.254 test_loss:135.0 test_rmse:0.05397 test_acc:0.6751\n",
      "Epoch  333\n",
      "Accuracy ---->  0.705296665430069\n",
      "Iter:333 train_rmse:4.253 test_loss:134.9 test_rmse:0.05395 test_acc:0.6752\n",
      "Epoch  334\n",
      "Accuracy ---->  0.7054011821746826\n",
      "Iter:334 train_rmse:4.251 test_loss:134.8 test_rmse:0.05392 test_acc:0.6754\n",
      "Epoch  335\n",
      "Accuracy ---->  0.7055052816867828\n",
      "Iter:335 train_rmse:4.25 test_loss:134.6 test_rmse:0.0539 test_acc:0.6755\n",
      "Epoch  336\n",
      "Accuracy ---->  0.7056097090244293\n",
      "Iter:336 train_rmse:4.248 test_loss:134.5 test_rmse:0.05387 test_acc:0.6757\n",
      "Epoch  337\n",
      "Accuracy ---->  0.7057133913040161\n",
      "Iter:337 train_rmse:4.247 test_loss:134.4 test_rmse:0.05385 test_acc:0.6758\n",
      "Epoch  338\n",
      "Accuracy ---->  0.7058169841766357\n",
      "Iter:338 train_rmse:4.245 test_loss:134.3 test_rmse:0.05382 test_acc:0.676\n",
      "Epoch  339\n",
      "Accuracy ---->  0.7059208154678345\n",
      "Iter:339 train_rmse:4.244 test_loss:134.2 test_rmse:0.0538 test_acc:0.6761\n",
      "Epoch  340\n",
      "Accuracy ---->  0.7060244083404541\n",
      "Iter:340 train_rmse:4.242 test_loss:134.0 test_rmse:0.05378 test_acc:0.6762\n",
      "Epoch  341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.7061276137828827\n",
      "Iter:341 train_rmse:4.241 test_loss:133.9 test_rmse:0.05375 test_acc:0.6764\n",
      "Epoch  342\n",
      "Accuracy ---->  0.7062309980392456\n",
      "Iter:342 train_rmse:4.239 test_loss:133.8 test_rmse:0.05373 test_acc:0.6765\n",
      "Epoch  343\n",
      "Accuracy ---->  0.7063346803188324\n",
      "Iter:343 train_rmse:4.238 test_loss:133.7 test_rmse:0.05371 test_acc:0.6767\n",
      "Epoch  344\n",
      "Accuracy ---->  0.706438422203064\n",
      "Iter:344 train_rmse:4.236 test_loss:133.6 test_rmse:0.05368 test_acc:0.6768\n",
      "Epoch  345\n",
      "Accuracy ---->  0.7065428793430328\n",
      "Iter:345 train_rmse:4.235 test_loss:133.5 test_rmse:0.05366 test_acc:0.6769\n",
      "Epoch  346\n",
      "Accuracy ---->  0.7066478729248047\n",
      "Iter:346 train_rmse:4.233 test_loss:133.4 test_rmse:0.05364 test_acc:0.6771\n",
      "Epoch  347\n",
      "Accuracy ---->  0.7067538499832153\n",
      "Iter:347 train_rmse:4.232 test_loss:133.2 test_rmse:0.05362 test_acc:0.6772\n",
      "Epoch  348\n",
      "Accuracy ---->  0.706860363483429\n",
      "Iter:348 train_rmse:4.23 test_loss:133.1 test_rmse:0.05359 test_acc:0.6773\n",
      "Epoch  349\n",
      "Accuracy ---->  0.7069686055183411\n",
      "Iter:349 train_rmse:4.229 test_loss:133.0 test_rmse:0.05357 test_acc:0.6775\n",
      "Epoch  350\n",
      "Accuracy ---->  0.7070775926113129\n",
      "Iter:350 train_rmse:4.227 test_loss:132.9 test_rmse:0.05355 test_acc:0.6776\n",
      "Epoch  351\n",
      "Accuracy ---->  0.7071882784366608\n",
      "Iter:351 train_rmse:4.225 test_loss:132.8 test_rmse:0.05353 test_acc:0.6777\n",
      "Epoch  352\n",
      "Accuracy ---->  0.7072997689247131\n",
      "Iter:352 train_rmse:4.224 test_loss:132.7 test_rmse:0.05351 test_acc:0.6779\n",
      "Epoch  353\n",
      "Accuracy ---->  0.7074131071567535\n",
      "Iter:353 train_rmse:4.222 test_loss:132.6 test_rmse:0.05349 test_acc:0.678\n",
      "Epoch  354\n",
      "Accuracy ---->  0.7075272500514984\n",
      "Iter:354 train_rmse:4.221 test_loss:132.5 test_rmse:0.05347 test_acc:0.6781\n",
      "Epoch  355\n",
      "Accuracy ---->  0.7076427638530731\n",
      "Iter:355 train_rmse:4.219 test_loss:132.4 test_rmse:0.05345 test_acc:0.6782\n",
      "Epoch  356\n",
      "Accuracy ---->  0.7077593803405762\n",
      "Iter:356 train_rmse:4.217 test_loss:132.3 test_rmse:0.05343 test_acc:0.6784\n",
      "Epoch  357\n",
      "Accuracy ---->  0.7078764736652374\n",
      "Iter:357 train_rmse:4.216 test_loss:132.2 test_rmse:0.05341 test_acc:0.6785\n",
      "Epoch  358\n",
      "Accuracy ---->  0.707993894815445\n",
      "Iter:358 train_rmse:4.214 test_loss:132.1 test_rmse:0.05339 test_acc:0.6786\n",
      "Epoch  359\n",
      "Accuracy ---->  0.7081122696399689\n",
      "Iter:359 train_rmse:4.212 test_loss:132.0 test_rmse:0.05337 test_acc:0.6787\n",
      "Epoch  360\n",
      "Accuracy ---->  0.7082302868366241\n",
      "Iter:360 train_rmse:4.21 test_loss:131.9 test_rmse:0.05335 test_acc:0.6788\n",
      "Epoch  361\n",
      "Accuracy ---->  0.7083487212657928\n",
      "Iter:361 train_rmse:4.209 test_loss:131.8 test_rmse:0.05333 test_acc:0.6789\n",
      "Epoch  362\n",
      "Accuracy ---->  0.7084671854972839\n",
      "Iter:362 train_rmse:4.207 test_loss:131.7 test_rmse:0.05331 test_acc:0.6791\n",
      "Epoch  363\n",
      "Accuracy ---->  0.7085852324962616\n",
      "Iter:363 train_rmse:4.205 test_loss:131.6 test_rmse:0.05329 test_acc:0.6792\n",
      "Epoch  364\n",
      "Accuracy ---->  0.70870241522789\n",
      "Iter:364 train_rmse:4.204 test_loss:131.5 test_rmse:0.05327 test_acc:0.6793\n",
      "Epoch  365\n",
      "Accuracy ---->  0.7088200449943542\n",
      "Iter:365 train_rmse:4.202 test_loss:131.4 test_rmse:0.05325 test_acc:0.6794\n",
      "Epoch  366\n",
      "Accuracy ---->  0.7089361846446991\n",
      "Iter:366 train_rmse:4.2 test_loss:131.3 test_rmse:0.05323 test_acc:0.6795\n",
      "Epoch  367\n",
      "Accuracy ---->  0.7090521156787872\n",
      "Iter:367 train_rmse:4.199 test_loss:131.3 test_rmse:0.05321 test_acc:0.6797\n",
      "Epoch  368\n",
      "Accuracy ---->  0.7091671526432037\n",
      "Iter:368 train_rmse:4.197 test_loss:131.2 test_rmse:0.05319 test_acc:0.6798\n",
      "Epoch  369\n",
      "Accuracy ---->  0.7092814743518829\n",
      "Iter:369 train_rmse:4.195 test_loss:131.1 test_rmse:0.05317 test_acc:0.6799\n",
      "Epoch  370\n",
      "Accuracy ---->  0.70939502120018\n",
      "Iter:370 train_rmse:4.194 test_loss:131.0 test_rmse:0.05315 test_acc:0.68\n",
      "Epoch  371\n",
      "Accuracy ---->  0.7095082402229309\n",
      "Iter:371 train_rmse:4.192 test_loss:130.9 test_rmse:0.05313 test_acc:0.6801\n",
      "Epoch  372\n",
      "Accuracy ---->  0.7096208333969116\n",
      "Iter:372 train_rmse:4.19 test_loss:130.8 test_rmse:0.05311 test_acc:0.6803\n",
      "Epoch  373\n",
      "Accuracy ---->  0.7097320258617401\n",
      "Iter:373 train_rmse:4.189 test_loss:130.7 test_rmse:0.05309 test_acc:0.6804\n",
      "Epoch  374\n",
      "Accuracy ---->  0.7098440825939178\n",
      "Iter:374 train_rmse:4.187 test_loss:130.6 test_rmse:0.05307 test_acc:0.6805\n",
      "Epoch  375\n",
      "Accuracy ---->  0.709954708814621\n",
      "Iter:375 train_rmse:4.186 test_loss:130.5 test_rmse:0.05305 test_acc:0.6806\n",
      "Epoch  376\n",
      "Accuracy ---->  0.7100650072097778\n",
      "Iter:376 train_rmse:4.184 test_loss:130.4 test_rmse:0.05303 test_acc:0.6807\n",
      "Epoch  377\n",
      "Accuracy ---->  0.7101758420467377\n",
      "Iter:377 train_rmse:4.182 test_loss:130.3 test_rmse:0.05301 test_acc:0.6809\n",
      "Epoch  378\n",
      "Accuracy ---->  0.7102851569652557\n",
      "Iter:378 train_rmse:4.181 test_loss:130.2 test_rmse:0.05298 test_acc:0.681\n",
      "Epoch  379\n",
      "Accuracy ---->  0.7103956639766693\n",
      "Iter:379 train_rmse:4.179 test_loss:130.1 test_rmse:0.05296 test_acc:0.6811\n",
      "Epoch  380\n",
      "Accuracy ---->  0.7105056941509247\n",
      "Iter:380 train_rmse:4.178 test_loss:129.9 test_rmse:0.05294 test_acc:0.6813\n",
      "Epoch  381\n",
      "Accuracy ---->  0.7106160819530487\n",
      "Iter:381 train_rmse:4.176 test_loss:129.8 test_rmse:0.05292 test_acc:0.6814\n",
      "Epoch  382\n",
      "Accuracy ---->  0.7107262015342712\n",
      "Iter:382 train_rmse:4.174 test_loss:129.7 test_rmse:0.0529 test_acc:0.6815\n",
      "Epoch  383\n",
      "Accuracy ---->  0.7108364403247833\n",
      "Iter:383 train_rmse:4.173 test_loss:129.6 test_rmse:0.05288 test_acc:0.6817\n",
      "Epoch  384\n",
      "Accuracy ---->  0.7109476923942566\n",
      "Iter:384 train_rmse:4.171 test_loss:129.5 test_rmse:0.05285 test_acc:0.6818\n",
      "Epoch  385\n",
      "Accuracy ---->  0.7110582292079926\n",
      "Iter:385 train_rmse:4.17 test_loss:129.4 test_rmse:0.05283 test_acc:0.6819\n",
      "Epoch  386\n",
      "Accuracy ---->  0.7111703157424927\n",
      "Iter:386 train_rmse:4.168 test_loss:129.3 test_rmse:0.05281 test_acc:0.6821\n",
      "Epoch  387\n",
      "Accuracy ---->  0.711282879114151\n",
      "Iter:387 train_rmse:4.166 test_loss:129.2 test_rmse:0.05279 test_acc:0.6822\n",
      "Epoch  388\n",
      "Accuracy ---->  0.7113957703113556\n",
      "Iter:388 train_rmse:4.165 test_loss:129.1 test_rmse:0.05276 test_acc:0.6823\n",
      "Epoch  389\n",
      "Accuracy ---->  0.7115089893341064\n",
      "Iter:389 train_rmse:4.163 test_loss:129.0 test_rmse:0.05274 test_acc:0.6825\n",
      "Epoch  390\n",
      "Accuracy ---->  0.7116228640079498\n",
      "Iter:390 train_rmse:4.161 test_loss:128.9 test_rmse:0.05272 test_acc:0.6826\n",
      "Epoch  391\n",
      "Accuracy ---->  0.7117368578910828\n",
      "Iter:391 train_rmse:4.16 test_loss:128.7 test_rmse:0.05269 test_acc:0.6827\n",
      "Epoch  392\n",
      "Accuracy ---->  0.7118517756462097\n",
      "Iter:392 train_rmse:4.158 test_loss:128.6 test_rmse:0.05267 test_acc:0.6829\n",
      "Epoch  393\n",
      "Accuracy ---->  0.7119669616222382\n",
      "Iter:393 train_rmse:4.157 test_loss:128.5 test_rmse:0.05265 test_acc:0.683\n",
      "Epoch  394\n",
      "Accuracy ---->  0.7120823562145233\n",
      "Iter:394 train_rmse:4.155 test_loss:128.4 test_rmse:0.05263 test_acc:0.6832\n",
      "Epoch  395\n",
      "Accuracy ---->  0.7121982574462891\n",
      "Iter:395 train_rmse:4.153 test_loss:128.3 test_rmse:0.0526 test_acc:0.6833\n",
      "Epoch  396\n",
      "Accuracy ---->  0.7123134732246399\n",
      "Iter:396 train_rmse:4.152 test_loss:128.2 test_rmse:0.05258 test_acc:0.6834\n",
      "Epoch  397\n",
      "Accuracy ---->  0.71242955327034\n",
      "Iter:397 train_rmse:4.15 test_loss:128.1 test_rmse:0.05256 test_acc:0.6836\n",
      "Epoch  398\n",
      "Accuracy ---->  0.7125449478626251\n",
      "Iter:398 train_rmse:4.148 test_loss:128.0 test_rmse:0.05254 test_acc:0.6837\n",
      "Epoch  399\n",
      "Accuracy ---->  0.7126602530479431\n",
      "Iter:399 train_rmse:4.147 test_loss:127.9 test_rmse:0.05252 test_acc:0.6838\n",
      "Epoch  400\n",
      "Accuracy ---->  0.7127751410007477\n",
      "Iter:400 train_rmse:4.145 test_loss:127.8 test_rmse:0.0525 test_acc:0.684\n",
      "Epoch  401\n",
      "Accuracy ---->  0.712889701128006\n",
      "Iter:401 train_rmse:4.143 test_loss:127.7 test_rmse:0.05247 test_acc:0.6841\n",
      "Epoch  402\n",
      "Accuracy ---->  0.7130036950111389\n",
      "Iter:402 train_rmse:4.142 test_loss:127.6 test_rmse:0.05245 test_acc:0.6842\n",
      "Epoch  403\n",
      "Accuracy ---->  0.7131173312664032\n",
      "Iter:403 train_rmse:4.14 test_loss:127.5 test_rmse:0.05243 test_acc:0.6843\n",
      "Epoch  404\n",
      "Accuracy ---->  0.7132298648357391\n",
      "Iter:404 train_rmse:4.138 test_loss:127.4 test_rmse:0.05241 test_acc:0.6845\n",
      "Epoch  405\n",
      "Accuracy ---->  0.7133425176143646\n",
      "Iter:405 train_rmse:4.137 test_loss:127.3 test_rmse:0.05239 test_acc:0.6846\n",
      "Epoch  406\n",
      "Accuracy ---->  0.7134547233581543\n",
      "Iter:406 train_rmse:4.135 test_loss:127.2 test_rmse:0.05237 test_acc:0.6847\n",
      "Epoch  407\n",
      "Accuracy ---->  0.7135660648345947\n",
      "Iter:407 train_rmse:4.133 test_loss:127.1 test_rmse:0.05235 test_acc:0.6848\n",
      "Epoch  408\n",
      "Accuracy ---->  0.7136766612529755\n",
      "Iter:408 train_rmse:4.132 test_loss:127.0 test_rmse:0.05233 test_acc:0.685\n",
      "Epoch  409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ---->  0.713787168264389\n",
      "Iter:409 train_rmse:4.13 test_loss:126.9 test_rmse:0.05231 test_acc:0.6851\n",
      "Epoch  410\n",
      "Accuracy ---->  0.7138969302177429\n",
      "Iter:410 train_rmse:4.129 test_loss:126.8 test_rmse:0.05229 test_acc:0.6852\n",
      "Epoch  411\n",
      "Accuracy ---->  0.7140066027641296\n",
      "Iter:411 train_rmse:4.127 test_loss:126.7 test_rmse:0.05227 test_acc:0.6853\n",
      "Epoch  412\n",
      "Accuracy ---->  0.7141149640083313\n",
      "Iter:412 train_rmse:4.126 test_loss:126.6 test_rmse:0.05225 test_acc:0.6855\n",
      "Epoch  413\n",
      "Accuracy ---->  0.7142232358455658\n",
      "Iter:413 train_rmse:4.124 test_loss:126.5 test_rmse:0.05223 test_acc:0.6856\n",
      "Epoch  414\n",
      "Accuracy ---->  0.7143313884735107\n",
      "Iter:414 train_rmse:4.122 test_loss:126.4 test_rmse:0.05221 test_acc:0.6857\n",
      "Epoch  415\n",
      "Accuracy ---->  0.7144386172294617\n",
      "Iter:415 train_rmse:4.121 test_loss:126.3 test_rmse:0.05219 test_acc:0.6858\n",
      "Epoch  416\n",
      "Accuracy ---->  0.7145452797412872\n",
      "Iter:416 train_rmse:4.119 test_loss:126.2 test_rmse:0.05216 test_acc:0.6859\n",
      "Epoch  417\n",
      "Accuracy ---->  0.7146520614624023\n",
      "Iter:417 train_rmse:4.118 test_loss:126.1 test_rmse:0.05214 test_acc:0.6861\n",
      "Epoch  418\n",
      "Accuracy ---->  0.7147580683231354\n",
      "Iter:418 train_rmse:4.116 test_loss:126.0 test_rmse:0.05212 test_acc:0.6862\n",
      "Epoch  419\n",
      "Accuracy ---->  0.7148636877536774\n",
      "Iter:419 train_rmse:4.115 test_loss:125.9 test_rmse:0.0521 test_acc:0.6863\n",
      "Epoch  420\n",
      "Accuracy ---->  0.714969277381897\n",
      "Iter:420 train_rmse:4.113 test_loss:125.8 test_rmse:0.05208 test_acc:0.6864\n",
      "Epoch  421\n",
      "Accuracy ---->  0.715073972940445\n",
      "Iter:421 train_rmse:4.112 test_loss:125.7 test_rmse:0.05206 test_acc:0.6865\n",
      "Epoch  422\n",
      "Accuracy ---->  0.7151789367198944\n",
      "Iter:422 train_rmse:4.11 test_loss:125.6 test_rmse:0.05204 test_acc:0.6867\n",
      "Epoch  423\n",
      "Accuracy ---->  0.7152831256389618\n",
      "Iter:423 train_rmse:4.109 test_loss:125.5 test_rmse:0.05202 test_acc:0.6868\n",
      "Epoch  424\n",
      "Accuracy ---->  0.7153878211975098\n",
      "Iter:424 train_rmse:4.107 test_loss:125.4 test_rmse:0.052 test_acc:0.6869\n",
      "Epoch  425\n",
      "Accuracy ---->  0.7154910564422607\n",
      "Iter:425 train_rmse:4.106 test_loss:125.3 test_rmse:0.05198 test_acc:0.687\n",
      "Epoch  426\n",
      "Accuracy ---->  0.7155943512916565\n",
      "Iter:426 train_rmse:4.104 test_loss:125.2 test_rmse:0.05196 test_acc:0.6871\n",
      "Epoch  427\n",
      "Accuracy ---->  0.7156968414783478\n",
      "Iter:427 train_rmse:4.103 test_loss:125.1 test_rmse:0.05195 test_acc:0.6873\n",
      "Epoch  428\n",
      "Accuracy ---->  0.7157996892929077\n",
      "Iter:428 train_rmse:4.101 test_loss:125.0 test_rmse:0.05193 test_acc:0.6874\n",
      "Epoch  429\n",
      "Accuracy ---->  0.715901792049408\n",
      "Iter:429 train_rmse:4.1 test_loss:125.0 test_rmse:0.05191 test_acc:0.6875\n",
      "Epoch  430\n",
      "Accuracy ---->  0.716003954410553\n",
      "Iter:430 train_rmse:4.098 test_loss:124.9 test_rmse:0.05189 test_acc:0.6876\n",
      "Epoch  431\n",
      "Accuracy ---->  0.7161055505275726\n",
      "Iter:431 train_rmse:4.097 test_loss:124.8 test_rmse:0.05187 test_acc:0.6877\n",
      "Epoch  432\n",
      "Accuracy ---->  0.7162070572376251\n",
      "Iter:432 train_rmse:4.095 test_loss:124.7 test_rmse:0.05185 test_acc:0.6879\n",
      "Epoch  433\n",
      "Accuracy ---->  0.7163076400756836\n",
      "Iter:433 train_rmse:4.094 test_loss:124.6 test_rmse:0.05183 test_acc:0.688\n",
      "Epoch  434\n",
      "Accuracy ---->  0.7164080440998077\n",
      "Iter:434 train_rmse:4.092 test_loss:124.5 test_rmse:0.05181 test_acc:0.6881\n",
      "Epoch  435\n",
      "Accuracy ---->  0.7165080606937408\n",
      "Iter:435 train_rmse:4.091 test_loss:124.4 test_rmse:0.05179 test_acc:0.6882\n",
      "Epoch  436\n",
      "Accuracy ---->  0.7166078388690948\n",
      "Iter:436 train_rmse:4.09 test_loss:124.3 test_rmse:0.05177 test_acc:0.6883\n",
      "Epoch  437\n",
      "Accuracy ---->  0.7167070508003235\n",
      "Iter:437 train_rmse:4.088 test_loss:124.2 test_rmse:0.05175 test_acc:0.6884\n",
      "Epoch  438\n",
      "Accuracy ---->  0.7168056070804596\n",
      "Iter:438 train_rmse:4.087 test_loss:124.1 test_rmse:0.05173 test_acc:0.6885\n",
      "Epoch  439\n",
      "Accuracy ---->  0.7169038355350494\n",
      "Iter:439 train_rmse:4.085 test_loss:124.0 test_rmse:0.05172 test_acc:0.6886\n",
      "Epoch  440\n",
      "Accuracy ---->  0.7170016467571259\n",
      "Iter:440 train_rmse:4.084 test_loss:124.0 test_rmse:0.0517 test_acc:0.6888\n",
      "Epoch  441\n",
      "Accuracy ---->  0.7170992493629456\n",
      "Iter:441 train_rmse:4.082 test_loss:123.9 test_rmse:0.05168 test_acc:0.6889\n",
      "Epoch  442\n",
      "Accuracy ---->  0.7171960473060608\n",
      "Iter:442 train_rmse:4.081 test_loss:123.8 test_rmse:0.05166 test_acc:0.689\n",
      "Epoch  443\n",
      "Accuracy ---->  0.7172924876213074\n",
      "Iter:443 train_rmse:4.08 test_loss:123.7 test_rmse:0.05164 test_acc:0.6891\n",
      "Epoch  444\n",
      "Accuracy ---->  0.7173885107040405\n",
      "Iter:444 train_rmse:4.078 test_loss:123.6 test_rmse:0.05162 test_acc:0.6892\n",
      "Epoch  445\n",
      "Accuracy ---->  0.7174835801124573\n",
      "Iter:445 train_rmse:4.077 test_loss:123.5 test_rmse:0.05161 test_acc:0.6893\n",
      "Epoch  446\n",
      "Accuracy ---->  0.717578262090683\n",
      "Iter:446 train_rmse:4.076 test_loss:123.4 test_rmse:0.05159 test_acc:0.6894\n",
      "Epoch  447\n",
      "Accuracy ---->  0.7176728248596191\n",
      "Iter:447 train_rmse:4.074 test_loss:123.4 test_rmse:0.05157 test_acc:0.6895\n",
      "Epoch  448\n",
      "Accuracy ---->  0.7177665531635284\n",
      "Iter:448 train_rmse:4.073 test_loss:123.3 test_rmse:0.05155 test_acc:0.6896\n",
      "Epoch  449\n",
      "Accuracy ---->  0.7178599238395691\n",
      "Iter:449 train_rmse:4.071 test_loss:123.2 test_rmse:0.05154 test_acc:0.6897\n",
      "Epoch  450\n",
      "Accuracy ---->  0.7179521024227142\n",
      "Iter:450 train_rmse:4.07 test_loss:123.1 test_rmse:0.05152 test_acc:0.6898\n",
      "Epoch  451\n",
      "Accuracy ---->  0.7180435955524445\n",
      "Iter:451 train_rmse:4.069 test_loss:123.0 test_rmse:0.0515 test_acc:0.6899\n",
      "Epoch  452\n",
      "Accuracy ---->  0.7181347608566284\n",
      "Iter:452 train_rmse:4.068 test_loss:122.9 test_rmse:0.05148 test_acc:0.69\n",
      "Epoch  453\n",
      "Accuracy ---->  0.7182260155677795\n",
      "Iter:453 train_rmse:4.066 test_loss:122.9 test_rmse:0.05147 test_acc:0.6902\n",
      "Epoch  454\n",
      "Accuracy ---->  0.7183162569999695\n",
      "Iter:454 train_rmse:4.065 test_loss:122.8 test_rmse:0.05145 test_acc:0.6903\n",
      "Epoch  455\n",
      "Accuracy ---->  0.7184056043624878\n",
      "Iter:455 train_rmse:4.064 test_loss:122.7 test_rmse:0.05143 test_acc:0.6904\n",
      "Epoch  456\n",
      "Accuracy ---->  0.7184947729110718\n",
      "Iter:456 train_rmse:4.062 test_loss:122.6 test_rmse:0.05141 test_acc:0.6905\n",
      "Epoch  457\n",
      "Accuracy ---->  0.7185827493667603\n",
      "Iter:457 train_rmse:4.061 test_loss:122.5 test_rmse:0.0514 test_acc:0.6906\n",
      "Epoch  458\n",
      "Accuracy ---->  0.7186709046363831\n",
      "Iter:458 train_rmse:4.06 test_loss:122.5 test_rmse:0.05138 test_acc:0.6907\n",
      "Epoch  459\n",
      "Accuracy ---->  0.7187582552433014\n",
      "Iter:459 train_rmse:4.059 test_loss:122.4 test_rmse:0.05136 test_acc:0.6908\n",
      "Epoch  460\n",
      "Accuracy ---->  0.7188443839550018\n",
      "Iter:460 train_rmse:4.057 test_loss:122.3 test_rmse:0.05135 test_acc:0.6909\n",
      "Epoch  461\n",
      "Accuracy ---->  0.7189310789108276\n",
      "Iter:461 train_rmse:4.056 test_loss:122.2 test_rmse:0.05133 test_acc:0.691\n",
      "Epoch  462\n",
      "Accuracy ---->  0.7190167605876923\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bf461007a03e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy ----> \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n\u001b[1;32m---> 15\u001b[1;33m                                          feed_dict = {inputs:testX, labels:testY})\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;31m#train_label=np.reshape(trainY,[-1,num_nodes])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#train_acc=acc(train_label,train_output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Lema Labs ML Workshop x64\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Lema Labs ML Workshop x64\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Lema Labs ML Workshop x64\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Lema Labs ML Workshop x64\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Lema Labs ML Workshop x64\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Lema Labs ML Workshop x64\\python-3.7.2.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epoch):\n",
    "    print(\"Epoch \", epoch)\n",
    "    for m in range(totalbatch):\n",
    "        mini_batch = trainX[m * batch_size : (m+1) * batch_size]\n",
    "        mini_label = trainY[m * batch_size : (m+1) * batch_size]\n",
    "        _, loss1, rmse1, train_output = sess.run([optimizer, loss, error, y_pred],\n",
    "                                                 feed_dict = {inputs:mini_batch, labels:mini_label})\n",
    "        batch_loss.append(loss1)\n",
    "        batch_rmse.append(rmse1 * max_value)\n",
    "        train_label=np.reshape(mini_label,[-1,num_nodes])\n",
    "     #print(mini_label.shape,train_output.shape) (32, 1, 156) (32, 156)\n",
    "     # Test completely at every epoch\n",
    "    print(\"Accuracy ----> \", evaluation(train_label,train_output)[2])\n",
    "    loss2, rmse2, test_output = sess.run([loss, error, y_pred],\n",
    "                                         feed_dict = {inputs:testX, labels:testY})\n",
    "    #train_label=np.reshape(trainY,[-1,num_nodes])\n",
    "    #train_acc=acc(train_label,train_output)\n",
    "    test_label = np.reshape(testY,[-1,num_nodes])\n",
    "    rmse, mae, acc, r2_score, var_score = evaluation(test_label, test_output)\n",
    "    test_label1 = test_label * max_value#Inverse normalization\n",
    "    test_output1 = test_output * max_value\n",
    "    test_loss.append(loss2)\n",
    "    test_rmse.append(rmse * max_value)\n",
    "    test_mae.append(mae * max_value)\n",
    "    test_acc.append(acc)\n",
    "    test_r2.append(r2_score)\n",
    "    test_var.append(var_score)\n",
    "    test_pred.append(test_output1)\n",
    "    #print(mini_label.shape,train_output.shape)\n",
    "    print('Iter:{}'.format(epoch),\n",
    "          'train_rmse:{:.4}'.format(batch_rmse[-1]),\n",
    "          'test_loss:{:.4}'.format(loss2),\n",
    "          'test_rmse:{:.4}'.format(rmse),\n",
    "          'test_acc:{:.4}'.format(acc))\n",
    "    if (epoch % 500 == 0):        \n",
    "        saver.save(sess, path+'/model_10015TGCN_pre_%r'%epoch, global_step = epoch)\n",
    "        \n",
    "time_end = time.time()\n",
    "print('Time taken : ',time_end-time_start,'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = int(len(batch_rmse)/totalbatch)\n",
    "batch_rmse1 = [i for i in batch_rmse]\n",
    "train_rmse = [(sum(batch_rmse1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "batch_loss1 = [i for i in batch_loss]\n",
    "train_loss = [(sum(batch_loss1[i*totalbatch:(i+1)*totalbatch])/totalbatch) for i in range(b)]\n",
    "index = test_rmse.index(np.min(test_rmse))\n",
    "test_result = test_pred[index]\n",
    "var = pd.DataFrame(test_result)\n",
    "var.to_csv(path+'/test_result15.csv',index = False,header = False)\n",
    "#plot_result(test_result,test_label1,path)\n",
    "#plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing:\")\n",
    "print('min_rmse:%r'%(np.min(test_rmse)),\n",
    "      'min_mae:%r'%(test_mae[index]),\n",
    "      'max_acc:%r'%(test_acc[index]),\n",
    "      'r2:%r'%(test_r2[index]),\n",
    "      'var:%r'%test_var[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training:\")\n",
    "rmse, mae, acc, r2_score, var_score = evaluation(train_label,train_output)\n",
    "print('min_rmse:%r'%(rmse),\n",
    "      'min_mae:%r'%(mae),\n",
    "      'max_acc:%r'%(acc),\n",
    "      'r2:%r'%(r2_score),\n",
    "      'var:%r'%(var_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse normalisation\n",
    "print('min_rmse:%r'%(rmse*max_value),\n",
    "      'min_mae:%r'%(mae*max_value),\n",
    "      'max_acc:%r'%(acc),\n",
    "      'r2:%r'%(r2_score),\n",
    "      'var:%r'%(var_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(test_result,test_label1,path):\n",
    "    ##all test result visualization\n",
    "    fig1 = plt.figure(figsize=(7,1.5))\n",
    "#    ax1 = fig1.add_subplot(1,1,1)\n",
    "    a_pred = test_result[:,0]\n",
    "    a_true = test_label1[:,0]\n",
    "    plt.plot(a_pred,'r-',label='prediction')\n",
    "    plt.plot(a_true,'b-',label='true')\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_all.jpg')\n",
    "    plt.show()\n",
    "    ## oneday test result visualization\n",
    "    fig1 = plt.figure(figsize=(7,1.5))\n",
    "#    ax1 = fig1.add_subplot(1,1,1)\n",
    "    a_pred = test_result[0:96,0]\n",
    "    a_true = test_label1[0:96,0]\n",
    "    plt.plot(a_pred,'r-',label=\"prediction\")\n",
    "    plt.plot(a_true,'b-',label=\"true\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_oneday.jpg')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path):\n",
    "    ###train_rmse & test_rmse \n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(train_rmse, 'r-', label=\"train_rmse\")\n",
    "    plt.plot(test_rmse, 'b-', label=\"test_rmse\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/rmse.jpg')\n",
    "    plt.show()\n",
    "    #### train_loss & train_rmse\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(train_loss,'b-', label='train_loss')\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/train_loss.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(train_rmse,'b-', label='train_rmse')\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/train_rmse.jpg')\n",
    "    plt.show()\n",
    "\n",
    "    ### accuracy\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(test_acc, 'b-', label=\"test_acc\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_acc.jpg')\n",
    "    plt.show()\n",
    "    ### rmse\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(test_rmse, 'b-', label=\"test_rmse\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_rmse.jpg')\n",
    "    plt.show()\n",
    "    ### mae\n",
    "    fig1 = plt.figure(figsize=(5,3))\n",
    "    plt.plot(test_mae, 'b-', label=\"test_mae\")\n",
    "    plt.legend(loc='best',fontsize=10)\n",
    "    plt.savefig(path+'/test_mae.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(test_result,test_label1,path)\n",
    "plot_error(train_rmse,train_loss,test_rmse,test_acc,test_mae,path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
